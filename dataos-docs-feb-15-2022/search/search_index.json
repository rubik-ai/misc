{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Modern Data Fabric for the AI-powered World \u00b6 A first of its kind Data Operating System that removes complexity to future proof your data ecosystem so you can build and deploy new data services and applications... in months weeks. TBD","title":"Home"},{"location":"#modern-data-fabric-for-the-ai-powered-world","text":"A first of its kind Data Operating System that removes complexity to future proof your data ecosystem so you can build and deploy new data services and applications... in months weeks. TBD","title":"Modern Data Fabric for the AI-powered World"},{"location":"atlas/","text":"DataOS \u00ae Atlas \u00b6 Purpose \u00b6 All the ingested and processed data is not of any consequence unless it is used to derive actionable intelligence. One quick way to draw insights is via the visual depiction of patterns on the data within the target time period. Atlas helps you connect and query your data sources, build dashboards to visualize data and share them with the stakeholders. The conventional yet powerful SQL editor combined with slick UI tools to work on visualizations help you achieve deeper understanding leading to better and more informed decisions. Launching Atlas \u00b6 You can launch Atlas from the business section of the apps screen. The landing screen of the app has a list of previously created dashboards having dashboard names with tags, the creator name and created time. Querying \u00b6 After navigating to the query section from the 'Queries' tab, click Create Query in the navbar to spawn the editor. Here you can write the required query, and see the results. Keyboard Shortcuts \u00b6 Execute query: Ctrl/Cmd + Enter Save query: Ctrl/Cmd + S Toggle Auto Complete: Ctrl + Space Toggle Schema Browser: Alt/Option + D Schema Browser \u00b6 The schema browser lists all the tables and associated columns for the dataset. Search box helps speed up the filtering and picking the relevant tables and refresh button keeps the list updated. The list also periodically refreshes in the background. Double arrow at the end of the field is a shortcut to add the required table or column in the query. Auto Complete \u00b6 This query editor feature helps you in picking required tables/ columns from a suggestive dropdown appearing when you type. This feature, which is ON by default, can be turned off by the clicking the lightning bolt icon at the bottom of the editor pane or simply using the keyboard shortcut: Ctrl + Space Published and Unpublished Queries \u00b6 By default each query starts as an unpublished draft named New Query. It can't be included on dashboards unless it is published. To publish a query, change its name or click the Publish button. You can toggle the published status by clicking the Unpublish button. Unpublishing a query will not remove it from dashboards. But it will prevent you from adding it to any others. Archiving a query \u00b6 A query cannot be deleted, however, you can archive a query. It can be archived via the vertical ellipses menu at the top-right corner of the query editor. An archived query can only be accessed via direct links. Duplicating (forking) a Query \u00b6 If you need to create a copy of an existing query (created by you or someone else), you can fork it. To fork a query, just click on the Fork button. Query results \u00b6 Visit any query page and click the vertical ellipsis ( \u22ee ) button beneath the results pane. Then choose to download a CSV, TSV, or Excel file. This action downloads the current query result.","title":"DataOS<sup>\u00ae</sup> Atlas"},{"location":"atlas/#dataos-atlas","text":"","title":"DataOS\u00ae Atlas"},{"location":"atlas/#purpose","text":"All the ingested and processed data is not of any consequence unless it is used to derive actionable intelligence. One quick way to draw insights is via the visual depiction of patterns on the data within the target time period. Atlas helps you connect and query your data sources, build dashboards to visualize data and share them with the stakeholders. The conventional yet powerful SQL editor combined with slick UI tools to work on visualizations help you achieve deeper understanding leading to better and more informed decisions.","title":"Purpose"},{"location":"atlas/#launching-atlas","text":"You can launch Atlas from the business section of the apps screen. The landing screen of the app has a list of previously created dashboards having dashboard names with tags, the creator name and created time.","title":"Launching Atlas"},{"location":"atlas/#querying","text":"After navigating to the query section from the 'Queries' tab, click Create Query in the navbar to spawn the editor. Here you can write the required query, and see the results.","title":"Querying"},{"location":"atlas/#keyboard-shortcuts","text":"Execute query: Ctrl/Cmd + Enter Save query: Ctrl/Cmd + S Toggle Auto Complete: Ctrl + Space Toggle Schema Browser: Alt/Option + D","title":"Keyboard Shortcuts"},{"location":"atlas/#schema-browser","text":"The schema browser lists all the tables and associated columns for the dataset. Search box helps speed up the filtering and picking the relevant tables and refresh button keeps the list updated. The list also periodically refreshes in the background. Double arrow at the end of the field is a shortcut to add the required table or column in the query.","title":"Schema Browser"},{"location":"atlas/#auto-complete","text":"This query editor feature helps you in picking required tables/ columns from a suggestive dropdown appearing when you type. This feature, which is ON by default, can be turned off by the clicking the lightning bolt icon at the bottom of the editor pane or simply using the keyboard shortcut: Ctrl + Space","title":"Auto Complete"},{"location":"atlas/#published-and-unpublished-queries","text":"By default each query starts as an unpublished draft named New Query. It can't be included on dashboards unless it is published. To publish a query, change its name or click the Publish button. You can toggle the published status by clicking the Unpublish button. Unpublishing a query will not remove it from dashboards. But it will prevent you from adding it to any others.","title":"Published and Unpublished Queries"},{"location":"atlas/#archiving-a-query","text":"A query cannot be deleted, however, you can archive a query. It can be archived via the vertical ellipses menu at the top-right corner of the query editor. An archived query can only be accessed via direct links.","title":"Archiving a query"},{"location":"atlas/#duplicating-forking-a-query","text":"If you need to create a copy of an existing query (created by you or someone else), you can fork it. To fork a query, just click on the Fork button.","title":"Duplicating (forking) a Query"},{"location":"atlas/#query-results","text":"Visit any query page and click the vertical ellipsis ( \u22ee ) button beneath the results pane. Then choose to download a CSV, TSV, or Excel file. This action downloads the current query result.","title":"Query results"},{"location":"atlas/alerts/","text":"You can wish to be notified when the results of a certain query match a criterion or a set of criteria. For such cases, alerts can be generated and the notification format can be edited. To see a list of current Alerts click 'Alerts' on the top navbar. By default, they are sorted in reverse chronological order by the Created At column. You can reorder the list by clicking the column headings. Name shows the string name of each alert. You can change this at any time. Created By shows the user that created this Alert. State shows whether the Alert status is UNKNOWN, TRIGGERED, or OK. An Alert can be added via the Queries section. Top right side menu shows 'Alerts[0]', clicking on that the user is shown his created alerts Alert Notifications \u00b6 Next to the setting labeled \u201cTemplate\u201d, click the dropdown and select \u201cCustom template\u201d. A box will appear, consisting of input fields for subject and body. Any static content is valid, and you can also incorporate some built-in template variables: ALERT_STATUS - The evaluated alert status (string). ALERT_SUBJECT - The field/ column of importance (string). ALERT_PREDICATE - The field column of reference (can be used for comparison) (string). ALERT_OPERATOR - The alert condition operator (string). ALERT_NAME - The alert name (string). ALERT_URL - The alert page url (string). QUERY_NAME - The correlated query name (string). QUERY_URL - The correlated query page url (string). An example subject, for instance, could be: Alert \"{{ALERT_NAME}}\" changed status to {{ALERT_STATUS} Alert \"{{ALERT_NAME}}\" changed state to \"{{ALERT_STATUS}}\" Query Details - Name: {{QUERY_NAME}} Link: {{QUERY_URL}} Alert Details - Subject: {{ALERT_SUBJECT}} (calculated value was {{ALERT_SUBJECT_VALUE}}) Operator: {{ALERT_OPERATOR}} Predicate: {{ALERT_PREDICATE}} (calculated value was {{ALERT_PREDICATE_VALUE}}) Result: {{ALERT_STATUS}} Click here {{QUERY_URL}} to open Query's page. Click the \u201cPreview\u201d toggle button to preview the rendered result and save your changes by clicking the \u201cSave\u201d button. The preview is useful for verifying that template variables get rendered correctly. It is not an accurate representation of the eventual notification content, as each alert destinations can display notifications differently. To return to the default Redash message templates, reselect \u201cDefault template\u201d at any time.","title":"Alerts"},{"location":"atlas/alerts/#alert-notifications","text":"Next to the setting labeled \u201cTemplate\u201d, click the dropdown and select \u201cCustom template\u201d. A box will appear, consisting of input fields for subject and body. Any static content is valid, and you can also incorporate some built-in template variables: ALERT_STATUS - The evaluated alert status (string). ALERT_SUBJECT - The field/ column of importance (string). ALERT_PREDICATE - The field column of reference (can be used for comparison) (string). ALERT_OPERATOR - The alert condition operator (string). ALERT_NAME - The alert name (string). ALERT_URL - The alert page url (string). QUERY_NAME - The correlated query name (string). QUERY_URL - The correlated query page url (string). An example subject, for instance, could be: Alert \"{{ALERT_NAME}}\" changed status to {{ALERT_STATUS} Alert \"{{ALERT_NAME}}\" changed state to \"{{ALERT_STATUS}}\" Query Details - Name: {{QUERY_NAME}} Link: {{QUERY_URL}} Alert Details - Subject: {{ALERT_SUBJECT}} (calculated value was {{ALERT_SUBJECT_VALUE}}) Operator: {{ALERT_OPERATOR}} Predicate: {{ALERT_PREDICATE}} (calculated value was {{ALERT_PREDICATE_VALUE}}) Result: {{ALERT_STATUS}} Click here {{QUERY_URL}} to open Query's page. Click the \u201cPreview\u201d toggle button to preview the rendered result and save your changes by clicking the \u201cSave\u201d button. The preview is useful for verifying that template variables get rendered correctly. It is not an accurate representation of the eventual notification content, as each alert destinations can display notifications differently. To return to the default Redash message templates, reselect \u201cDefault template\u201d at any time.","title":"Alert Notifications"},{"location":"atlas/choropleth/","text":"Choropleth Maps display coloured, shaded or patterned geographical areas or regions that conform to a data variable. This provides a way to visualise trends over a geographical area. Colour variance and progression (dark to light and vice versa) show the data value measure. Typically, this can be a blending from one colour to another, a single hue progression, transparent to opaque, light to dark or an entire colour spectrum.","title":"Choropleth"},{"location":"atlas/cohort/","text":"A Cohort is a group of users that share a common characteristic. For instance, users acquired on the same date can belong to a cohort. Likewise users living in a specific region, can also belong to a cohort. Cohort analysis, or a cohort chart helps compare behaviors and metrics of different cohorts over time. It is easy to identify highest performing and lowest performing cohorts. The query written for generating a cohort chart is divided into 4 parts: 1. Date (Bucket) 2. Stage 3. Bucket Population Size 4. Stage Value In the cohort chart shown above, the 4 parts are as follows: Sample SQL query written for a cohort: /* Collecting 'Date Bucket' data */ with account_transaction as ( select brand_name , min ( date ( order_date )) as first_transaction , date_trunc ( 'month' , min ( date ( order_date ))) as month_cohort FROM hive . public_set01 . orders_enriched_view where date_format ( order_date , '%Y' ) = '2020' group by 1 ), /* Collecting 'Bucket Population Size' data */ orders_per_cohort as ( select month_cohort , count ( * ) as orders_count from account_transaction group by 1 ), months_after_first_purchase as ( select ov . customer_id , at . month_cohort , date_diff ( 'month' , at . first_transaction , date ( ov . order_date )) AS months_after_purchase from hive . public_set01 . orders_enriched_view as ov left join account_transaction as at on ov . brand_name = at . brand_name where date_format ( ov . order_date , '%Y' ) = '2020' ), final as ( select maf . month_cohort , /* Date Bucket */ uc . orders_count , /* Bucket Population Size */ maf . months_after_purchase , /* Stage */ count ( * ) as account_count_per_cohort /* Stage Value */ from months_after_first_purchase as maf left join orders_per_cohort as uc on maf . month_cohort = uc . month_cohort where maf . months_after_purchase > 0 group by 1 , 2 , 3 order by 1 ) select * from final The query above describes the steps/ stages in which a cohort chart generating query is structured.","title":"Cohort"},{"location":"atlas/dashboards/","text":"Manage Permissions \u00b6 Any dashboard created by a user can be shared with another user, via the 'Manage Permission' option. Any user added, can edit the Dashboard i.e add/edit and delete visualizations, repanel the windows etc. However the author of the dashboard reserves the right to share the dashboard, and add other users.","title":"Dashboards"},{"location":"atlas/dashboards/#manage-permissions","text":"Any dashboard created by a user can be shared with another user, via the 'Manage Permission' option. Any user added, can edit the Dashboard i.e add/edit and delete visualizations, repanel the windows etc. However the author of the dashboard reserves the right to share the dashboard, and add other users.","title":"Manage Permissions"},{"location":"atlas/funnel/","text":"A funnel chart is a specialized chart type that demonstrates the flow of data through a business or sales process. The chart takes its name from its shape, which starts from a broad head and ends in a narrow neck. For instance, let the data be number of users going through different stages/ processes. The number of users at each step of the process are indicated from the funnel\u2019s width as it narrows. A simple funnel chart above describes users going through a process of registering themselves on a website after receiving an email. The steps depict the dipping user interest, or a user drop in general at different steps. Example SQL query: with total_orders as ( select count ( distinct order_id ) as allorders from public_set01 . orders_enriched_view ), total_orders_men as ( select count ( distinct order_id ) as allordersmen from public_set01 . orders_enriched_view Where department_name = 'Men' ), men_shoes as ( select count ( distinct order_id ) as menshoes from public_set01 . orders_enriched_view Where department_name = 'Men' and category_name like 'Mens S%' ), galaxy_shoes as ( select count ( distinct order_id ) as galaxy from public_set01 . orders_enriched_view Where department_name = 'Men' and category_name like 'Mens S%' and brand_name = 'Galaxy by Harvic' ) SELECT 'Orders - All' AS step , allorders as count_column from total_orders /* Step 1 */ union select 'Orders - Men' AS step , allordersmen as count_column from total_orders_men /* Step 2 */ union SELECT 'Orders - Men - Shoes' AS step , menshoes as count_column from men_shoes /* Step 3 */ union SELECT 'Orders - Men - Shoes - Galaxy' AS step , galaxy as count_column from galaxy_shoes /* Step 4 */ order by count_column desc The above query results in the following chart: The above query funnels the orders for Mens Shoes of brand 'Galaxy' out of all orders.","title":"Funnel"},{"location":"atlas/sankey/","text":"A sankey diagram is a visualization used to depict relationships in a flow manner from one set of values to another. The things being connected are called nodes and the connections are called links . Sankeys are best used when you want to show a many-to-many mapping between two domains (e.g., universities and majors) or multiple paths through a set of stages (for instance, Google Analytics uses sankeys to show how traffic flows from pages to other pages on your web site). Example query: SELECT department_name AS e1 , category_name AS e2 , brand_name AS e3 , sum ( order_amount ) AS value FROM public_set01 . orders_enriched_view WHERE category_name LIKE 'Mens S%' GROUP BY 1 , 2 , 3 ORDER BY 4 DESC If you analyze this query in detail, it will give you a pattern which is needed for all queries to be written to generate it. Stage 1 Value: e1 (department_name) in the example Stage 2 Value: e2 (category_name) in the example Stage 3 Value: e3 (brand_name) in the example Value: value (Sum of order_amount) in the example Here the 'value' should always be named like such in the query, and their should be at least 2 stages to generate a Sankey diagram. It determines the count of 'Stage 1 - Stage 2 - Stage 3 - (and so on)' pattern. The results of the above query have been plot into a Sankey diagram as follows. The thickness of links shows the specific weight associated with it. In the example above, the sum of revenue generated by brand 'Galaxy by Harvic' for category 'Mens Shorts' is more than brand 'Reebok'. Hovering over any of the links highlights the connection/ relation between nodes.","title":"Sankey"},{"location":"atlas/sunburst/","text":"The sunburst chart is ideal for displaying hierarchical data. Each level of the hierarchy is represented by one ring or circle with the innermost circle as the top of the hierarchy. A sunburst chart without any hierarchical data (one level of categories), looks similar to a doughnut chart. Consider the following tree: This is portrayed in the following form using the surburst chart: Rings are sliced up and divided based on their hierarchical relationship to the parent slice. The angle of each slice is either divided equally under its parent node or can be made proportional to a value. Example query: SELECT department_name AS e1 , category_name AS e2 , brand_name AS e3 , sum ( order_amount ) AS value FROM public_set01 . orders_enriched_view WHERE category_name LIKE 'Mens S%' GROUP BY 1 , 2 , 3 ORDER BY 4 DESC If you analyze this query in detail, it will give you a pattern which is needed for all queries to be written to generate a sunburst chart. Stage 1 Value: e1 (department_name) in the example Stage 2 Value: e2 (category_name) in the example Stage 3 Value: e3 (brand_name) in the example Value: value (Sum of order_amount) in the example Their should be at least 2 stages to generate a Surburst chart (a sunburst chart with only 1 stage is a pie chart). Here the 'value' should always be named like such in the query. It determines the number of time 'Stage 1 - Stage 2 - Stage 3 - (and so on)' pattern is repeated. The example query, taken to plot a sunburst chart here. Here 'e1' or the department_name is the top of the hierarchy. The flow depicted here is as follows: Here the 21.6% value denotes the share of 'Men-Mens Sweatshirts and Hoodies-Shop4Ever' combination.","title":"Sunburst"},{"location":"atlas/treemap/","text":"Treemap charts are good for comparing proportions within the hierarchy, however, treemap charts aren't great at showing hierarchical levels between the largest categories and each data point. A sunburst chart is a much better visual chart for showing that. The treemap chart is used for representing hierarchical data in a tree-like structure. Data, organized as branches and sub-branches, is represented using rectangles and the dimensions which are calculated w.r.t the quantitative variables associated with each rectangle\u2014each rectangle represents two numerical values. This makes the at-a-glance distinguishing between categories and data values easy. In the above example, Baby Clothing category contributes a major part of the overall revenue of the Kids Clothing department, followed by Boys Clothing and Girls Clothing respectively. Example query: SELECT department_name AS e1 , category_name AS e2 , sum ( order_amount ) AS value FROM public_set01 . orders_enriched_view GROUP BY 1 , 2 ORDER BY 3 DESC If you analyze this query in detail, it will give you a pattern which is needed for all queries to be written to generate a treemap chart. Stage 1 Value: e1 (department_name) in the example Stage 2 Value: e2 (category_name) in the example Value: value (Sum of order_amount) in the example There should be at least 2 stages to generate a Treemap chart. Here the 'value' should always be named like such in the query. It determines the number of time 'Stage 1 - Stage 2 - Stage 3 - (and so on)' pattern is repeated. It is depicted, as shown above, by the different sizes of e2 (category_name)","title":"Treemap"},{"location":"atlas/visualizations/","text":"You can visualize the query results by clicking the \u201c New Visualization \u201d button above the results table. Edit Visualization \u00b6 You can modify the settings of an existing visualization from the query editor screen. Clicking the 'Edit Visualization' will open the current settings for that visualization (type, X axis, Y axis, groupings etc.). Data Labels \u00b6 Various visualizations may need the nubers to be formatted in different manners in atlas. Here is a quick reference to how these formats can be achieved: NUMBERS Number Format Output 10000 '0,0.0000' 10,000.0000 10000.23 '0,0' 10,000 10000.23 '+0,0' +10,000 -10000 '0,0.0' -10000.0 10000.1234 \u20180.000\u2019 10000.123 100.1234 \u201800000\u2019 00100 1000.1234 \u2018000000,0\u2019 001,000 10 \u2018000.00\u2019 010.00 10000.1234 \u20180[.]00000\u2019 10000.12340 -10000 \u2018(0,0.0000)\u2019 (10,000.0000) -0.23 \u2018.00\u2019 -.23 -0.23 \u2018(.00)\u2019 (.23) 0.23 \u20180.00000\u2019 0.23000 0.23 \u20180.0[0000]\u2019 0.23 1230974 \u20180.0a\u2019 1.2m 1460 \u20180 a\u2019 1 k -104000 \u20180a\u2019 -104k 1 \u20180o\u2019 1 st 100 \u20180o\u2019 100 th _ _ _ _ _ _ _ CURRENCY Number Format Output 1000.234 \u2019$0,0.00\u2019 $1,000.23 1000.2 \u20180,0[.]00 $\u2019 1,000.20 $ 1001 \u2019$ 0,0[.]00\u2019 $ 1,001 -1000.234 \u2018($0,0)\u2019 ($1,000) -1000.234 \u2019$0.00\u2019 -$1000.23 1230974 \u2018($ 0.00 a)\u2019 $ 1.23 m _ _ _ _ _ _ _ DATA SIZE Number Format Output 100 \u20180b\u2019 100B 1024 \u20180b\u2019 1KB 2048 \u20180 ib\u2019 2 KiB 3072 \u20180.0 b\u2019 3.1 KB 7884486213 \u20180.00b\u2019 7.88GB 3467479682787 \u20180.000 ib\u2019 3.154 TiB _ _ _ _ _ _ _ PERCENTAGES Number Format Output 100 \u20180%\u2019 100% 97.4878234 \u20180.000%\u2019 97.488% -4.3 \u20180 %\u2019 -4 % 65.43 \u2018(0.000 %)\u2019 65.430 % _ _ _ _ _ _ _ EXPONENTIAL Number Format Output 1123456789 \u20180,0e+0\u2019 1e+9 12398734.202 \u20180.00e+0\u2019 1.24e+7 0.000123987 \u20180.000e+0\u2019 1.240e-4","title":"Visualizations"},{"location":"atlas/visualizations/#edit-visualization","text":"You can modify the settings of an existing visualization from the query editor screen. Clicking the 'Edit Visualization' will open the current settings for that visualization (type, X axis, Y axis, groupings etc.).","title":"Edit Visualization"},{"location":"atlas/visualizations/#data-labels","text":"Various visualizations may need the nubers to be formatted in different manners in atlas. Here is a quick reference to how these formats can be achieved: NUMBERS Number Format Output 10000 '0,0.0000' 10,000.0000 10000.23 '0,0' 10,000 10000.23 '+0,0' +10,000 -10000 '0,0.0' -10000.0 10000.1234 \u20180.000\u2019 10000.123 100.1234 \u201800000\u2019 00100 1000.1234 \u2018000000,0\u2019 001,000 10 \u2018000.00\u2019 010.00 10000.1234 \u20180[.]00000\u2019 10000.12340 -10000 \u2018(0,0.0000)\u2019 (10,000.0000) -0.23 \u2018.00\u2019 -.23 -0.23 \u2018(.00)\u2019 (.23) 0.23 \u20180.00000\u2019 0.23000 0.23 \u20180.0[0000]\u2019 0.23 1230974 \u20180.0a\u2019 1.2m 1460 \u20180 a\u2019 1 k -104000 \u20180a\u2019 -104k 1 \u20180o\u2019 1 st 100 \u20180o\u2019 100 th _ _ _ _ _ _ _ CURRENCY Number Format Output 1000.234 \u2019$0,0.00\u2019 $1,000.23 1000.2 \u20180,0[.]00 $\u2019 1,000.20 $ 1001 \u2019$ 0,0[.]00\u2019 $ 1,001 -1000.234 \u2018($0,0)\u2019 ($1,000) -1000.234 \u2019$0.00\u2019 -$1000.23 1230974 \u2018($ 0.00 a)\u2019 $ 1.23 m _ _ _ _ _ _ _ DATA SIZE Number Format Output 100 \u20180b\u2019 100B 1024 \u20180b\u2019 1KB 2048 \u20180 ib\u2019 2 KiB 3072 \u20180.0 b\u2019 3.1 KB 7884486213 \u20180.00b\u2019 7.88GB 3467479682787 \u20180.000 ib\u2019 3.154 TiB _ _ _ _ _ _ _ PERCENTAGES Number Format Output 100 \u20180%\u2019 100% 97.4878234 \u20180.000%\u2019 97.488% -4.3 \u20180 %\u2019 -4 % 65.43 \u2018(0.000 %)\u2019 65.430 % _ _ _ _ _ _ _ EXPONENTIAL Number Format Output 1123456789 \u20180,0e+0\u2019 1e+9 12398734.202 \u20180.00e+0\u2019 1.24e+7 0.000123987 \u20180.000e+0\u2019 1.240e-4","title":"Data Labels"},{"location":"catalog/","text":"DataOS \u00ae Catalog \u00b6 Introduction \u00b6 Purpose \u00b6 The purpose of the Catalog is to help you easily find data within DataOS, to understand the events that have affected the data and to understand the quality of the data. When you first enter the Catalog screen, you see a search panel on the left side and a list of data sets and jobs on the right side. The list on the right side includes all of the data sets in DataOS and all of the jobs that were run against those data sets. When you have found a data set or job that you are interested in, clicking on the Name will drill into that data set or job and provide lots of detailed information. Data Sets and Jobs Panel \u00b6 The list panel on the right side of the page displays all of the data sets in DataOS and all of the jobs that were run on the data sets. The list can contain as many as two types of items. The identifiers for these two types are: Dataset Job Dataset - Dataset is the identifier given to a dataset that has been run through some kind of processing in a DataOS\u00ae job (for example, transformations, corrections, etc.). These jobs are usually designed to improve the usability of the data. Post-processing it is usually the case that some or much of the data is not exactly the same as it was in the original source. Job - Job is the identifier given to any DataOS\u00ae job i.e data ingestion, data processing, etc. Sorting - The list can be sorted using this pulldown menu. Name - This is the name of the dataset or job. This value is originally given to it by the user who created it. Clicking on the name field will drill into detail screens where this name can be edited. Description - This is the description given to the dataset or job. This value is originally given to it by the user who created it. It can be modified at any time. Updated Date - This is the last updated date of either the record metadata or the data in the dataset. Owner - This is the owner that was assigned to the data set or job. This value can be edited by a user and is separate from the Created By value. Labels - Labels can be added to datasets, jobs and columns in the dictionary after the data has been ingested by DataOS\u00ae. These labels can be used when searching for specific data sets and when creating workflows. Filter Panel \u00b6 The filter panel on the left side of the page is used to search for and quickly find data sets or jobs that you are interested in. There are five sections in the filter panel that provide different features for narrowing your search. All searches are not case sensitive unless you put the search word or words in single quotes. This is explained in more detail in the Exact Match section below. Advanced Search \u00b6 At the top of the search panel there is a data entry field where you can enter text to search for data sets and jobs. This field acts similar to the search field in Google. You simply enter text, hit enter and the results of your search are displayed on the right. There are functions that you can use in your search to create more complex queries. Exact Match And Or Exclude Wildcard Exact Match - To return data sets and jobs that have an exact word or phrase in them, put single quotes around the word or phrase, for example, \u2018customer 360\u2019. This will return data sets or jobs that contain the exact phrase and it will prevent data sets or jobs with similar words or close matches from being returned (known as fuzzy matching). And - By default, multiple search words are ANDed together without using any special syntax or functions. Searching for customer transactions will return data sets or jobs that have both the words \u201ccustomer\u201d and \u201ctransactions\u201d in them. However, there are times when you would want to specifically use the AND function, like when combining functions in a search, for example, customer AND (transactions OR txn). Both the ampersand symbol \u201c&\u201d and the word AND perform the same function. The AND function does not need to be uppercase. It is only used here to distinguish the function from the other text. Or - Both the pipe symbol \u201c|\u201d and the word OR perform the OR function between words. Searching for transactions OR txn or alternatively for transactions | txn will return data sets and jobs that contain the word \u201ctransactions\u201d or \u201ctxn\u201d. Exclusion - A dash or minus sign \u201c-\u201d can be used to return data sets or jobs that do not contain a word, for example, customer -transactions. This search will return data sets or jobs that contain the word \u201ccustomer\u201d and not the word \u201ctransactions\u201d. Wildcard - A colon followed by an asterisk '*' can be used at the end of a word as a wildcard character to find words beginning with the text entered. Searching for Custom:* will return data sets or jobs that contain words including \u201cCustom\u201d, \u201cCustomer\u201d, \u201cCustomers\u201d, etc. Type \u00b6 You may only want to search for data sets or you may only want to search for jobs. To include or exclude either one, you simply select or deselect their respective checkboxes under the \u201cType\u201d heading. Selecting \u201cDATA\u201d will display all data sets. Deselesting \u201cDATA\u201d will prevent data sets from being displayed in the list panel. Selecting \u201cJOB\u201d will display jobs. Deselecting \u201cJOB\u201d will prevent jobs from being displayed in the list panel. These types are not user definable.","title":"DataOS<sup>\u00ae</sup> Catalog"},{"location":"catalog/#dataos-catalog","text":"","title":"DataOS\u00ae Catalog"},{"location":"catalog/#introduction","text":"","title":"Introduction"},{"location":"catalog/#purpose","text":"The purpose of the Catalog is to help you easily find data within DataOS, to understand the events that have affected the data and to understand the quality of the data. When you first enter the Catalog screen, you see a search panel on the left side and a list of data sets and jobs on the right side. The list on the right side includes all of the data sets in DataOS and all of the jobs that were run against those data sets. When you have found a data set or job that you are interested in, clicking on the Name will drill into that data set or job and provide lots of detailed information.","title":"Purpose"},{"location":"catalog/#data-sets-and-jobs-panel","text":"The list panel on the right side of the page displays all of the data sets in DataOS and all of the jobs that were run on the data sets. The list can contain as many as two types of items. The identifiers for these two types are: Dataset Job Dataset - Dataset is the identifier given to a dataset that has been run through some kind of processing in a DataOS\u00ae job (for example, transformations, corrections, etc.). These jobs are usually designed to improve the usability of the data. Post-processing it is usually the case that some or much of the data is not exactly the same as it was in the original source. Job - Job is the identifier given to any DataOS\u00ae job i.e data ingestion, data processing, etc. Sorting - The list can be sorted using this pulldown menu. Name - This is the name of the dataset or job. This value is originally given to it by the user who created it. Clicking on the name field will drill into detail screens where this name can be edited. Description - This is the description given to the dataset or job. This value is originally given to it by the user who created it. It can be modified at any time. Updated Date - This is the last updated date of either the record metadata or the data in the dataset. Owner - This is the owner that was assigned to the data set or job. This value can be edited by a user and is separate from the Created By value. Labels - Labels can be added to datasets, jobs and columns in the dictionary after the data has been ingested by DataOS\u00ae. These labels can be used when searching for specific data sets and when creating workflows.","title":"Data Sets and Jobs Panel"},{"location":"catalog/#filter-panel","text":"The filter panel on the left side of the page is used to search for and quickly find data sets or jobs that you are interested in. There are five sections in the filter panel that provide different features for narrowing your search. All searches are not case sensitive unless you put the search word or words in single quotes. This is explained in more detail in the Exact Match section below.","title":"Filter Panel"},{"location":"catalog/#advanced-search","text":"At the top of the search panel there is a data entry field where you can enter text to search for data sets and jobs. This field acts similar to the search field in Google. You simply enter text, hit enter and the results of your search are displayed on the right. There are functions that you can use in your search to create more complex queries. Exact Match And Or Exclude Wildcard Exact Match - To return data sets and jobs that have an exact word or phrase in them, put single quotes around the word or phrase, for example, \u2018customer 360\u2019. This will return data sets or jobs that contain the exact phrase and it will prevent data sets or jobs with similar words or close matches from being returned (known as fuzzy matching). And - By default, multiple search words are ANDed together without using any special syntax or functions. Searching for customer transactions will return data sets or jobs that have both the words \u201ccustomer\u201d and \u201ctransactions\u201d in them. However, there are times when you would want to specifically use the AND function, like when combining functions in a search, for example, customer AND (transactions OR txn). Both the ampersand symbol \u201c&\u201d and the word AND perform the same function. The AND function does not need to be uppercase. It is only used here to distinguish the function from the other text. Or - Both the pipe symbol \u201c|\u201d and the word OR perform the OR function between words. Searching for transactions OR txn or alternatively for transactions | txn will return data sets and jobs that contain the word \u201ctransactions\u201d or \u201ctxn\u201d. Exclusion - A dash or minus sign \u201c-\u201d can be used to return data sets or jobs that do not contain a word, for example, customer -transactions. This search will return data sets or jobs that contain the word \u201ccustomer\u201d and not the word \u201ctransactions\u201d. Wildcard - A colon followed by an asterisk '*' can be used at the end of a word as a wildcard character to find words beginning with the text entered. Searching for Custom:* will return data sets or jobs that contain words including \u201cCustom\u201d, \u201cCustomer\u201d, \u201cCustomers\u201d, etc.","title":"Advanced Search"},{"location":"catalog/#type","text":"You may only want to search for data sets or you may only want to search for jobs. To include or exclude either one, you simply select or deselect their respective checkboxes under the \u201cType\u201d heading. Selecting \u201cDATA\u201d will display all data sets. Deselesting \u201cDATA\u201d will prevent data sets from being displayed in the list panel. Selecting \u201cJOB\u201d will display jobs. Deselecting \u201cJOB\u201d will prevent jobs from being displayed in the list panel. These types are not user definable.","title":"Type"},{"location":"cli/","text":"About DataOS CLI \u00b6 The DataOS Command Line Interface (DataOS CLI) enables you to interact with DataOS environment/services using commands in the command-line shell. Features/Capabilities \u00b6 DataOS CLI has several commands that enable you to install/deploy DataOS components, manage and monitor the resources/contexts and view logs. Using CLI commands, you can deploy data pipelines. For example, you can run Flare jobs to extract data from various data sources and load into the targets such as DataOS Depots. Getting started \u00b6 Installation - Refer to CLI Installation Document Log in to CLI dataos-ctl login","title":"Introduction"},{"location":"cli/#about-dataos-cli","text":"The DataOS Command Line Interface (DataOS CLI) enables you to interact with DataOS environment/services using commands in the command-line shell.","title":"About DataOS CLI"},{"location":"cli/#featurescapabilities","text":"DataOS CLI has several commands that enable you to install/deploy DataOS components, manage and monitor the resources/contexts and view logs. Using CLI commands, you can deploy data pipelines. For example, you can run Flare jobs to extract data from various data sources and load into the targets such as DataOS Depots.","title":"Features/Capabilities"},{"location":"cli/#getting-started","text":"Installation - Refer to CLI Installation Document Log in to CLI dataos-ctl login","title":"Getting started"},{"location":"cli/cli/","text":"DataOS Command Line Interface \u00b6 Structure of the DataOS CLI Command \u00b6 Command subcommand flags parameters List of DataOS CLI Commands \u00b6 You can generate a list of all available commands with -h. dataos-ctl --help To get help for specific command use: dataos-ctl command --help Apply \u00b6 Create and update resources in a DataOS cluster through running apply . This command manages applications through .yaml files defining DataOS resources. You can use apply to recursively create and update DataOS objects as needed. Usage: dataos-ctl apply [flags] Flags: -h, --help help for apply -l, --lint Lint the files, do not apply -f, --manifestFile string Manifest file location -o, --overrideWarnings Override the warnings and apply -R, --recursive Get manifest files recursively from the provided directory -w, --workspace string Workspace to target resource (default \"public\") Completion \u00b6 This mode places you in an interactive mode with auto-completion for the given shell (zsh or bash). To setup autocomplete in current bash shell, bash-completion package should be installed first. zsh: option 1 (speedy prompt startup time): $ dataos-ctl completion zsh > ~/.dataos/.dataos-ctl-completion # for zsh users $ source ~/.dataos/.dataos-ctl-completion option 2 (always gets current commands): $ source <(dataos-ctl completion zsh) # for zsh users bash: This depends on the bash-completion binary. Example installation instructions: OS X: $ brew install bash-completion $ source $(brew --prefix)/etc/bash_completion $ dataos-ctl completion bash > ~/.dataos/.dataos-ctl-completion # for bash users $ source ~/.dataos/.dataos-ctl-completion Ubuntu: $ apt-get install bash-completion $ source /etc/bash-completion $ source <(dataos-ctl completion bash) Additionally, you may want to output the completion to a file and source in your .bashrc or .zshrc Usage: dataos-ctl completion SHELL [flags] Flags: -h, --help help for completion Context \u00b6 Manage DataOS Contexts. A Context represents the connection to a DataOS cluster/instance/environment and can be used to create depots, jobs, queries etc on that cluster. Note : Only one context can be active. Usage dataos-ctl context [command] Available Commands: delete Delete DataOS Context list List DataOS contexts select Select DataOS Context Flags: -h, --help help for context Use \"dataos-ctl context [command] --help\" for more information about a command. Context List \u00b6 Get the list of environments you have initialized using the dataos-ctl init command. The active context will be shown with a star and you can see its URL in the output. Usage: dataos-ctl context select [flags] Flags: -h, --help help for select -n, --name string name of context to select Here is the expected output: \u279c ~ dataos-ctl context list INFO[0000] * meerkat INFO[0000] squirrel INFO[0000] wholly-merry-orca INFO[0000] \ud83d\udd17...https://cleanly-mutual-meerkat.dataos.io Context Delete \u00b6 Delete the context which is pointing to your DataOS environment. Usage: dataos-ctl context delete [flags] Flags: -h, --help help for delete -n, --name string name of context to select Context Select \u00b6 Select a new context to point to, from the available environments. Usage: dataos-ctl context select [flags] Flags: -h, --help help for select -n, --name string name of context to select Delete \u00b6 Delete resources in the DataOS. Usage: dataos-ctl delete [flags] Flags: -h, --help help for delete -i, --identifier string Identifier of resource, like: NAME:VERSION:TYPE -n, --name string Name of resource --namespace string Namespace to target resource (default \"default\") -t, --type string The resource type to get, possible values: depot, function, job, policy, service, secret DataOS checks resource dependability while deleting resources. Datanet \u00b6 Manage the Datanet of the DataOS. Usage: dataos-ctl datanet [command] Aliases: datanet, catalog Available Commands: create Create Entry in DataOS Catalog dataset Manage Datasets in the DataOS Catalog get Get Entry heartbeat Send Workflow Run Entry Heartbeat lifecycle Manage Run Entry Lifecycle map-stack-run Map the Stack Run to a Run Entry Flags: -h, --help help for datanet Use \"dataos-ctl datanet [command] --help\" for more information about a command. Datenet Create \u00b6 Create Entry in the DataOS catalog. Usage: dataos-ctl datanet create [flags] Flags: -h, --help help for create --idOutputFile string Id output file location -f, --manifestFile string Manifest file location -o, --outputFile string Yaml output file location --parentRunId string Parent Run Id to associate with the Child Catalog Entry -p, --printRequest Print the request as yaml -t, --type string Type of entry: workflow, workflow_run, job, job_run, service, service_run, topology -w, --workspace string Workspace to target catalog entries (default \"public\") Datanet Dataset \u00b6 Manage datasets in the DataOS catalog. Usage: dataos-ctl datanet dataset [command] Available Commands: create Create Entry in DataOS Catalog get Get Entry versions Dataset Versions Entry Flags: -h, --help help for dataset Use \"dataos-ctl datanet dataset [command] --help\" for more information about a command. Datanet Dataset Create \u00b6 Create Entry in the DataOS catalog. Usage: dataos-ctl datanet dataset create [flags] Flags: -h, --help help for create --idOutputFile string Id output file location -f, --manifestFile string Manifest file location -o, --outputFile string Yaml output file location -p, --printRequest Print the request as yaml Datanet Dataset Get \u00b6 Get Entry in the DataOS Catalog Usage: dataos-ctl datanet dataset get [flags] Flags: -a, --address string Dataset address -h, --help help for get -o, --outputFile string Yaml output file location -v, --version string Dataset version Datanet Dataset Versions \u00b6 Dataset Versions Entry in the DataOS Catalog. Usage: dataos-ctl datanet dataset versions [flags] Flags: -a, --address string Dataset address -h, --help help for versions Datanet Get \u00b6 Get Entry in the DataOS Catalog. Usage: dataos-ctl datanet get [flags] Flags: -h, --help help for get -n, --name string Name or ID of the entry -o, --outputFile string Yaml output file location -r, --runId string Entry run id -t, --type string Type of entry: workflow, workflow_run, job, job_run, service, service_run, topology -w, --workspace string Workspace to target catalog entries (default \"public\") Datanet Heartbeat \u00b6 Send workflow run entry heartbeat in the DataOS catalog. Usage: dataos-ctl datanet heartbeat [flags] Flags: -h, --help help for heartbeat -n, --name string Name of the entry -o, --outputFile string Yaml output file location -r, --runId string Entry run id -t, --type string Type of entry: workflow_run, job_run, service_run -w, --workspace string Workspace to target catalog entries (default \"public\") Datanet Lifecyle \u00b6 Manage Run Entry Lifecycle in the DataOS Catalog. Usage: dataos-ctl datanet lifecycle [flags] Flags: -h, --help help for lifecycle -m, --message string Run state transition message -n, --name string Name of the entry -o, --outputFile string Yaml output file location -r, --runId string Entry run id -s, --state string Run state target transition, can be: start, complete, abort, fail -t, --type string Type of entry: workflow_run, job_run, service_run -w, --workspace string Workspace to target catalog entries (default \"public\") Datanet Map-Stack-Run \u00b6 Map the stack run to a run entry in the DataOS catalog. Usage: dataos-ctl datanet map-stack-run [flags] Flags: -h, --help help for map-stack-run -n, --name string Name of the entry -r, --runId string Entry run id -s, --stackRunId string Stack run id -v, --topologyVersion string Stack run topology version -t, --type string Type of entry: workflow_run, job_run, service_run -w, --workspace string Workspace to target catalog entries (default \"public\") Develop \u00b6 With this command , manage DataOS Development. You can test the changes on the local machine before directly aplying on the server. Usage: dataos-ctl develop [command] Available Commands: apply Apply development resources get Get development containers get-stack-version Get stack versions log-stream Create development log-stream port-forward Create development port-forward start Start development container stop Stop development containers Flags: -h, --help help for develop Use \"dataos-ctl develop [command] --help\" for more information about a command. Develop Create \u00b6 Create a development container. Usage: dataos-ctl develop create [flags] Flags: -d, --dataDir string Directory containing the data -h, --help help for create -f, --manifestFile string Manifest file location -s, --stack string Job stack Develop Get \u00b6 Get running development containers. Usage: dataos-ctl develop get [flags] Flags: -h, --help help for get Develop Port-Forward \u00b6 Create development port-forward. Usage: dataos-ctl develop port-forward [flags] Flags: -h, --help help for port-forward -n, --name string Name of the Resource --pod Forward specific pod directly -p, --port string Port mapping of the Resource (default \"3000:3000\") -t, --type string Type of the Resource (default \"service\") -w, --workspace string Workspace of the Resource (default \"public\") Develop Stop \u00b6 Stop running development containers. Usage: dataos-ctl develop stop [flags] Flags: -i, --containerId string Container ID -h, --help help for stop Get \u00b6 Use get to pull a list of resources you have currently on your DataOS cluster. The types of resources you can get include-depot, function, job, policy, service, secret. Usage: dataos-ctl get [flags] Flags: -d, --details Set to true to include details in the result -h, --help help for get -i, --identifier string Identifier of resource, like: NAME:VERSION:TYPE -n, --name string Name to query -r, --runtime Set to true to include runtime details in the result --tags Set to true to include tags in the result -t, --type string The resource type to get. Workspace resources: workflow, service, secret. Cluster resources: depot, policy. -w, --workspace string Workspace to query Examples: dataos-ctl -t workflow -w public -n quality-checks-test-cases get dataos-ctl -i \"quality-checks-test-cases | v1beta1 | workflow | public\" get Output: INFO[0000] \ud83d\udd0d workflow... INFO[0002] \ud83d\udd0d workflow...complete NAME | VERSION | TYPE | WORKSPACE | TITLE | OWNER ----------------------------|---------|----------|-----------|----------------|---------------------- quality-checks-test-cases | v1beta1 | workflow | public | Quality-Checks | rakeshvishvakarma21 JOB NAME | STACK | JOB TITLE | JOB DEPENDENCIES -----------------------------|------------|-------------------------|------------------------- dataos-tool-quality-checks | toolbox | | quality-checks-summary quality-checks-summary | flare:1.0 | quality-checks datasets | system | dataos_cli | System Runnable Steps | SCHEDULED RUNTIME | LAST SCHEDULED TIME --------------------|---------------------------- RUNNING | 2021-11-01T16:30:00+05:30 RUNTIME | PROGRESS | STARTED | FINISHED ----------|----------|---------------------------|----------- running | 2/3 | 2021-11-01T16:30:00+05:30 | NODE NAME | JOB NAME | POD NAME | TYPE | CONTAINERS | PHASE ----------------------------------------|------------------------|------------------------------------------------------|--------------|-------------------------|------------ quality-checks-summary-bviw-driver | quality-checks-summary | quality-checks-summary-bviw-driver | pod-flare | spark-kubernetes-driver | running Get Runtime \u00b6 Get the runtime details of a resource in the DataOS. Usage: dataos-ctl get [flags] Flags: -d, --details Set to true to include details in the result -h, --help help for get -i, --identifier string Identifier of resource, like: NAME:VERSION:TYPE -n, --name string Name to query -r, --runtime Set to true to include runtime details in the result --tags Set to true to include tags in the result -t, --type string The resource type to get. Workspace resources: workflow, service, secret. Cluster resources: depot, policy. -w, --workspace string Workspace to query Examples: dataos-ctl get runtime -t depot -n icebase dataos-ctl get runtime -w system -t cluster -n minervaa dataos-ctl get runtime -w system -t service -n iosa-receiver dataos-ctl get runtime -w public -t workflow -n cnt-city-demo-01 dataos-ctl -i \"quality-checks-test-cases | v1beta1 | workflow | public\" get runtime You can also provide a string to get the runtime information. dataos-ctl -i \"quality-checks-test-cases | v1beta1 | workflow | public\" get runtime Output: INFO[0000] \ud83d\udd0d workflow... INFO[0002] \ud83d\udd0d workflow...complete NAME | VERSION | TYPE | WORKSPACE | TITLE | OWNER ----------------------------|---------|----------|-----------|----------------|---------------------- quality-checks-test-cases | v1beta1 | workflow | public | Quality-Checks | rakeshvishvakarma21 JOB NAME | STACK | JOB TITLE | JOB DEPENDENCIES -----------------------------|------------|-------------------------|------------------------- dataos-tool-quality-checks | toolbox | | quality-checks-summary quality-checks-summary | flare:1.0 | quality-checks datasets | system | dataos_cli | System Runnable Steps | SCHEDULED RUNTIME | LAST SCHEDULED TIME --------------------|---------------------------- RUNNING | 2021-11-01T14:30:00+05:30 RUNTIME | PROGRESS | STARTED | FINISHED ----------|----------|---------------------------|----------- running | 2/3 | 2021-11-01T14:30:00+05:30 | NODE NAME | JOB NAME | POD NAME | TYPE | CONTAINERS | PHASE ----------------------------------------|------------------------|------------------------------------------------------|--------------|-------------------------|------------ quality-checks-summary-bviw-driver | quality-checks-summary | quality-checks-summary-bviw-driver | pod-flare | spark-kubernetes-driver | running quality-checks-summary-execute | quality-checks-summary | quality-checks-test-cases-bviw-1635757200-996077945 | pod-workflow | main | running quality-checks-summary-start-rnnbl | quality-checks-summary | -checks-test-cases-bviw-1635757200-3571325227 | These commands get the runtime info and display the \"node\"s that are involved, then you can get the details of a specific node which then gives you the pod details of that node: dataos-ctl -i \"quality-checks-test-cases | v1beta1 | workflow | public\" --node quality-checks-summary-bviw-driver get runtime Output: INFO[0000] \ud83d\udd0d node... INFO[0003] \ud83d\udd0d node...complete NODE NAME | POD NAME | IMAGE PULL SECRETS | PHASE -------------------------------------|------------------------------------|---------------------------|------------ quality-checks-summary-bviw-driver | quality-checks-summary-bviw-driver | dataos-container-registry | Succeeded CONTAINER NAME | CONTAINER IMAGE | CONTAINER IMAGE PULL POLICY --------------------------|----------------------------------|------------------------------ spark-kubernetes-driver | docker.io/rubiklabs/flare:5.5.48 | IfNotPresent POD CONDITION TYPE | POD CONDITION STATUS | MESSAGE | TIME ---------------------|----------------------|---------|---------------------------- Initialized | True | | 2021-11-01T15:45:26+05:30 PodScheduled | True | | 2021-11-01T15:45:26+05:30 Ready | False | | 2021-11-01T15:47:25+05:30 ContainersReady | False | | 2021-11-01T15:47:25+05:30 CONTAINER NAME | CONTAINER STATE | STARTED | FINISHED --------------------------|--------------------------------|---------------------------|---------------------------- spark-kubernetes-driver | Terminated Reason:Completed | 2021-11-01T15:45:28+05:30 | 2021-11-01T15:47:25+05:30 | Message: ExitCode: 0 | | EVENT TYPE | EVENT REASON | EVENT SOURCE | EVENT MESSAGE | LAST OCCURRED | COUNT -------------|--------------|-------------------|-----------------------------------------------------|---------------------------|-------- Warning | FailedMount | kubelet | MountVolume.SetUp failed for | 2021-11-01T15:00:31+05:30 | 1 | | | volume \"spark-conf-volume-driver\" | | | | | : configmap | | | | | \"spark-drv-9f60d37cdad602e6-conf-map\" | | | | | not found | | Health \u00b6 Get health of DataOS CLI, DataOS resources and services. It checks if server is reachable and helps in troubleshooting. Usage: dataos-ctl health [flags] Flags: -h, --help help for health Here is the expected output of this command: % dataos-ctl health INFO[0000] \ud83c\udfe5... INFO[0000] DataOS\u00ae CLI...OK INFO[0005] DataOS\u00ae CK...OK INFO[0005] \ud83c\udfe5...complete INFO[0005] \ud83d\udd17...https://formerly-saving-lynx.dataos.io INFO[0005] \u26c5\ufe0f...gcp Help \u00b6 Get help for any command in the application. Usage: dataos-ctl help [command] [flags] Flags: -h, --help help for help Init \u00b6 Initialize the DataOS environment. Usage: dataos-ctl init [flags] Flags: -h, --help help for init Log \u00b6 Get the logs for a resource in the DataOS. Usage: dataos-ctl log [flags] Flags: -f, --follow Follow the logs -h, --help help for log -i, --identifier string Identifier of resource, like: NAME:VERSION:TYPE -r, --includeRunnable Include runnable system pods and logs -n, --name string Name to query -t, --type string The resource type to get, possible values: service, workflow -w, --workspace string Workspace to query (default \"public\") Examples: The log command has been updated to pass a \"node\" as well as to support getting logs for \"cluster\" and \"depot\" types that have runtimes. If you don't pass a \"node\" to the logs command it will try to display all the \"main\" logs for all nodes. dataos-ctl log -w public -t workflow -n cnt-city-demo-01 --node city-execute dataos-ctl log -w system -t cluster -n minervab --node minervab-ss-0 You can also pass the \"-i\" command with the string to get the logs. dataos-ctl -i \"quality-checks-test-cases | v1beta1 | workflow | public\" --node quality-checks-summary-bviw-driver log Output: INFO[0000] \ud83d\udcc3 log(public)... INFO[0003] \ud83d\udcc3 log(public)...complete NODE NAME | CONTAINER NAME | ERROR -------------------------------------|-------------------------|-------- quality-checks-summary-bviw-driver | spark-kubernetes-driver | -------------------LOGS------------------- 2021-11-01 08:32:06,938 INFO [task-result-getter-1] o.a.s.s.TaskSetManager: Finished task 54.0 in stage 1.0 (TID 69) in 17 ms on 10.212.16.7 (executor 1) (67/200) 2021-11-01 08:32:06,954 INFO [dispatcher-CoarseGrainedScheduler] o.a.s.s.TaskSetManager: Starting task 57.0 in stage 1.0 (TID 71) (10.212.16.7, executor 1, partition 57, PROCESS_LOCAL, 4472 bytes) taskResourceAssignments Map() 2021-11-01 08:32:06,954 INFO [task-result-getter-2] o.a.s.s.TaskSetManager: Finished task 56.0 in stage 1.0 (TID 70) in 17 ms on 10.212.16.7 (executor 1) (68/200) ... ... ... you can also pass the \"-c\" command with the container name you want to see the logs for. Login \u00b6 Login to the DataOS. Usage: dataos-ctl login [flags] Flags: -h, --help help for login Maintenance \u00b6 Maintenance of the DataOS. Usage: dataos-ctl maintenance [command] Available Commands: collect-garbage collects garbage on the DataOS. Flags: -h, --help help for maintenance Use \"dataos-ctl maintenance [command] --help\" for more information about a command. Maintenance Collect-garbage \u00b6 Collect Garbage on the DataOS. Usage: dataos-ctl maintenance collect-garbage [flags] Flags: -d, --duration string The duration to calculate the age of resources that are eligible for garbage collection (default \"168h\") -h, --help help for collect-garbage -k, --kubeconfig string Kubeconfig file location -l, --layer string The layer to target in the DataOS, user|system (default \"user\") Operate \u00b6 Operate the DataOS. Usage: dataos-ctl operate [command] Available Commands: apply Apply manifest chart-export Exports a Helm Chart from a Chart Registry git Git component manifests install Install components ping Ping upgrade Upgrade components view View DataOS\u00ae Operator Services zip Zip install files Flags: -h, --help help for operate Use \"dataos-ctl operate [command] --help\" for more information about a command. Operate Apply \u00b6 Apply manifest on the DataOS. Usage: dataos-ctl operate apply [flags] Flags: -h, --help help for apply -f, --manifestFile string Single Manifest File Location -n, --namespace string Namespace Operate Chart-Export \u00b6 Exports a Helm Chart from a Chart Registry. Usage: dataos-ctl operate chart-export [flags] Flags: --accessKey string The AWS Access Key for ECR Chart Registry --accessSecret string The AWS Access Secret for ECR Chart Registry -c, --chart string The chart ref -d, --exportDir string The directory to export the Helm chart -h, --help help for chart-export --region string The AWS Region for ECR Chart Registry --registry string The AWS ECR Chart Registry Operate Get-Secret \u00b6 Gets a secret from Heimdall. Usage: dataos-ctl operate get-secret [flags] Flags: -h, --help help for get-secret -i, --id string The secret id Operate Git \u00b6 Git component manifests on the DataOS. Usage: dataos-ctl operate git [flags] Flags: -e, --email string Operator email -h, --help help for git -l, --localOnly Perform local only -n, --name string Operator name -p, --push Push changes -r, --resetGitDir Reset the local git directory Operate Install \u00b6 When you create a new server, you want to install new applications on the server. Use this command to install one or more applications/components on the server. Usage: dataos-ctl operate install [flags] Flags: -h, --help help for install -i, --imagesFile string Installation Images File Location -f, --installFile string Installation Manifest File Location -n, --noGitOps Do not push changes to the GitOps repo in DataOS\u00ae --oldReleaseManifest Use old install manifest format --renderOnly Render only -r, --replaceIfExists Replace existing resources -s, --secretsFile string Installation Secrets File Location --useExternalPostgresql Use external postgresql -v, --valuesFile string Installation Values File Location Operate Ping \u00b6 Usage: dataos-ctl operate ping [flags] Flags: -h, --help help for ping Operate Upgrade \u00b6 Upgrade components on the DataOS. Usage: dataos-ctl operate upgrade [flags] Flags: -h, --help help for upgrade -i, --imagesFile string Installation Images File Location -f, --installFile string Installation Manifest File Location --oldReleaseManifest Use old install manifest format -s, --secretsFile string Installation Secrets File Location --useExternalPostgresql Use external postgresql -v, --valuesFile string Installation Values File Location Operate View \u00b6 View DataOS Operator Services from the local machine without going to server. You can create a data pipe from server to local machine. Usage: dataos-ctl operate view [flags] Flags: -h, --help help for view -p, --localPort int The starting local port to port-forward services to (default 8081) -s, --servicesToPortForward strings The comma separated list of services to port-forward local: metis,cerebro,aurora-beanstalkd,git,prometheus, service-mesh,cruise-control,kibana,spark-history \u279c ~ dataos-ctl operate view -s metis INFO[0000] \ud83d\udcda metis view... INFO[0000] \ud83d\udd2d metis port-forward.. INFO[0003] close connections hit enter/return? INFO[0004] \ud83d\udd2d metis port-forward.. ready INFO[0004] : metis http://localhost:8081 Note : Config File \".dataos.ck.config\" should be present in the folder \"[/Users/[username]/.dataos/context]. TUI \u00b6 Dataos-ctl TUI is a Terminal User Interface for DataOS. It shows all the key resources deployed on the server. You can click on the resource menu to see the corresponding details in the Resource Summary section. You can view artefacts and Run time services/resources and and their YAML. You can also view logs for runtime. Usage: dataos-ctl tui [flags] Flags: -h, --help help for tui -w, --workspaces string list of workspaces to include, comma separated User \u00b6 Manage DataOS users. Usage: dataos-ctl user [command] Available Commands: apikey Manage a DataOS\u00ae User apikey delete Delete a user get Get users tag Manage DataOS\u00ae User's tags Flags: -h, --help help for user Use \"dataos-ctl user [command] --help\" for more information about a command. User Apikey \u00b6 Manage a DataOS user apikey. Usage: dataos-ctl user apikey [command] Available Commands: create Create an apikey for a user delete Delete the apikey for a user get Get the apikey for a user Flags: -h, --help help for apikey Use \"dataos-ctl user apikey [command] --help\" for more information about a command. User Delete \u00b6 Delete a user Usage: dataos-ctl user delete [flags] Flags: -h, --help help for delete -i, --id string Id of the user User Get \u00b6 Get users Usage: dataos-ctl user get [flags] Flags: -a, --all Get all users -h, --help help for get -i, --id string Id of the user User Tag \u00b6 Manage DataOS user's tags. Usage: dataos-ctl user tag [command] Available Commands: add Add tags to a user delete Delete tags from a user Flags: -h, --help help for tag Use \"dataos-ctl user tag [command] --help\" for more information about a command. User Tag Add \u00b6 Add tags to a user. Usage: dataos-ctl user tag add [flags] Flags: -h, --help help for add -i, --id string Id of the user -t, --tags strings The tags to add User Tag Delete \u00b6 Delete tags from a user. Usage: dataos-ctl user tag delete [flags] Flags: -h, --help help for delete -i, --id string Id of the user -t, --tags strings The tags to delete Version \u00b6 Print the version number of DataOS. Usage: dataos-ctl version [flags] Flags: -h, --help help for version View \u00b6 View core applications in the DataOS. Usage: dataos-ctl view [flags] Flags: -a, --application string The application to view in your default browser: apps, datanet, workbench, atlas -h, --help help for view Workspace \u00b6 Manage DataOS workspaces. Usage: dataos-ctl workspace [command] Available Commands: create Create workspace delete Delete workspaces get Get workspaces Flags: -h, --help help for workspace Use \"dataos-ctl workspace [command] --help\" for more information about a command. Workspace Create \u00b6 Create workspace. Usage: dataos-ctl workspace create [flags] Flags: -d, --description string workspace description -h, --help help for create --labels strings The workspace labels -l, --layer string workspace layer (default \"user\") -n, --name string workspace name --tags strings The workspace tags -v, --version string workspace version (default \"v1beta1\") Workspace Delete \u00b6 Delete workspaces. Usage: dataos-ctl workspace delete [flags] Flags: -h, --help help for delete -n, --name string workspace name Workspace Get \u00b6 Get workspaces. Usage: dataos-ctl workspace get [flags] Flags: -h, --help help for get -l, --layer string workspace layer (default \"user\")","title":"Reference"},{"location":"cli/cli/#dataos-command-line-interface","text":"","title":"DataOS Command Line Interface"},{"location":"cli/cli/#structure-of-the-dataos-cli-command","text":"Command subcommand flags parameters","title":"Structure of the DataOS CLI Command"},{"location":"cli/cli/#list-of-dataos-cli-commands","text":"You can generate a list of all available commands with -h. dataos-ctl --help To get help for specific command use: dataos-ctl command --help","title":"List of DataOS CLI Commands"},{"location":"cli/cli/#apply","text":"Create and update resources in a DataOS cluster through running apply . This command manages applications through .yaml files defining DataOS resources. You can use apply to recursively create and update DataOS objects as needed. Usage: dataos-ctl apply [flags] Flags: -h, --help help for apply -l, --lint Lint the files, do not apply -f, --manifestFile string Manifest file location -o, --overrideWarnings Override the warnings and apply -R, --recursive Get manifest files recursively from the provided directory -w, --workspace string Workspace to target resource (default \"public\")","title":"Apply"},{"location":"cli/cli/#completion","text":"This mode places you in an interactive mode with auto-completion for the given shell (zsh or bash). To setup autocomplete in current bash shell, bash-completion package should be installed first. zsh: option 1 (speedy prompt startup time): $ dataos-ctl completion zsh > ~/.dataos/.dataos-ctl-completion # for zsh users $ source ~/.dataos/.dataos-ctl-completion option 2 (always gets current commands): $ source <(dataos-ctl completion zsh) # for zsh users bash: This depends on the bash-completion binary. Example installation instructions: OS X: $ brew install bash-completion $ source $(brew --prefix)/etc/bash_completion $ dataos-ctl completion bash > ~/.dataos/.dataos-ctl-completion # for bash users $ source ~/.dataos/.dataos-ctl-completion Ubuntu: $ apt-get install bash-completion $ source /etc/bash-completion $ source <(dataos-ctl completion bash) Additionally, you may want to output the completion to a file and source in your .bashrc or .zshrc Usage: dataos-ctl completion SHELL [flags] Flags: -h, --help help for completion","title":"Completion"},{"location":"cli/cli/#context","text":"Manage DataOS Contexts. A Context represents the connection to a DataOS cluster/instance/environment and can be used to create depots, jobs, queries etc on that cluster. Note : Only one context can be active. Usage dataos-ctl context [command] Available Commands: delete Delete DataOS Context list List DataOS contexts select Select DataOS Context Flags: -h, --help help for context Use \"dataos-ctl context [command] --help\" for more information about a command.","title":"Context"},{"location":"cli/cli/#context-list","text":"Get the list of environments you have initialized using the dataos-ctl init command. The active context will be shown with a star and you can see its URL in the output. Usage: dataos-ctl context select [flags] Flags: -h, --help help for select -n, --name string name of context to select Here is the expected output: \u279c ~ dataos-ctl context list INFO[0000] * meerkat INFO[0000] squirrel INFO[0000] wholly-merry-orca INFO[0000] \ud83d\udd17...https://cleanly-mutual-meerkat.dataos.io","title":"Context List"},{"location":"cli/cli/#context-delete","text":"Delete the context which is pointing to your DataOS environment. Usage: dataos-ctl context delete [flags] Flags: -h, --help help for delete -n, --name string name of context to select","title":"Context Delete"},{"location":"cli/cli/#context-select","text":"Select a new context to point to, from the available environments. Usage: dataos-ctl context select [flags] Flags: -h, --help help for select -n, --name string name of context to select","title":"Context Select"},{"location":"cli/cli/#delete","text":"Delete resources in the DataOS. Usage: dataos-ctl delete [flags] Flags: -h, --help help for delete -i, --identifier string Identifier of resource, like: NAME:VERSION:TYPE -n, --name string Name of resource --namespace string Namespace to target resource (default \"default\") -t, --type string The resource type to get, possible values: depot, function, job, policy, service, secret DataOS checks resource dependability while deleting resources.","title":"Delete"},{"location":"cli/cli/#datanet","text":"Manage the Datanet of the DataOS. Usage: dataos-ctl datanet [command] Aliases: datanet, catalog Available Commands: create Create Entry in DataOS Catalog dataset Manage Datasets in the DataOS Catalog get Get Entry heartbeat Send Workflow Run Entry Heartbeat lifecycle Manage Run Entry Lifecycle map-stack-run Map the Stack Run to a Run Entry Flags: -h, --help help for datanet Use \"dataos-ctl datanet [command] --help\" for more information about a command.","title":"Datanet"},{"location":"cli/cli/#datenet-create","text":"Create Entry in the DataOS catalog. Usage: dataos-ctl datanet create [flags] Flags: -h, --help help for create --idOutputFile string Id output file location -f, --manifestFile string Manifest file location -o, --outputFile string Yaml output file location --parentRunId string Parent Run Id to associate with the Child Catalog Entry -p, --printRequest Print the request as yaml -t, --type string Type of entry: workflow, workflow_run, job, job_run, service, service_run, topology -w, --workspace string Workspace to target catalog entries (default \"public\")","title":"Datenet Create"},{"location":"cli/cli/#datanet-dataset","text":"Manage datasets in the DataOS catalog. Usage: dataos-ctl datanet dataset [command] Available Commands: create Create Entry in DataOS Catalog get Get Entry versions Dataset Versions Entry Flags: -h, --help help for dataset Use \"dataos-ctl datanet dataset [command] --help\" for more information about a command.","title":"Datanet Dataset"},{"location":"cli/cli/#datanet-dataset-create","text":"Create Entry in the DataOS catalog. Usage: dataos-ctl datanet dataset create [flags] Flags: -h, --help help for create --idOutputFile string Id output file location -f, --manifestFile string Manifest file location -o, --outputFile string Yaml output file location -p, --printRequest Print the request as yaml","title":"Datanet Dataset Create"},{"location":"cli/cli/#datanet-dataset-get","text":"Get Entry in the DataOS Catalog Usage: dataos-ctl datanet dataset get [flags] Flags: -a, --address string Dataset address -h, --help help for get -o, --outputFile string Yaml output file location -v, --version string Dataset version","title":"Datanet Dataset Get"},{"location":"cli/cli/#datanet-dataset-versions","text":"Dataset Versions Entry in the DataOS Catalog. Usage: dataos-ctl datanet dataset versions [flags] Flags: -a, --address string Dataset address -h, --help help for versions","title":"Datanet Dataset Versions"},{"location":"cli/cli/#datanet-get","text":"Get Entry in the DataOS Catalog. Usage: dataos-ctl datanet get [flags] Flags: -h, --help help for get -n, --name string Name or ID of the entry -o, --outputFile string Yaml output file location -r, --runId string Entry run id -t, --type string Type of entry: workflow, workflow_run, job, job_run, service, service_run, topology -w, --workspace string Workspace to target catalog entries (default \"public\")","title":"Datanet Get"},{"location":"cli/cli/#datanet-heartbeat","text":"Send workflow run entry heartbeat in the DataOS catalog. Usage: dataos-ctl datanet heartbeat [flags] Flags: -h, --help help for heartbeat -n, --name string Name of the entry -o, --outputFile string Yaml output file location -r, --runId string Entry run id -t, --type string Type of entry: workflow_run, job_run, service_run -w, --workspace string Workspace to target catalog entries (default \"public\")","title":"Datanet Heartbeat"},{"location":"cli/cli/#datanet-lifecyle","text":"Manage Run Entry Lifecycle in the DataOS Catalog. Usage: dataos-ctl datanet lifecycle [flags] Flags: -h, --help help for lifecycle -m, --message string Run state transition message -n, --name string Name of the entry -o, --outputFile string Yaml output file location -r, --runId string Entry run id -s, --state string Run state target transition, can be: start, complete, abort, fail -t, --type string Type of entry: workflow_run, job_run, service_run -w, --workspace string Workspace to target catalog entries (default \"public\")","title":"Datanet Lifecyle"},{"location":"cli/cli/#datanet-map-stack-run","text":"Map the stack run to a run entry in the DataOS catalog. Usage: dataos-ctl datanet map-stack-run [flags] Flags: -h, --help help for map-stack-run -n, --name string Name of the entry -r, --runId string Entry run id -s, --stackRunId string Stack run id -v, --topologyVersion string Stack run topology version -t, --type string Type of entry: workflow_run, job_run, service_run -w, --workspace string Workspace to target catalog entries (default \"public\")","title":"Datanet Map-Stack-Run"},{"location":"cli/cli/#develop","text":"With this command , manage DataOS Development. You can test the changes on the local machine before directly aplying on the server. Usage: dataos-ctl develop [command] Available Commands: apply Apply development resources get Get development containers get-stack-version Get stack versions log-stream Create development log-stream port-forward Create development port-forward start Start development container stop Stop development containers Flags: -h, --help help for develop Use \"dataos-ctl develop [command] --help\" for more information about a command.","title":"Develop"},{"location":"cli/cli/#develop-create","text":"Create a development container. Usage: dataos-ctl develop create [flags] Flags: -d, --dataDir string Directory containing the data -h, --help help for create -f, --manifestFile string Manifest file location -s, --stack string Job stack","title":"Develop Create"},{"location":"cli/cli/#develop-get","text":"Get running development containers. Usage: dataos-ctl develop get [flags] Flags: -h, --help help for get","title":"Develop Get"},{"location":"cli/cli/#develop-port-forward","text":"Create development port-forward. Usage: dataos-ctl develop port-forward [flags] Flags: -h, --help help for port-forward -n, --name string Name of the Resource --pod Forward specific pod directly -p, --port string Port mapping of the Resource (default \"3000:3000\") -t, --type string Type of the Resource (default \"service\") -w, --workspace string Workspace of the Resource (default \"public\")","title":"Develop Port-Forward"},{"location":"cli/cli/#develop-stop","text":"Stop running development containers. Usage: dataos-ctl develop stop [flags] Flags: -i, --containerId string Container ID -h, --help help for stop","title":"Develop Stop"},{"location":"cli/cli/#get","text":"Use get to pull a list of resources you have currently on your DataOS cluster. The types of resources you can get include-depot, function, job, policy, service, secret. Usage: dataos-ctl get [flags] Flags: -d, --details Set to true to include details in the result -h, --help help for get -i, --identifier string Identifier of resource, like: NAME:VERSION:TYPE -n, --name string Name to query -r, --runtime Set to true to include runtime details in the result --tags Set to true to include tags in the result -t, --type string The resource type to get. Workspace resources: workflow, service, secret. Cluster resources: depot, policy. -w, --workspace string Workspace to query Examples: dataos-ctl -t workflow -w public -n quality-checks-test-cases get dataos-ctl -i \"quality-checks-test-cases | v1beta1 | workflow | public\" get Output: INFO[0000] \ud83d\udd0d workflow... INFO[0002] \ud83d\udd0d workflow...complete NAME | VERSION | TYPE | WORKSPACE | TITLE | OWNER ----------------------------|---------|----------|-----------|----------------|---------------------- quality-checks-test-cases | v1beta1 | workflow | public | Quality-Checks | rakeshvishvakarma21 JOB NAME | STACK | JOB TITLE | JOB DEPENDENCIES -----------------------------|------------|-------------------------|------------------------- dataos-tool-quality-checks | toolbox | | quality-checks-summary quality-checks-summary | flare:1.0 | quality-checks datasets | system | dataos_cli | System Runnable Steps | SCHEDULED RUNTIME | LAST SCHEDULED TIME --------------------|---------------------------- RUNNING | 2021-11-01T16:30:00+05:30 RUNTIME | PROGRESS | STARTED | FINISHED ----------|----------|---------------------------|----------- running | 2/3 | 2021-11-01T16:30:00+05:30 | NODE NAME | JOB NAME | POD NAME | TYPE | CONTAINERS | PHASE ----------------------------------------|------------------------|------------------------------------------------------|--------------|-------------------------|------------ quality-checks-summary-bviw-driver | quality-checks-summary | quality-checks-summary-bviw-driver | pod-flare | spark-kubernetes-driver | running","title":"Get"},{"location":"cli/cli/#get-runtime","text":"Get the runtime details of a resource in the DataOS. Usage: dataos-ctl get [flags] Flags: -d, --details Set to true to include details in the result -h, --help help for get -i, --identifier string Identifier of resource, like: NAME:VERSION:TYPE -n, --name string Name to query -r, --runtime Set to true to include runtime details in the result --tags Set to true to include tags in the result -t, --type string The resource type to get. Workspace resources: workflow, service, secret. Cluster resources: depot, policy. -w, --workspace string Workspace to query Examples: dataos-ctl get runtime -t depot -n icebase dataos-ctl get runtime -w system -t cluster -n minervaa dataos-ctl get runtime -w system -t service -n iosa-receiver dataos-ctl get runtime -w public -t workflow -n cnt-city-demo-01 dataos-ctl -i \"quality-checks-test-cases | v1beta1 | workflow | public\" get runtime You can also provide a string to get the runtime information. dataos-ctl -i \"quality-checks-test-cases | v1beta1 | workflow | public\" get runtime Output: INFO[0000] \ud83d\udd0d workflow... INFO[0002] \ud83d\udd0d workflow...complete NAME | VERSION | TYPE | WORKSPACE | TITLE | OWNER ----------------------------|---------|----------|-----------|----------------|---------------------- quality-checks-test-cases | v1beta1 | workflow | public | Quality-Checks | rakeshvishvakarma21 JOB NAME | STACK | JOB TITLE | JOB DEPENDENCIES -----------------------------|------------|-------------------------|------------------------- dataos-tool-quality-checks | toolbox | | quality-checks-summary quality-checks-summary | flare:1.0 | quality-checks datasets | system | dataos_cli | System Runnable Steps | SCHEDULED RUNTIME | LAST SCHEDULED TIME --------------------|---------------------------- RUNNING | 2021-11-01T14:30:00+05:30 RUNTIME | PROGRESS | STARTED | FINISHED ----------|----------|---------------------------|----------- running | 2/3 | 2021-11-01T14:30:00+05:30 | NODE NAME | JOB NAME | POD NAME | TYPE | CONTAINERS | PHASE ----------------------------------------|------------------------|------------------------------------------------------|--------------|-------------------------|------------ quality-checks-summary-bviw-driver | quality-checks-summary | quality-checks-summary-bviw-driver | pod-flare | spark-kubernetes-driver | running quality-checks-summary-execute | quality-checks-summary | quality-checks-test-cases-bviw-1635757200-996077945 | pod-workflow | main | running quality-checks-summary-start-rnnbl | quality-checks-summary | -checks-test-cases-bviw-1635757200-3571325227 | These commands get the runtime info and display the \"node\"s that are involved, then you can get the details of a specific node which then gives you the pod details of that node: dataos-ctl -i \"quality-checks-test-cases | v1beta1 | workflow | public\" --node quality-checks-summary-bviw-driver get runtime Output: INFO[0000] \ud83d\udd0d node... INFO[0003] \ud83d\udd0d node...complete NODE NAME | POD NAME | IMAGE PULL SECRETS | PHASE -------------------------------------|------------------------------------|---------------------------|------------ quality-checks-summary-bviw-driver | quality-checks-summary-bviw-driver | dataos-container-registry | Succeeded CONTAINER NAME | CONTAINER IMAGE | CONTAINER IMAGE PULL POLICY --------------------------|----------------------------------|------------------------------ spark-kubernetes-driver | docker.io/rubiklabs/flare:5.5.48 | IfNotPresent POD CONDITION TYPE | POD CONDITION STATUS | MESSAGE | TIME ---------------------|----------------------|---------|---------------------------- Initialized | True | | 2021-11-01T15:45:26+05:30 PodScheduled | True | | 2021-11-01T15:45:26+05:30 Ready | False | | 2021-11-01T15:47:25+05:30 ContainersReady | False | | 2021-11-01T15:47:25+05:30 CONTAINER NAME | CONTAINER STATE | STARTED | FINISHED --------------------------|--------------------------------|---------------------------|---------------------------- spark-kubernetes-driver | Terminated Reason:Completed | 2021-11-01T15:45:28+05:30 | 2021-11-01T15:47:25+05:30 | Message: ExitCode: 0 | | EVENT TYPE | EVENT REASON | EVENT SOURCE | EVENT MESSAGE | LAST OCCURRED | COUNT -------------|--------------|-------------------|-----------------------------------------------------|---------------------------|-------- Warning | FailedMount | kubelet | MountVolume.SetUp failed for | 2021-11-01T15:00:31+05:30 | 1 | | | volume \"spark-conf-volume-driver\" | | | | | : configmap | | | | | \"spark-drv-9f60d37cdad602e6-conf-map\" | | | | | not found | |","title":"Get Runtime"},{"location":"cli/cli/#health","text":"Get health of DataOS CLI, DataOS resources and services. It checks if server is reachable and helps in troubleshooting. Usage: dataos-ctl health [flags] Flags: -h, --help help for health Here is the expected output of this command: % dataos-ctl health INFO[0000] \ud83c\udfe5... INFO[0000] DataOS\u00ae CLI...OK INFO[0005] DataOS\u00ae CK...OK INFO[0005] \ud83c\udfe5...complete INFO[0005] \ud83d\udd17...https://formerly-saving-lynx.dataos.io INFO[0005] \u26c5\ufe0f...gcp","title":"Health"},{"location":"cli/cli/#help","text":"Get help for any command in the application. Usage: dataos-ctl help [command] [flags] Flags: -h, --help help for help","title":"Help"},{"location":"cli/cli/#init","text":"Initialize the DataOS environment. Usage: dataos-ctl init [flags] Flags: -h, --help help for init","title":"Init"},{"location":"cli/cli/#log","text":"Get the logs for a resource in the DataOS. Usage: dataos-ctl log [flags] Flags: -f, --follow Follow the logs -h, --help help for log -i, --identifier string Identifier of resource, like: NAME:VERSION:TYPE -r, --includeRunnable Include runnable system pods and logs -n, --name string Name to query -t, --type string The resource type to get, possible values: service, workflow -w, --workspace string Workspace to query (default \"public\") Examples: The log command has been updated to pass a \"node\" as well as to support getting logs for \"cluster\" and \"depot\" types that have runtimes. If you don't pass a \"node\" to the logs command it will try to display all the \"main\" logs for all nodes. dataos-ctl log -w public -t workflow -n cnt-city-demo-01 --node city-execute dataos-ctl log -w system -t cluster -n minervab --node minervab-ss-0 You can also pass the \"-i\" command with the string to get the logs. dataos-ctl -i \"quality-checks-test-cases | v1beta1 | workflow | public\" --node quality-checks-summary-bviw-driver log Output: INFO[0000] \ud83d\udcc3 log(public)... INFO[0003] \ud83d\udcc3 log(public)...complete NODE NAME | CONTAINER NAME | ERROR -------------------------------------|-------------------------|-------- quality-checks-summary-bviw-driver | spark-kubernetes-driver | -------------------LOGS------------------- 2021-11-01 08:32:06,938 INFO [task-result-getter-1] o.a.s.s.TaskSetManager: Finished task 54.0 in stage 1.0 (TID 69) in 17 ms on 10.212.16.7 (executor 1) (67/200) 2021-11-01 08:32:06,954 INFO [dispatcher-CoarseGrainedScheduler] o.a.s.s.TaskSetManager: Starting task 57.0 in stage 1.0 (TID 71) (10.212.16.7, executor 1, partition 57, PROCESS_LOCAL, 4472 bytes) taskResourceAssignments Map() 2021-11-01 08:32:06,954 INFO [task-result-getter-2] o.a.s.s.TaskSetManager: Finished task 56.0 in stage 1.0 (TID 70) in 17 ms on 10.212.16.7 (executor 1) (68/200) ... ... ... you can also pass the \"-c\" command with the container name you want to see the logs for.","title":"Log"},{"location":"cli/cli/#login","text":"Login to the DataOS. Usage: dataos-ctl login [flags] Flags: -h, --help help for login","title":"Login"},{"location":"cli/cli/#maintenance","text":"Maintenance of the DataOS. Usage: dataos-ctl maintenance [command] Available Commands: collect-garbage collects garbage on the DataOS. Flags: -h, --help help for maintenance Use \"dataos-ctl maintenance [command] --help\" for more information about a command.","title":"Maintenance"},{"location":"cli/cli/#maintenance-collect-garbage","text":"Collect Garbage on the DataOS. Usage: dataos-ctl maintenance collect-garbage [flags] Flags: -d, --duration string The duration to calculate the age of resources that are eligible for garbage collection (default \"168h\") -h, --help help for collect-garbage -k, --kubeconfig string Kubeconfig file location -l, --layer string The layer to target in the DataOS, user|system (default \"user\")","title":"Maintenance Collect-garbage"},{"location":"cli/cli/#operate","text":"Operate the DataOS. Usage: dataos-ctl operate [command] Available Commands: apply Apply manifest chart-export Exports a Helm Chart from a Chart Registry git Git component manifests install Install components ping Ping upgrade Upgrade components view View DataOS\u00ae Operator Services zip Zip install files Flags: -h, --help help for operate Use \"dataos-ctl operate [command] --help\" for more information about a command.","title":"Operate"},{"location":"cli/cli/#operate-apply","text":"Apply manifest on the DataOS. Usage: dataos-ctl operate apply [flags] Flags: -h, --help help for apply -f, --manifestFile string Single Manifest File Location -n, --namespace string Namespace","title":"Operate Apply"},{"location":"cli/cli/#operate-chart-export","text":"Exports a Helm Chart from a Chart Registry. Usage: dataos-ctl operate chart-export [flags] Flags: --accessKey string The AWS Access Key for ECR Chart Registry --accessSecret string The AWS Access Secret for ECR Chart Registry -c, --chart string The chart ref -d, --exportDir string The directory to export the Helm chart -h, --help help for chart-export --region string The AWS Region for ECR Chart Registry --registry string The AWS ECR Chart Registry","title":"Operate Chart-Export"},{"location":"cli/cli/#operate-get-secret","text":"Gets a secret from Heimdall. Usage: dataos-ctl operate get-secret [flags] Flags: -h, --help help for get-secret -i, --id string The secret id","title":"Operate Get-Secret"},{"location":"cli/cli/#operate-git","text":"Git component manifests on the DataOS. Usage: dataos-ctl operate git [flags] Flags: -e, --email string Operator email -h, --help help for git -l, --localOnly Perform local only -n, --name string Operator name -p, --push Push changes -r, --resetGitDir Reset the local git directory","title":"Operate Git"},{"location":"cli/cli/#operate-install","text":"When you create a new server, you want to install new applications on the server. Use this command to install one or more applications/components on the server. Usage: dataos-ctl operate install [flags] Flags: -h, --help help for install -i, --imagesFile string Installation Images File Location -f, --installFile string Installation Manifest File Location -n, --noGitOps Do not push changes to the GitOps repo in DataOS\u00ae --oldReleaseManifest Use old install manifest format --renderOnly Render only -r, --replaceIfExists Replace existing resources -s, --secretsFile string Installation Secrets File Location --useExternalPostgresql Use external postgresql -v, --valuesFile string Installation Values File Location","title":"Operate Install"},{"location":"cli/cli/#operate-ping","text":"Usage: dataos-ctl operate ping [flags] Flags: -h, --help help for ping","title":"Operate Ping"},{"location":"cli/cli/#operate-upgrade","text":"Upgrade components on the DataOS. Usage: dataos-ctl operate upgrade [flags] Flags: -h, --help help for upgrade -i, --imagesFile string Installation Images File Location -f, --installFile string Installation Manifest File Location --oldReleaseManifest Use old install manifest format -s, --secretsFile string Installation Secrets File Location --useExternalPostgresql Use external postgresql -v, --valuesFile string Installation Values File Location","title":"Operate Upgrade"},{"location":"cli/cli/#operate-view","text":"View DataOS Operator Services from the local machine without going to server. You can create a data pipe from server to local machine. Usage: dataos-ctl operate view [flags] Flags: -h, --help help for view -p, --localPort int The starting local port to port-forward services to (default 8081) -s, --servicesToPortForward strings The comma separated list of services to port-forward local: metis,cerebro,aurora-beanstalkd,git,prometheus, service-mesh,cruise-control,kibana,spark-history \u279c ~ dataos-ctl operate view -s metis INFO[0000] \ud83d\udcda metis view... INFO[0000] \ud83d\udd2d metis port-forward.. INFO[0003] close connections hit enter/return? INFO[0004] \ud83d\udd2d metis port-forward.. ready INFO[0004] : metis http://localhost:8081 Note : Config File \".dataos.ck.config\" should be present in the folder \"[/Users/[username]/.dataos/context].","title":"Operate View"},{"location":"cli/cli/#tui","text":"Dataos-ctl TUI is a Terminal User Interface for DataOS. It shows all the key resources deployed on the server. You can click on the resource menu to see the corresponding details in the Resource Summary section. You can view artefacts and Run time services/resources and and their YAML. You can also view logs for runtime. Usage: dataos-ctl tui [flags] Flags: -h, --help help for tui -w, --workspaces string list of workspaces to include, comma separated","title":"TUI"},{"location":"cli/cli/#user","text":"Manage DataOS users. Usage: dataos-ctl user [command] Available Commands: apikey Manage a DataOS\u00ae User apikey delete Delete a user get Get users tag Manage DataOS\u00ae User's tags Flags: -h, --help help for user Use \"dataos-ctl user [command] --help\" for more information about a command.","title":"User"},{"location":"cli/cli/#user-apikey","text":"Manage a DataOS user apikey. Usage: dataos-ctl user apikey [command] Available Commands: create Create an apikey for a user delete Delete the apikey for a user get Get the apikey for a user Flags: -h, --help help for apikey Use \"dataos-ctl user apikey [command] --help\" for more information about a command.","title":"User Apikey"},{"location":"cli/cli/#user-delete","text":"Delete a user Usage: dataos-ctl user delete [flags] Flags: -h, --help help for delete -i, --id string Id of the user","title":"User Delete"},{"location":"cli/cli/#user-get","text":"Get users Usage: dataos-ctl user get [flags] Flags: -a, --all Get all users -h, --help help for get -i, --id string Id of the user","title":"User Get"},{"location":"cli/cli/#user-tag","text":"Manage DataOS user's tags. Usage: dataos-ctl user tag [command] Available Commands: add Add tags to a user delete Delete tags from a user Flags: -h, --help help for tag Use \"dataos-ctl user tag [command] --help\" for more information about a command.","title":"User Tag"},{"location":"cli/cli/#user-tag-add","text":"Add tags to a user. Usage: dataos-ctl user tag add [flags] Flags: -h, --help help for add -i, --id string Id of the user -t, --tags strings The tags to add","title":"User Tag Add"},{"location":"cli/cli/#user-tag-delete","text":"Delete tags from a user. Usage: dataos-ctl user tag delete [flags] Flags: -h, --help help for delete -i, --id string Id of the user -t, --tags strings The tags to delete","title":"User Tag Delete"},{"location":"cli/cli/#version","text":"Print the version number of DataOS. Usage: dataos-ctl version [flags] Flags: -h, --help help for version","title":"Version"},{"location":"cli/cli/#view","text":"View core applications in the DataOS. Usage: dataos-ctl view [flags] Flags: -a, --application string The application to view in your default browser: apps, datanet, workbench, atlas -h, --help help for view","title":"View"},{"location":"cli/cli/#workspace","text":"Manage DataOS workspaces. Usage: dataos-ctl workspace [command] Available Commands: create Create workspace delete Delete workspaces get Get workspaces Flags: -h, --help help for workspace Use \"dataos-ctl workspace [command] --help\" for more information about a command.","title":"Workspace"},{"location":"cli/cli/#workspace-create","text":"Create workspace. Usage: dataos-ctl workspace create [flags] Flags: -d, --description string workspace description -h, --help help for create --labels strings The workspace labels -l, --layer string workspace layer (default \"user\") -n, --name string workspace name --tags strings The workspace tags -v, --version string workspace version (default \"v1beta1\")","title":"Workspace Create"},{"location":"cli/cli/#workspace-delete","text":"Delete workspaces. Usage: dataos-ctl workspace delete [flags] Flags: -h, --help help for delete -n, --name string workspace name","title":"Workspace Delete"},{"location":"cli/cli/#workspace-get","text":"Get workspaces. Usage: dataos-ctl workspace get [flags] Flags: -h, --help help for get -l, --layer string workspace layer (default \"user\")","title":"Workspace Get"},{"location":"cli/installation/","text":"DataOS CLI Install and Set Up [For Internal Users] \u00b6 This article describes the steps required to install DataOS CLI. These installation steps are for Unix/Linux based shell. Requirements \u00b6 Google Chrome browser installed. Access permissions given for the BitBucket repository. curl utility installed. Download \u00b6 To download the latest release from the bitbucket repository, you need username and an app-password. Generate app password, go to the following link. Username:Your bitbucket username. Password: It is a token string that you have to generate from the following link. link- https://bitbucket.org/account/settings/app-passwords/ Open a terminal on your operating system and run the following commands. export BITBUCKET_USERNAME=<your_username> export BITBUCKET_APP_PASSWORD=<generated_app_password> Run the curl command to download the dataos-CLI file. curl -L --user $BITBUCKET_USERNAME:$BITBUCKET_APP_PASSWORD https://api.bitbucket.org/2.0/repositories/rubik_/dataos-cli/downloads/download-dataos-ctl.sh | bash Add the bin folder to the PATH. export PATH=$PATH:$HOME/.dataos/bin Note : You can also download from the UI Downloads section of the Repo: https://bitbucket.org/rubik_/dataos-cli/downloads/ Initialize \u00b6 You are ready to run DataOS CLI with the dataos-ctl command. To initialize, run the init command. dataos-ctl init Provide the following: License API key This is only for the 'Operator' level user of DataOS who wants to install and perform admin tasks. For additional steps in Operator configuration, click on here Context name Fully qualified domain name This command will produce the following output: ~ % dataos-ctl init INFO[0000] \ud83d\ude80 initialization... INFO[0000] The DataOS\u00ae is not initialized, do you want to proceed with initialization? (Y,n) ->Y INFO[0012] Please enter your license key? ->wru-123456 INFO[0369] Are you installing and operating the DataOS\u00ae? (Y,n) ->Y INFO[0397] Please enter your DataOS\u00ae Prime Apikey? ->MmI4MjFiNDctZDZjZC00ODRmLThmYzMtOGI4NDAwY2U0ZWE1 INFO[0432] Please enter a name for the current DataOS\u00ae Context? ->formerly-saving-lynx INFO[0456] Please enter the fully qualified domain name of the DataOS\u00ae? ->formerly-saving-lynx.dataos.io INFO[0486] entered DataOS\u00ae: formerly-saving-lynx : formerly-saving-lynx.dataos.io INFO[0486] \ud83d\ude80 initialization...complete ~ % Log in \u00b6 After successful initialization of DataOS CLI, you can log in to CLI with the following command. dataos-ctl login Test \u00b6 Run the following commands to ensure the successful installation of DataOS CLI. These command will show the version and health status of the installed DataOS CLI. dataos-ctl version dataos-ctl health Operator configuration \u00b6 The following steps should only be setup for operators of the DataOS platform, users who need to install components into the DataOS and run administrative jobs. Download Helm-3. Make sure that Helm-3 version should be v3.2.0 and above. curl -L https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash 2. Run the following commands to access Kubernetes layer. export KUBECONFIG=~/.dataos/<context name>/.dataos.ck.k8s.config.yaml gcloud container clusters get-credentials <cluster name> -- region <region> --project <project name> For example: Parameter Value context name reasonably-welcome-grub cluster name dataos-ck-gke-grub-dev region us-west-4 project name dataos-ck-res-yak-dev With the above values, your commands may look like the following: export KUBECONFIG=~/.dataos/reasonably-welcome-grub/.dataos.ck.k8s.config.yaml gcloud container clusters get-credentials dataos-ck-gke-grub-dev --region us-west4 --project dataos-ck-res-yak-dev","title":"Installation[Internal Users]"},{"location":"cli/installation/#dataos-cli-install-and-set-up-for-internal-users","text":"This article describes the steps required to install DataOS CLI. These installation steps are for Unix/Linux based shell.","title":"DataOS CLI Install and Set Up [For Internal Users]"},{"location":"cli/installation/#requirements","text":"Google Chrome browser installed. Access permissions given for the BitBucket repository. curl utility installed.","title":"Requirements"},{"location":"cli/installation/#download","text":"To download the latest release from the bitbucket repository, you need username and an app-password. Generate app password, go to the following link. Username:Your bitbucket username. Password: It is a token string that you have to generate from the following link. link- https://bitbucket.org/account/settings/app-passwords/ Open a terminal on your operating system and run the following commands. export BITBUCKET_USERNAME=<your_username> export BITBUCKET_APP_PASSWORD=<generated_app_password> Run the curl command to download the dataos-CLI file. curl -L --user $BITBUCKET_USERNAME:$BITBUCKET_APP_PASSWORD https://api.bitbucket.org/2.0/repositories/rubik_/dataos-cli/downloads/download-dataos-ctl.sh | bash Add the bin folder to the PATH. export PATH=$PATH:$HOME/.dataos/bin Note : You can also download from the UI Downloads section of the Repo: https://bitbucket.org/rubik_/dataos-cli/downloads/","title":"Download"},{"location":"cli/installation/#initialize","text":"You are ready to run DataOS CLI with the dataos-ctl command. To initialize, run the init command. dataos-ctl init Provide the following: License API key This is only for the 'Operator' level user of DataOS who wants to install and perform admin tasks. For additional steps in Operator configuration, click on here Context name Fully qualified domain name This command will produce the following output: ~ % dataos-ctl init INFO[0000] \ud83d\ude80 initialization... INFO[0000] The DataOS\u00ae is not initialized, do you want to proceed with initialization? (Y,n) ->Y INFO[0012] Please enter your license key? ->wru-123456 INFO[0369] Are you installing and operating the DataOS\u00ae? (Y,n) ->Y INFO[0397] Please enter your DataOS\u00ae Prime Apikey? ->MmI4MjFiNDctZDZjZC00ODRmLThmYzMtOGI4NDAwY2U0ZWE1 INFO[0432] Please enter a name for the current DataOS\u00ae Context? ->formerly-saving-lynx INFO[0456] Please enter the fully qualified domain name of the DataOS\u00ae? ->formerly-saving-lynx.dataos.io INFO[0486] entered DataOS\u00ae: formerly-saving-lynx : formerly-saving-lynx.dataos.io INFO[0486] \ud83d\ude80 initialization...complete ~ %","title":"Initialize"},{"location":"cli/installation/#log-in","text":"After successful initialization of DataOS CLI, you can log in to CLI with the following command. dataos-ctl login","title":"Log in"},{"location":"cli/installation/#test","text":"Run the following commands to ensure the successful installation of DataOS CLI. These command will show the version and health status of the installed DataOS CLI. dataos-ctl version dataos-ctl health","title":"Test"},{"location":"cli/installation/#operator-configuration","text":"The following steps should only be setup for operators of the DataOS platform, users who need to install components into the DataOS and run administrative jobs. Download Helm-3. Make sure that Helm-3 version should be v3.2.0 and above. curl -L https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash 2. Run the following commands to access Kubernetes layer. export KUBECONFIG=~/.dataos/<context name>/.dataos.ck.k8s.config.yaml gcloud container clusters get-credentials <cluster name> -- region <region> --project <project name> For example: Parameter Value context name reasonably-welcome-grub cluster name dataos-ck-gke-grub-dev region us-west-4 project name dataos-ck-res-yak-dev With the above values, your commands may look like the following: export KUBECONFIG=~/.dataos/reasonably-welcome-grub/.dataos.ck.k8s.config.yaml gcloud container clusters get-credentials dataos-ck-gke-grub-dev --region us-west4 --project dataos-ck-res-yak-dev","title":"Operator configuration"},{"location":"cli/installationcli/","text":"DataOS CLI Install and Set Up \u00b6 This article describes the steps required to install DataOS CLI. These installation steps are for Mac/Unix/Linux/Windows. Requirements \u00b6 Please ensure that curl utility is installed on your system. Also note that Google Chrome is the recommended browser. Get the following items from your Customer Success team, depending upon your user role: Users \u00b6 DataOS license key DataOS CLI download link Operators \u00b6 DataOS license key DataOS prime apikey DataOS CLI download link DataOS prime cloud username DataOS prime cloud password Download \u00b6 Follow the instructions for the respective operating system on your machine. MacOS \u00b6 Export the environment variable PRIME_APIKEY to pass it to the next commands. $ export PRIME_APIKEY=<DataOS\u00ae prime apikey> Download the checksum file and DataOS CLI binary using the following commands: $ curl --silent --output dataos-ctl-darwin-amd64.tar.gz.sha256sum --location --request GET \"https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-darwin-amd64.tar.gz.sha256sum&dir=cli-apps-1.1&apikey=$PRIME_APIKEY\" $ curl --silent --output dataos-ctl-darwin-amd64.tar.gz --location --request GET \"https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-darwin-amd64.tar.gz&dir=cli-apps-1.1&apikey=$PRIME_APIKEY\" Validate that the zip has not been tampered. $ shasum -a 256 -c dataos-ctl-darwin-amd64.tar.gz.sha256sum Output: dataos-ctl-darwin-amd64.tar.gz: OK Extract the dataos-ctl binary. $ tar -xvf dataos-ctl-darwin-amd64.tar.gz Output: x darwin-amd64/ x darwin-amd64/dataos-ctl Note : Place the extracted dataos-ctl in a directory that is in your PATH. Linux \u00b6 Export the environment variable PRIME_APIKEY to pass it to the next commands. $ export PRIME_APIKEY=<DataOS\u00ae prime apikey> Determine processor architecture. $ uname -p You will get the following output based on your processor. amd64 Update the ARCH value in the following commands to download the CLI binary and checksum file. The available ARCH values are 386, amd64, arm, arm64. $ curl --silent --output dataos-ctl-linux-<ARCH>.tar.gz.sha256sum --location --request GET \"https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-linux-<ARCH>.tar.gz.sha256sum&dir=cli-apps-1.1&apikey=$PRIME_APIKEY\" $ curl --silent --output dataos-ctl-linux-<ARCH>.tar.gz --location --request GET \"https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-linux-<ARCH>.tar.gz&dir=cli-apps-1.1&apikey=$PRIME_APIKEY\" Validate that the zip has not been tampered. $ shasum -a 256 -c dataos-ctl-linux-<ARCH>.tar.gz.sha256sum Output: dataos-ctl-linux-<ARCH>.tar.gz: OK Extract the dataos-ctl binary. $ tar -xvf dataos-ctl-linux-<ARCH>.tar.gz Output: x linux-<ARCH>/ x linux-<ARCH>/dataos-ctl Note : Place the extracted dataos-ctl binary in your PATH Windows \u00b6 Run the following commands to download the CLI binary. https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-windows-386.tar.gz.sha256sum&dir=cli-apps-1.1&apikey=$PRIME_APIKEY https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-windows-386.tar.gz&dir=cli-apps-1.1&apikey=$PRIME_APIKEY https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-windows-amd64.tar.gz.sha256sum&dir=cli-apps-1.1&apikey=$PRIME_APIKEY https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-windows-amd64.tar.gz&dir=cli-apps-1.1&apikey=$PRIME_APIKEY Note : Place the extracted dataos-ctl in a directory that is in your PATH. Initialize \u00b6 You are ready to run DataOS CLI with the dataos-ctl command. To initialize, run the init command. dataos-ctl init Provide the following: License API key This is only for the 'Operator' level user of DataOS who wants to install and perform admin tasks. For additional steps in Operator configuration, click on here Context name Fully qualified domain name This command will produce the following output: ~ % dataos-ctl init INFO[0000] \ud83d\ude80 initialization... INFO[0000] The DataOS\u00ae is not initialized, do you want to proceed with initialization? (Y,n) ->Y INFO[0012] Please enter your license key? ->wru-123456 INFO[0369] Are you installing and operating the DataOS\u00ae? (Y,n) ->Y INFO[0397] Please enter your DataOS\u00ae Prime Apikey? ->MmI4MjFiNDctZDZjZC00ODRmLThmYzMtOGI4NDAwY2U0ZWE1 INFO[0432] Please enter a name for the current DataOS\u00ae Context? ->formerly-saving-lynx INFO[0456] Please enter the fully qualified domain name of the DataOS\u00ae? ->formerly-saving-lynx.dataos.io INFO[0486] entered DataOS\u00ae: formerly-saving-lynx : formerly-saving-lynx.dataos.io INFO[0486] \ud83d\ude80 initialization...complete ~ % Log in \u00b6 After successful initialization of DataOS CLI, you can log in to CLI with the following command. dataos-ctl login Test \u00b6 Run the following commands to ensure the successful installation of DataOS CLI. These commands will show the version and health status of the installed DataOS CLI. dataos-ctl version dataos-ctl health Operator configuration \u00b6 The following steps should only be setup for operators of the DataOS platform, users who need to install components into the DataOS and run administrative jobs. Download Helm-3. Make sure that Helm-3 version should be v3.2.0 and above. curl -L https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Run the following commands to access Kubernetes layer. export KUBECONFIG=~/.dataos/<context name>/.dataos.ck.k8s.config.yaml gcloud container clusters get-credentials <cluster name> -- region <region> --project <project name> For example: Parameter Value context name reasonably-welcome-grub cluster name dataos-ck-gke-grub-dev region us-west-4 project name dataos-ck-res-yak-dev With the above values, your commands will look like the following: export KUBECONFIG=~/.dataos/reasonably-welcome-grub/.dataos.ck.k8s.config.yaml gcloud container clusters get-credentials dataos-ck-gke-grub-dev --region us-west4 --project dataos-ck-res-yak-dev","title":"Installation"},{"location":"cli/installationcli/#dataos-cli-install-and-set-up","text":"This article describes the steps required to install DataOS CLI. These installation steps are for Mac/Unix/Linux/Windows.","title":"DataOS CLI Install and Set Up"},{"location":"cli/installationcli/#requirements","text":"Please ensure that curl utility is installed on your system. Also note that Google Chrome is the recommended browser. Get the following items from your Customer Success team, depending upon your user role:","title":"Requirements"},{"location":"cli/installationcli/#users","text":"DataOS license key DataOS CLI download link","title":"Users"},{"location":"cli/installationcli/#operators","text":"DataOS license key DataOS prime apikey DataOS CLI download link DataOS prime cloud username DataOS prime cloud password","title":"Operators"},{"location":"cli/installationcli/#download","text":"Follow the instructions for the respective operating system on your machine.","title":"Download"},{"location":"cli/installationcli/#macos","text":"Export the environment variable PRIME_APIKEY to pass it to the next commands. $ export PRIME_APIKEY=<DataOS\u00ae prime apikey> Download the checksum file and DataOS CLI binary using the following commands: $ curl --silent --output dataos-ctl-darwin-amd64.tar.gz.sha256sum --location --request GET \"https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-darwin-amd64.tar.gz.sha256sum&dir=cli-apps-1.1&apikey=$PRIME_APIKEY\" $ curl --silent --output dataos-ctl-darwin-amd64.tar.gz --location --request GET \"https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-darwin-amd64.tar.gz&dir=cli-apps-1.1&apikey=$PRIME_APIKEY\" Validate that the zip has not been tampered. $ shasum -a 256 -c dataos-ctl-darwin-amd64.tar.gz.sha256sum Output: dataos-ctl-darwin-amd64.tar.gz: OK Extract the dataos-ctl binary. $ tar -xvf dataos-ctl-darwin-amd64.tar.gz Output: x darwin-amd64/ x darwin-amd64/dataos-ctl Note : Place the extracted dataos-ctl in a directory that is in your PATH.","title":"MacOS"},{"location":"cli/installationcli/#linux","text":"Export the environment variable PRIME_APIKEY to pass it to the next commands. $ export PRIME_APIKEY=<DataOS\u00ae prime apikey> Determine processor architecture. $ uname -p You will get the following output based on your processor. amd64 Update the ARCH value in the following commands to download the CLI binary and checksum file. The available ARCH values are 386, amd64, arm, arm64. $ curl --silent --output dataos-ctl-linux-<ARCH>.tar.gz.sha256sum --location --request GET \"https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-linux-<ARCH>.tar.gz.sha256sum&dir=cli-apps-1.1&apikey=$PRIME_APIKEY\" $ curl --silent --output dataos-ctl-linux-<ARCH>.tar.gz --location --request GET \"https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-linux-<ARCH>.tar.gz&dir=cli-apps-1.1&apikey=$PRIME_APIKEY\" Validate that the zip has not been tampered. $ shasum -a 256 -c dataos-ctl-linux-<ARCH>.tar.gz.sha256sum Output: dataos-ctl-linux-<ARCH>.tar.gz: OK Extract the dataos-ctl binary. $ tar -xvf dataos-ctl-linux-<ARCH>.tar.gz Output: x linux-<ARCH>/ x linux-<ARCH>/dataos-ctl Note : Place the extracted dataos-ctl binary in your PATH","title":"Linux"},{"location":"cli/installationcli/#windows","text":"Run the following commands to download the CLI binary. https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-windows-386.tar.gz.sha256sum&dir=cli-apps-1.1&apikey=$PRIME_APIKEY https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-windows-386.tar.gz&dir=cli-apps-1.1&apikey=$PRIME_APIKEY https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-windows-amd64.tar.gz.sha256sum&dir=cli-apps-1.1&apikey=$PRIME_APIKEY https://prime.dataos.io/plutus/api/v1/files/download?name=dataos-ctl-windows-amd64.tar.gz&dir=cli-apps-1.1&apikey=$PRIME_APIKEY Note : Place the extracted dataos-ctl in a directory that is in your PATH.","title":"Windows"},{"location":"cli/installationcli/#initialize","text":"You are ready to run DataOS CLI with the dataos-ctl command. To initialize, run the init command. dataos-ctl init Provide the following: License API key This is only for the 'Operator' level user of DataOS who wants to install and perform admin tasks. For additional steps in Operator configuration, click on here Context name Fully qualified domain name This command will produce the following output: ~ % dataos-ctl init INFO[0000] \ud83d\ude80 initialization... INFO[0000] The DataOS\u00ae is not initialized, do you want to proceed with initialization? (Y,n) ->Y INFO[0012] Please enter your license key? ->wru-123456 INFO[0369] Are you installing and operating the DataOS\u00ae? (Y,n) ->Y INFO[0397] Please enter your DataOS\u00ae Prime Apikey? ->MmI4MjFiNDctZDZjZC00ODRmLThmYzMtOGI4NDAwY2U0ZWE1 INFO[0432] Please enter a name for the current DataOS\u00ae Context? ->formerly-saving-lynx INFO[0456] Please enter the fully qualified domain name of the DataOS\u00ae? ->formerly-saving-lynx.dataos.io INFO[0486] entered DataOS\u00ae: formerly-saving-lynx : formerly-saving-lynx.dataos.io INFO[0486] \ud83d\ude80 initialization...complete ~ %","title":"Initialize"},{"location":"cli/installationcli/#log-in","text":"After successful initialization of DataOS CLI, you can log in to CLI with the following command. dataos-ctl login","title":"Log in"},{"location":"cli/installationcli/#test","text":"Run the following commands to ensure the successful installation of DataOS CLI. These commands will show the version and health status of the installed DataOS CLI. dataos-ctl version dataos-ctl health","title":"Test"},{"location":"cli/installationcli/#operator-configuration","text":"The following steps should only be setup for operators of the DataOS platform, users who need to install components into the DataOS and run administrative jobs. Download Helm-3. Make sure that Helm-3 version should be v3.2.0 and above. curl -L https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash Run the following commands to access Kubernetes layer. export KUBECONFIG=~/.dataos/<context name>/.dataos.ck.k8s.config.yaml gcloud container clusters get-credentials <cluster name> -- region <region> --project <project name> For example: Parameter Value context name reasonably-welcome-grub cluster name dataos-ck-gke-grub-dev region us-west-4 project name dataos-ck-res-yak-dev With the above values, your commands will look like the following: export KUBECONFIG=~/.dataos/reasonably-welcome-grub/.dataos.ck.k8s.config.yaml gcloud container clusters get-credentials dataos-ck-gke-grub-dev --region us-west4 --project dataos-ck-res-yak-dev","title":"Operator configuration"},{"location":"cli/tutorials/","text":"Tutorials \u00b6 Submitting a workflow to run job(s) on CLI \u00b6 You can define workflows to carry out large-scale data processing. The workflows can have one or more jobs and are created as a sequential YAML. This example shows how to define a Flare job in a workflow to ingest customer data into DataOS. Once the job is successfully run, you can access this data for further analysis and processing in DataOS. Log in to DataOS CLI \u00b6 Before login, ensure that DataOS is initialized. Create a workflow \u00b6 To define a workflow for your Flare job you want to run, you have to provide various configuration values in key-value pair in yaml file. To learn more about creating Flare jobs, Refer Flare documentation. Note : Ensure that necessary Depot definitions are created to enable read & write of the data. To learn more about creating Depots, Refer Depot documentation. For this example, a sample yaml file (config_v2beta1.yaml) is given below for a Flare job to ingest data from a CSV file. version : v1beta1 name : wf-cust-demo-np # workflow name type : workflow tags : - Connect - Customer description : The job ingests customer data from dropzone into raw zone workflow : dag : # dag contains job definitions - name : customer title : Customer Dimension Ingester description : The job ingests customer data from dropzone into raw zone spec : tags : - Connect - Customer stack : flare:1.0 tier : connect flare : driver : coreLimit : 1200m cores : 1 memory : 1024m executor : coreLimit : 1200m cores : 1 instances : 1 memory : 1024m job : explain : true inputs : # input CSV file details - name : customer_connect dataset : dataos://thirdparty01:none/customer?acr=r # relative path of the file in the Depot that you want to read format : csv schemaPath : dataos://thirdparty01:none/schemas/avsc/customer.avsc logLevel : WARN outputs : - name : output01 depot : dataos://icebase:raw01?acr=rw # location where the data is saved in DataOS: in the form dataos://catalog:schema steps : - sink : - sequenceName : customers datasetName : customer_np # output table name outputName : output01 outputType : Iceberg description : Customer data ingested from external csv outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip tags : - Connect - Customer title : Customer Source Data # in the following section, you can perform some transformations on the data sequence : - name : customers doc : Pick all columns from customers and add version as yyyyMMddHHmm formatted timestamp. sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_customer FROM customer_connect functions : - name : rename column : Id asColumn : id Create your own workspace \u00b6 This is an optional step. you can always run your Flare jobs in \u201cpublic workspace\u201d. dataos-ctl workspace create -n <name of your workspace> Check and submit the workflow \u00b6 As an intermediate step, you can check if your workflow is valid by running the following command. dataos-ctl apply -f <address of the job you created> -w <name of the workspace> -l The command may look like this: % dataos-ctl apply -f dataos-resources/flare/connect-customer-iceberg/config_v2beta1.yaml -w public -l Output on the screen: ... ... ... INFO[0001] \ud83d\udd27 applying wf-cust-demo-np:v1beta1:workflow...valid 2. Now finally, submit the workflow for execution. The apply command will create the workflow resource in DataOS. dataos-ctl apply -f <address of the job you created> -w <name of the workspace> Check the workflow status \u00b6 You can take a look to the status of the workflow to check Running/Succeeded/Failed jobs. dataos-ctl get -t workflow -w <name of the workspace> -n <workflow name> The output may look like this: apple@ dataos % dataos-ctl get -t workflow -w public -n wf-cust-demo-np INFO[0000] \ud83d\udd0d get... INFO[0003] \ud83d\udd0d get...complete NAME | VERSION | TYPE | WORKSPACE | STATUS | RUNTIME | Owner --------------------------|---------|----------|-----------|--------|----------------|----------- wf-cust-demo-np | v1beta1 | workflow | public | active | succeeded | Rakeshv19 Get the logs \u00b6 CLI provides the log command for the Flare Workflow to keep track of important events. The output generated on the screen will help you to know the cause of error if any job execution in the workflow is failed. dataos-ctl log -t workflow -w <name of the workspace> -n <name of the workflow> > *Note: If you want to see the logs stream when the job is still running, just add -f at the end of the *log* command given above.* Run dataos-tool job \u00b6 This is a mandatory step when your Flare job is writing data in Iceberg format. Create a workflow containing dataos-tool job to see the ingested data in the workbench. Ensure the path for the output. Submit this workflow for execution and get the status of the workflow as described in earlier sections. For this example, a sample yaml file (datatool_v2beta1.yaml) is given below. version : v1beta1 name : dataos-tool-customer-np type : workflow workflow : dag : - name : dataos-tool-customer-01 spec : stack : toolbox toolbox : dataset : dataos://icebase:retail/customer?acl=rw action : name : set_version value : latest Note: Please contact administartor to get the proper privileges and policies added with your user profile to run the dataos-tool. The tag- tagdataos:u:toolbox-user will be required to run tool job. View the ingested data \u00b6 Launch DataOS instance and go to Workbench . dataos-ctl view -a workbench In the DataOS workbench, select catalog, schema and table. The data will be visible in the workbench. Delete the workflow \u00b6 You should delete the workflow from the environment, after your job is successfully run. The workflow, otherwise, will keep floating in the environment for three days. dataos-ctl delete -t workflow -w <name of the workspace> -n <name of the workflow>","title":"Tutorials"},{"location":"cli/tutorials/#tutorials","text":"","title":"Tutorials"},{"location":"cli/tutorials/#submitting-a-workflow-to-run-jobs-on-cli","text":"You can define workflows to carry out large-scale data processing. The workflows can have one or more jobs and are created as a sequential YAML. This example shows how to define a Flare job in a workflow to ingest customer data into DataOS. Once the job is successfully run, you can access this data for further analysis and processing in DataOS.","title":"Submitting a workflow to run job(s) on CLI"},{"location":"cli/tutorials/#log-in-to-dataos-cli","text":"Before login, ensure that DataOS is initialized.","title":"Log in to DataOS CLI"},{"location":"cli/tutorials/#create-a-workflow","text":"To define a workflow for your Flare job you want to run, you have to provide various configuration values in key-value pair in yaml file. To learn more about creating Flare jobs, Refer Flare documentation. Note : Ensure that necessary Depot definitions are created to enable read & write of the data. To learn more about creating Depots, Refer Depot documentation. For this example, a sample yaml file (config_v2beta1.yaml) is given below for a Flare job to ingest data from a CSV file. version : v1beta1 name : wf-cust-demo-np # workflow name type : workflow tags : - Connect - Customer description : The job ingests customer data from dropzone into raw zone workflow : dag : # dag contains job definitions - name : customer title : Customer Dimension Ingester description : The job ingests customer data from dropzone into raw zone spec : tags : - Connect - Customer stack : flare:1.0 tier : connect flare : driver : coreLimit : 1200m cores : 1 memory : 1024m executor : coreLimit : 1200m cores : 1 instances : 1 memory : 1024m job : explain : true inputs : # input CSV file details - name : customer_connect dataset : dataos://thirdparty01:none/customer?acr=r # relative path of the file in the Depot that you want to read format : csv schemaPath : dataos://thirdparty01:none/schemas/avsc/customer.avsc logLevel : WARN outputs : - name : output01 depot : dataos://icebase:raw01?acr=rw # location where the data is saved in DataOS: in the form dataos://catalog:schema steps : - sink : - sequenceName : customers datasetName : customer_np # output table name outputName : output01 outputType : Iceberg description : Customer data ingested from external csv outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip tags : - Connect - Customer title : Customer Source Data # in the following section, you can perform some transformations on the data sequence : - name : customers doc : Pick all columns from customers and add version as yyyyMMddHHmm formatted timestamp. sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_customer FROM customer_connect functions : - name : rename column : Id asColumn : id","title":"Create a workflow"},{"location":"cli/tutorials/#create-your-own-workspace","text":"This is an optional step. you can always run your Flare jobs in \u201cpublic workspace\u201d. dataos-ctl workspace create -n <name of your workspace>","title":"Create your own workspace"},{"location":"cli/tutorials/#check-and-submit-the-workflow","text":"As an intermediate step, you can check if your workflow is valid by running the following command. dataos-ctl apply -f <address of the job you created> -w <name of the workspace> -l The command may look like this: % dataos-ctl apply -f dataos-resources/flare/connect-customer-iceberg/config_v2beta1.yaml -w public -l Output on the screen: ... ... ... INFO[0001] \ud83d\udd27 applying wf-cust-demo-np:v1beta1:workflow...valid 2. Now finally, submit the workflow for execution. The apply command will create the workflow resource in DataOS. dataos-ctl apply -f <address of the job you created> -w <name of the workspace>","title":"Check and submit the workflow"},{"location":"cli/tutorials/#check-the-workflow-status","text":"You can take a look to the status of the workflow to check Running/Succeeded/Failed jobs. dataos-ctl get -t workflow -w <name of the workspace> -n <workflow name> The output may look like this: apple@ dataos % dataos-ctl get -t workflow -w public -n wf-cust-demo-np INFO[0000] \ud83d\udd0d get... INFO[0003] \ud83d\udd0d get...complete NAME | VERSION | TYPE | WORKSPACE | STATUS | RUNTIME | Owner --------------------------|---------|----------|-----------|--------|----------------|----------- wf-cust-demo-np | v1beta1 | workflow | public | active | succeeded | Rakeshv19","title":"Check the workflow status"},{"location":"cli/tutorials/#get-the-logs","text":"CLI provides the log command for the Flare Workflow to keep track of important events. The output generated on the screen will help you to know the cause of error if any job execution in the workflow is failed. dataos-ctl log -t workflow -w <name of the workspace> -n <name of the workflow> > *Note: If you want to see the logs stream when the job is still running, just add -f at the end of the *log* command given above.*","title":"Get the logs"},{"location":"cli/tutorials/#run-dataos-tool-job","text":"This is a mandatory step when your Flare job is writing data in Iceberg format. Create a workflow containing dataos-tool job to see the ingested data in the workbench. Ensure the path for the output. Submit this workflow for execution and get the status of the workflow as described in earlier sections. For this example, a sample yaml file (datatool_v2beta1.yaml) is given below. version : v1beta1 name : dataos-tool-customer-np type : workflow workflow : dag : - name : dataos-tool-customer-01 spec : stack : toolbox toolbox : dataset : dataos://icebase:retail/customer?acl=rw action : name : set_version value : latest Note: Please contact administartor to get the proper privileges and policies added with your user profile to run the dataos-tool. The tag- tagdataos:u:toolbox-user will be required to run tool job.","title":"Run dataos-tool job"},{"location":"cli/tutorials/#view-the-ingested-data","text":"Launch DataOS instance and go to Workbench . dataos-ctl view -a workbench In the DataOS workbench, select catalog, schema and table. The data will be visible in the workbench.","title":"View the ingested data"},{"location":"cli/tutorials/#delete-the-workflow","text":"You should delete the workflow from the environment, after your job is successfully run. The workflow, otherwise, will keep floating in the environment for three days. dataos-ctl delete -t workflow -w <name of the workspace> -n <name of the workflow>","title":"Delete the workflow"},{"location":"depots/","text":"DataOS Depot \u00b6 What is a Depot? \u00b6 Depot is a resource type and core primitive within the dataOS ecosystem. Depot provides a reference to the Data source/sink and abstracts the details of their configurations and storage formats. It helps you to easily understand the data source/sink and connect with it. You can use these Depots for accessing, processing and exploring data within DataOS. Depot contains the data location information along with any credentials and secrets that are required to access data from an external source. Once the Depot is created, you can use it in your Flare jobs, services and in other tools within the DataOS system. Supported data sources \u00b6 With the help of Depots, DataOS enables you to connect to and access the data from the following data sources. Amazon Simple Storage Service (s3) - connect with Amazon Web Services that provides object storage through a web service interface. Azure Blob Storage - connect with Microsoft's object storage solution for the cloud and access massive amounts of unstructured data, such as text or binary data. Azure Data Lake Storage - connect with running large-scale analytics systems to read huge amounts of data. Apache Pulsar - get connected to read from cloud-native, distributed messaging and streaming platform. BigQuery - connect with fully-managed, server-less data warehouse. Elastic Search - connect with document oriented database, distributed and open search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. File - read from Plain text file/ CSV file/ Parquet files. Google Cloud Storage - get connected to online file storage web service for accessing data on Google Cloud Platform infrastructure. JDBC/ODBC connections - access data stored in databases such as MYSQL, PostgreSQL, Oracle and other databases. Kafka - get connected to stream-processing framework to read from real-time data feeds. Presto - query a variety of data sources with distributed SQL query engine for big data. Redis - get connected to in-memory data structure store, used as a distributed, in-memory key\u2013value database, cache and message broker. Snowflake - connect with your Snowflake instance to read data and run queries in Snowflake tables.","title":"Introduction"},{"location":"depots/#dataos-depot","text":"","title":"DataOS Depot"},{"location":"depots/#what-is-a-depot","text":"Depot is a resource type and core primitive within the dataOS ecosystem. Depot provides a reference to the Data source/sink and abstracts the details of their configurations and storage formats. It helps you to easily understand the data source/sink and connect with it. You can use these Depots for accessing, processing and exploring data within DataOS. Depot contains the data location information along with any credentials and secrets that are required to access data from an external source. Once the Depot is created, you can use it in your Flare jobs, services and in other tools within the DataOS system.","title":"What is a Depot?"},{"location":"depots/#supported-data-sources","text":"With the help of Depots, DataOS enables you to connect to and access the data from the following data sources. Amazon Simple Storage Service (s3) - connect with Amazon Web Services that provides object storage through a web service interface. Azure Blob Storage - connect with Microsoft's object storage solution for the cloud and access massive amounts of unstructured data, such as text or binary data. Azure Data Lake Storage - connect with running large-scale analytics systems to read huge amounts of data. Apache Pulsar - get connected to read from cloud-native, distributed messaging and streaming platform. BigQuery - connect with fully-managed, server-less data warehouse. Elastic Search - connect with document oriented database, distributed and open search and analytics engine for all types of data, including textual, numerical, geospatial, structured, and unstructured. File - read from Plain text file/ CSV file/ Parquet files. Google Cloud Storage - get connected to online file storage web service for accessing data on Google Cloud Platform infrastructure. JDBC/ODBC connections - access data stored in databases such as MYSQL, PostgreSQL, Oracle and other databases. Kafka - get connected to stream-processing framework to read from real-time data feeds. Presto - query a variety of data sources with distributed SQL query engine for big data. Redis - get connected to in-memory data structure store, used as a distributed, in-memory key\u2013value database, cache and message broker. Snowflake - connect with your Snowflake instance to read data and run queries in Snowflake tables.","title":"Supported data sources"},{"location":"depots/amazons3/","text":"Amazon S3 \u00b6 DataOS allows you to connect to Amazon S3 to read objects within buckets using Depots. The Depot enables access to S3 object metadata. Amazon S3 is a key-based object store. Each object is stored and retrieved using a unique key. Each account contains one or more buckets to store data and to organize the Amazon S3 namespace. An object is uniquely identified within a bucket by a key (unique identifier) and a version ID. Requirements \u00b6 To connect to Amazon S3, you need: AWS access key id AWS bucket name Secret access key Scheme Relative Path Format Template \u00b6 To create a Depot of type \u2018S3\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> owner : <owner-name> layer : user depot : type : S3 description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : awsaccesskeyid : <access-key-id> awsbucketname : <bucket-name> awssecretaccesskey : <secret-access-key> - acl : r type : key-value-properties data : awsaccesskeyid : <access-key-id> awsbucketname : <bucket-name> awssecretaccesskey : <secret-access-key> spec : # Data source specific configurations # default scheme \"s3a\" scheme : <s3a> bucket : <project-name> relativePath : <relative-path> format : <format>","title":"Amazon S3"},{"location":"depots/amazons3/#amazon-s3","text":"DataOS allows you to connect to Amazon S3 to read objects within buckets using Depots. The Depot enables access to S3 object metadata. Amazon S3 is a key-based object store. Each object is stored and retrieved using a unique key. Each account contains one or more buckets to store data and to organize the Amazon S3 namespace. An object is uniquely identified within a bucket by a key (unique identifier) and a version ID.","title":"Amazon S3"},{"location":"depots/amazons3/#requirements","text":"To connect to Amazon S3, you need: AWS access key id AWS bucket name Secret access key Scheme Relative Path Format","title":"Requirements"},{"location":"depots/amazons3/#template","text":"To create a Depot of type \u2018S3\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> owner : <owner-name> layer : user depot : type : S3 description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : awsaccesskeyid : <access-key-id> awsbucketname : <bucket-name> awssecretaccesskey : <secret-access-key> - acl : r type : key-value-properties data : awsaccesskeyid : <access-key-id> awsbucketname : <bucket-name> awssecretaccesskey : <secret-access-key> spec : # Data source specific configurations # default scheme \"s3a\" scheme : <s3a> bucket : <project-name> relativePath : <relative-path> format : <format>","title":"Template"},{"location":"depots/azureblobfilestorage/","text":"Azure Blob Storage \u00b6 DataOS allows you to create a Depot of type 'ABFSS' to support reading data stored in an Azure Blob Storage account. This Depot enables all access through a storage account. This storage account may have multiple containers. A container is a grouping of multiple blobs. Define separate Depot for each container. Requirements \u00b6 To connect to Azure Blob storage, you need: Storage account name Storage account key Container Relative path Format Template \u00b6 To create a Depot of type \u2018ABFSS\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : ABFSS description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : azurestorageaccountname : <account-name> azurestorageaccountkey : <account-key> - acl : r type : key-value-properties data : azurestorageaccountname : <account-name> azurestorageaccountkey : <account-key> spec : # Data source specific configurations account : <account-name> container : <container-name> relativePath : <relative-path> format : <format>","title":"Azur Blob Storage"},{"location":"depots/azureblobfilestorage/#azure-blob-storage","text":"DataOS allows you to create a Depot of type 'ABFSS' to support reading data stored in an Azure Blob Storage account. This Depot enables all access through a storage account. This storage account may have multiple containers. A container is a grouping of multiple blobs. Define separate Depot for each container.","title":"Azure Blob Storage"},{"location":"depots/azureblobfilestorage/#requirements","text":"To connect to Azure Blob storage, you need: Storage account name Storage account key Container Relative path Format","title":"Requirements"},{"location":"depots/azureblobfilestorage/#template","text":"To create a Depot of type \u2018ABFSS\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : ABFSS description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : azurestorageaccountname : <account-name> azurestorageaccountkey : <account-key> - acl : r type : key-value-properties data : azurestorageaccountname : <account-name> azurestorageaccountkey : <account-key> spec : # Data source specific configurations account : <account-name> container : <container-name> relativePath : <relative-path> format : <format>","title":"Template"},{"location":"depots/bigquery/","text":"BigQuery \u00b6 DataOS allows you to create a Depot of type 'BIGQUERY' to read the data stored in the BigQuery projects. You can create several Depots, each pointing to a different project. Requirements \u00b6 To connect to BigQuery, you need: Project id Email Credential properties in JSON file Additional parameters Template \u00b6 To create a Depot of type 'BIGQUERY', use the following template: version : v1beta1 name : <depot-name> type : depot tags : - dropzone - bigquery owner : <owner-name> layer : user depot : type : BIGQUERY description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : projectid : <project-name> email : <email-id> files : json_keyfile : <json-file-path> - acl : r type : key-value-properties data : projectid : <project-name> email : <email-id> files : json_keyfile : <json-file-path> spec : # Data source specific configurations project : <project-name> params : \"key1\" : \"value1\" \"key2\" : \"value2\"","title":"BigQuery"},{"location":"depots/bigquery/#bigquery","text":"DataOS allows you to create a Depot of type 'BIGQUERY' to read the data stored in the BigQuery projects. You can create several Depots, each pointing to a different project.","title":"BigQuery"},{"location":"depots/bigquery/#requirements","text":"To connect to BigQuery, you need: Project id Email Credential properties in JSON file Additional parameters","title":"Requirements"},{"location":"depots/bigquery/#template","text":"To create a Depot of type 'BIGQUERY', use the following template: version : v1beta1 name : <depot-name> type : depot tags : - dropzone - bigquery owner : <owner-name> layer : user depot : type : BIGQUERY description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : projectid : <project-name> email : <email-id> files : json_keyfile : <json-file-path> - acl : r type : key-value-properties data : projectid : <project-name> email : <email-id> files : json_keyfile : <json-file-path> spec : # Data source specific configurations project : <project-name> params : \"key1\" : \"value1\" \"key2\" : \"value2\"","title":"Template"},{"location":"depots/compatibilitymatrix/","text":"Depot-Stack Compatibility Matrix \u00b6 Depot type Flare Benthos Minerva Amazon S3 RW/PUSHDOWN^ WIP WIP Azure Blob File Storage RW/PUSHDOWN^ WIP WIP BigQuery RW/PUSHDOWN WIP WIP Elasticsearch RW/PUSHDOWN WIP WIP Kafka RW WIP WIP Pulsar RW WIP WIP Redis RW/PUSHDOWN WIP WIP MySQL RW/PUSHDOWN WIP WIP JDBC RW/PUSHDOWN WIP WIP Google Cloud Storage RW/PUSHDOWN^ WIP WIP PostgreSQL RW/PUSHDOWN WIP WIP ^ Pushdown supported for Parquet format","title":"Compatibility Matrix"},{"location":"depots/compatibilitymatrix/#depot-stack-compatibility-matrix","text":"Depot type Flare Benthos Minerva Amazon S3 RW/PUSHDOWN^ WIP WIP Azure Blob File Storage RW/PUSHDOWN^ WIP WIP BigQuery RW/PUSHDOWN WIP WIP Elasticsearch RW/PUSHDOWN WIP WIP Kafka RW WIP WIP Pulsar RW WIP WIP Redis RW/PUSHDOWN WIP WIP MySQL RW/PUSHDOWN WIP WIP JDBC RW/PUSHDOWN WIP WIP Google Cloud Storage RW/PUSHDOWN^ WIP WIP PostgreSQL RW/PUSHDOWN WIP WIP ^ Pushdown supported for Parquet format","title":"Depot-Stack Compatibility Matrix"},{"location":"depots/concepts/","text":"A Minerva catalog contains schemas and references a data source via a connector.","title":"Concepts"},{"location":"depots/createdepots/","text":"Create Depots \u00b6 DataOS enables you to create a Depot using its declarative language. You specify a Depot definition using yaml format. Perform the following steps to create a Depot: \u00b6 1. Create a yaml config file \u00b6 The following sample yaml file explains the configuration properties and their values. version : v1beta1 # Resource API version here name : \"nameofdepo\" #(REQUIRED) depo will be referred with this name. # name of depot should be alphanumeric type : depot #(REQUIRED) resource type tags : #(OPTIONAL) helpful in classification and governance - dropzone - bigquery owner : bob_tmdc_io #(OPTIONAL) owner layer : user #(OPTIONAL) user or operator depot : type : <depottype> #(REQUIRED) depends on the underlying data source description : <description> #(OPTIONAL) helps in stating the purpose external : true #(REQUIRED) data is managed externally or by DataOS connectionSecret : #(OPTIONAL) connection secrets and access permissions - acl : r # Properties depend on the underlying data source type : key-value-properties data : awsaccesskeyid : <access-key-id> awsbucketname : <bucket-name> awssecretaccesskey : <secret-access-key> - acl : rw type : key-value-properties data : awsaccesskeyid : <access-key-id> awsbucketname : <bucket-name> awssecretaccesskey : <secret-access-key> spec : #(REQUIRED) depot connection specifications bucket : <bucket-name> # Properties depend on the underlying data source relativePath : \"raw\" 2. Run apply command to create a Depot \u00b6 dataos-ctl apply -f <yamlfilepath> -n <workspace> 3. Get a list of Depots in DataOS \u00b6 dataos-ctl get -t depot","title":"Create Depots"},{"location":"depots/createdepots/#create-depots","text":"DataOS enables you to create a Depot using its declarative language. You specify a Depot definition using yaml format.","title":"Create Depots"},{"location":"depots/createdepots/#perform-the-following-steps-to-create-a-depot","text":"","title":"Perform the following steps to create a Depot:"},{"location":"depots/createdepots/#1-create-a-yaml-config-file","text":"The following sample yaml file explains the configuration properties and their values. version : v1beta1 # Resource API version here name : \"nameofdepo\" #(REQUIRED) depo will be referred with this name. # name of depot should be alphanumeric type : depot #(REQUIRED) resource type tags : #(OPTIONAL) helpful in classification and governance - dropzone - bigquery owner : bob_tmdc_io #(OPTIONAL) owner layer : user #(OPTIONAL) user or operator depot : type : <depottype> #(REQUIRED) depends on the underlying data source description : <description> #(OPTIONAL) helps in stating the purpose external : true #(REQUIRED) data is managed externally or by DataOS connectionSecret : #(OPTIONAL) connection secrets and access permissions - acl : r # Properties depend on the underlying data source type : key-value-properties data : awsaccesskeyid : <access-key-id> awsbucketname : <bucket-name> awssecretaccesskey : <secret-access-key> - acl : rw type : key-value-properties data : awsaccesskeyid : <access-key-id> awsbucketname : <bucket-name> awssecretaccesskey : <secret-access-key> spec : #(REQUIRED) depot connection specifications bucket : <bucket-name> # Properties depend on the underlying data source relativePath : \"raw\"","title":"1. Create a yaml config file"},{"location":"depots/createdepots/#2-run-apply-command-to-create-a-depot","text":"dataos-ctl apply -f <yamlfilepath> -n <workspace>","title":"2. Run apply command to create a Depot"},{"location":"depots/createdepots/#3-get-a-list-of-depots-in-dataos","text":"dataos-ctl get -t depot","title":"3. Get a list of Depots in DataOS"},{"location":"depots/elasticsearch/","text":"Elasticsearch \u00b6 DataOS alllows you to connect to Elasticsearch data using Depot. The Depot enables the access to all documents visible to the specified user for text queries and analytics. Requirements \u00b6 To connect to Elasticsearch, you need: Username Password Nodes, Host name/url of the server and ports Template \u00b6 To create a Depot of type \u2018ELASTICSEARCH\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : ELASTICSEARCH description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw values : username : <username> password : <password> - acl : r values : username : <username> password : <password> spec : # Data source specific configurations nodes : [ \"localhost:9092\" , \"localhost:9093\" ]","title":"Elasticsearch"},{"location":"depots/elasticsearch/#elasticsearch","text":"DataOS alllows you to connect to Elasticsearch data using Depot. The Depot enables the access to all documents visible to the specified user for text queries and analytics.","title":"Elasticsearch"},{"location":"depots/elasticsearch/#requirements","text":"To connect to Elasticsearch, you need: Username Password Nodes, Host name/url of the server and ports","title":"Requirements"},{"location":"depots/elasticsearch/#template","text":"To create a Depot of type \u2018ELASTICSEARCH\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : ELASTICSEARCH description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw values : username : <username> password : <password> - acl : r values : username : <username> password : <password> spec : # Data source specific configurations nodes : [ \"localhost:9092\" , \"localhost:9093\" ]","title":"Template"},{"location":"depots/file/","text":"File \u00b6 DataOS allows you to create a Depot of type 'FILE' to read the CSV/ plain text data stored in the local file storage. Requirements \u00b6 To connect, you need: Path Template \u00b6 To create a Depot of type \u2018File\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : FILE description : <description> external : true spec : path : \"tmp/dataos\" # Data source specific configuration","title":"File"},{"location":"depots/file/#file","text":"DataOS allows you to create a Depot of type 'FILE' to read the CSV/ plain text data stored in the local file storage.","title":"File"},{"location":"depots/file/#requirements","text":"To connect, you need: Path","title":"Requirements"},{"location":"depots/file/#template","text":"To create a Depot of type \u2018File\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : FILE description : <description> external : true spec : path : \"tmp/dataos\" # Data source specific configuration","title":"Template"},{"location":"depots/googlecloudstorage/","text":"Google Cloud Storage \u00b6 DataOS allows you to create a Depot of type 'GCS' to read the data stored in the Google cloud storage. You can create several Depots, each pointing to a different bucket. Requirements \u00b6 To connect to Google Cloud Services, you need: Service account key Service account name Project id Email Bucket name Cluster Relative path Format Template \u00b6 To create a Depot of type \u2018GCS\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> owner : <owner-name> layer : user depot : type : GCS description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : projectid : <project-name> email : <email-id> files : gcskey_json : <json-file-path> - acl : r type : key-value-properties data : projectid : <project-name> email : <email-id> files : gcskey_json : <json-file-path> spec : # Data source specific configurations bucket : <project-name> relativePath : <relative-path> format : <format>","title":"Google Cloud Storage"},{"location":"depots/googlecloudstorage/#google-cloud-storage","text":"DataOS allows you to create a Depot of type 'GCS' to read the data stored in the Google cloud storage. You can create several Depots, each pointing to a different bucket.","title":"Google Cloud Storage"},{"location":"depots/googlecloudstorage/#requirements","text":"To connect to Google Cloud Services, you need: Service account key Service account name Project id Email Bucket name Cluster Relative path Format","title":"Requirements"},{"location":"depots/googlecloudstorage/#template","text":"To create a Depot of type \u2018GCS\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> owner : <owner-name> layer : user depot : type : GCS description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : projectid : <project-name> email : <email-id> files : gcskey_json : <json-file-path> - acl : r type : key-value-properties data : projectid : <project-name> email : <email-id> files : gcskey_json : <json-file-path> spec : # Data source specific configurations bucket : <project-name> relativePath : <relative-path> format : <format>","title":"Template"},{"location":"depots/jdbc/","text":"JDBC \u00b6 DataOS allows you to connect to a database with JDBC driver to read data from tables using Depot. The Depot enables access to all schemas visible to the specified user in the configured database. Requirements \u00b6 To connect to JDBC, you need: Database name Sub protocol name Host name/url of the server, port and parameters Username Password Template \u00b6 To create a Depot of type \u2018JDBC\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> owner : <owner-name> layer : user depot : type : JDBC description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : username : <jdbc-username> password : <jdbc-password> - acl : r type : key-value-properties data : username : <jdbc-username> password : <jdbc-password> spec : # Data source specific configurations subprotocol : <mysql|postgresql> host : <host> port : <port> database : <database-name> params : \"key1\" : \"value1\" \"key2\" : \"value2\"","title":"JDBC"},{"location":"depots/jdbc/#jdbc","text":"DataOS allows you to connect to a database with JDBC driver to read data from tables using Depot. The Depot enables access to all schemas visible to the specified user in the configured database.","title":"JDBC"},{"location":"depots/jdbc/#requirements","text":"To connect to JDBC, you need: Database name Sub protocol name Host name/url of the server, port and parameters Username Password","title":"Requirements"},{"location":"depots/jdbc/#template","text":"To create a Depot of type \u2018JDBC\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> owner : <owner-name> layer : user depot : type : JDBC description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : username : <jdbc-username> password : <jdbc-password> - acl : r type : key-value-properties data : username : <jdbc-username> password : <jdbc-password> spec : # Data source specific configurations subprotocol : <mysql|postgresql> host : <host> port : <port> database : <database-name> params : \"key1\" : \"value1\" \"key2\" : \"value2\"","title":"Template"},{"location":"depots/kafka/","text":"Kafka \u00b6 DataOS allows you to create a Depot of type 'KAFKA' to read live topic data. The created Depot enables you to read live streaming data. Requirements \u00b6 To connect to Kafka, you need: KAFKA broker list Once you provide the broker list, the Depot enables fetching all the topics in the KAFKA cluster. Template \u00b6 To create a Depot of type 'KAFKA', use the follwoing template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> owner : <owner-name> layer : user depot : type : KAFKA description : <description> external : true spec : # Data source specific configurations brokers : - <broker1> - <broker2> Note : The support for creating 'KAFKA' Depot using secrets in configuration is coming soon.","title":"Kafka"},{"location":"depots/kafka/#kafka","text":"DataOS allows you to create a Depot of type 'KAFKA' to read live topic data. The created Depot enables you to read live streaming data.","title":"Kafka"},{"location":"depots/kafka/#requirements","text":"To connect to Kafka, you need: KAFKA broker list Once you provide the broker list, the Depot enables fetching all the topics in the KAFKA cluster.","title":"Requirements"},{"location":"depots/kafka/#template","text":"To create a Depot of type 'KAFKA', use the follwoing template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> owner : <owner-name> layer : user depot : type : KAFKA description : <description> external : true spec : # Data source specific configurations brokers : - <broker1> - <broker2> Note : The support for creating 'KAFKA' Depot using secrets in configuration is coming soon.","title":"Template"},{"location":"depots/mysql/","text":"MYSQL \u00b6 DataOS allows you to connect to MYSQL database to read data from tables using Depots. The Depot enables access to all tables visible to the specified schema in the configured database. You can create as many Depots as you need to access additional MYSQL servers/databases. Requirements \u00b6 To connect to MYSQL database, you need: Host url and parameters Port Username Password Template \u00b6 To create a Depot of type \u2018MYSQL\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> owner : <owner-name> layer : user depot : type : MYSQL description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : username : <mysql-username> password : <mysql-password> - acl : r type : key-value-properties data : username : <mysql-username> password : <mysql-password> spec : # Data source specific configurations host : <host> port : <port> params : \"key1\" : \"value1\" \"key2\" : \"value2\"","title":"MYSQL"},{"location":"depots/mysql/#mysql","text":"DataOS allows you to connect to MYSQL database to read data from tables using Depots. The Depot enables access to all tables visible to the specified schema in the configured database. You can create as many Depots as you need to access additional MYSQL servers/databases.","title":"MYSQL"},{"location":"depots/mysql/#requirements","text":"To connect to MYSQL database, you need: Host url and parameters Port Username Password","title":"Requirements"},{"location":"depots/mysql/#template","text":"To create a Depot of type \u2018MYSQL\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> owner : <owner-name> layer : user depot : type : MYSQL description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : username : <mysql-username> password : <mysql-password> - acl : r type : key-value-properties data : username : <mysql-username> password : <mysql-password> spec : # Data source specific configurations host : <host> port : <port> params : \"key1\" : \"value1\" \"key2\" : \"value2\"","title":"Template"},{"location":"depots/oracle/","text":"Oracle \u00b6 DataOS allows you to connect to Oracle database to access data from the tables using Depots. The Depot enables access to all schemas visible to the specified service in the configured database. Connect to on-premise Oracle Database to perform various actions such as create, update, get, and delete on rows in a table. You can create as many Depots as you need to access additional Oracle servers/databases. Requirements \u00b6 To connect to Oracle, you need: URL of your Oracle account User name, typically your login user Password Database name Database schema where your table belongs Template \u00b6 To create a Depot of type \u2018ORACLE\u2018, use the following template: version : v1beta1 name : \"oracle01\" type : depot tags : - dropzone - oracle layer : user depot : type : ORACLE description : \"Oracle Sample data\" spec : host : \"xxxx.cb98qlrtcmrz.us-east-1.rds.amazonaws.com\" port : 1521 service : \"service_name\" external : true connectionSecret : - acl : rw type : key-value-properties data : username : tmdc password : dshfkjiwvkwefhwoavv","title":"Oracle"},{"location":"depots/oracle/#oracle","text":"DataOS allows you to connect to Oracle database to access data from the tables using Depots. The Depot enables access to all schemas visible to the specified service in the configured database. Connect to on-premise Oracle Database to perform various actions such as create, update, get, and delete on rows in a table. You can create as many Depots as you need to access additional Oracle servers/databases.","title":"Oracle"},{"location":"depots/oracle/#requirements","text":"To connect to Oracle, you need: URL of your Oracle account User name, typically your login user Password Database name Database schema where your table belongs","title":"Requirements"},{"location":"depots/oracle/#template","text":"To create a Depot of type \u2018ORACLE\u2018, use the following template: version : v1beta1 name : \"oracle01\" type : depot tags : - dropzone - oracle layer : user depot : type : ORACLE description : \"Oracle Sample data\" spec : host : \"xxxx.cb98qlrtcmrz.us-east-1.rds.amazonaws.com\" port : 1521 service : \"service_name\" external : true connectionSecret : - acl : rw type : key-value-properties data : username : tmdc password : dshfkjiwvkwefhwoavv","title":"Template"},{"location":"depots/postgresql/","text":"PostgreSQL \u00b6 DataOS allows you to connect to PostgreSQL database to read data from tables using Depot. The Depot enables access to all schemas visible to the specified user in the configured database. Requirements \u00b6 To create a Depot to connect to PostgreSQL database, you need: Database name Host name/url of the server and parameters Username Password Template \u00b6 To create a Depot of type \u2018POSTGRESQL\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> owner : <owner-name> layer : user depot : type : POSTGRESQL description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : username : <posgresql-username> password : <posgresql-password> - acl : r type : key-value-properties data : username : <posgresql-username> password : <posgresql-password> spec : # Data source specific configurations host : <host> port : <port> database : <database-name> params : \"key1\" : \"value1\" \"key2\" : \"value2\"","title":"PostgreSQL"},{"location":"depots/postgresql/#postgresql","text":"DataOS allows you to connect to PostgreSQL database to read data from tables using Depot. The Depot enables access to all schemas visible to the specified user in the configured database.","title":"PostgreSQL"},{"location":"depots/postgresql/#requirements","text":"To create a Depot to connect to PostgreSQL database, you need: Database name Host name/url of the server and parameters Username Password","title":"Requirements"},{"location":"depots/postgresql/#template","text":"To create a Depot of type \u2018POSTGRESQL\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> owner : <owner-name> layer : user depot : type : POSTGRESQL description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : username : <posgresql-username> password : <posgresql-password> - acl : r type : key-value-properties data : username : <posgresql-username> password : <posgresql-password> spec : # Data source specific configurations host : <host> port : <port> database : <database-name> params : \"key1\" : \"value1\" \"key2\" : \"value2\"","title":"Template"},{"location":"depots/presto/","text":"Presto \u00b6 DataOS allows you to connect to Presto query engine for fast analytic queries against data of any size using Depot. The Depot enables access to all catalogs visible to the specified user in the configured query engine. Requirements \u00b6 To connect to Presto, you need: Host name and port Username Password Catalog name Template \u00b6 To create a Depot of type \u2018PRESTO\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : PRESTO description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw values : username : <username> password : <password> - acl : r values : username : <username> password : <password> spec : # Data source specific configurations host : \"localhost\" port : \"5432\" catalog : \"postgres\"","title":"Presto"},{"location":"depots/presto/#presto","text":"DataOS allows you to connect to Presto query engine for fast analytic queries against data of any size using Depot. The Depot enables access to all catalogs visible to the specified user in the configured query engine.","title":"Presto"},{"location":"depots/presto/#requirements","text":"To connect to Presto, you need: Host name and port Username Password Catalog name","title":"Requirements"},{"location":"depots/presto/#template","text":"To create a Depot of type \u2018PRESTO\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : PRESTO description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw values : username : <username> password : <password> - acl : r values : username : <username> password : <password> spec : # Data source specific configurations host : \"localhost\" port : \"5432\" catalog : \"postgres\"","title":"Template"},{"location":"depots/prodreadiness2/","text":"1 We are able to process bigger datasets. 2 Need to calculate upfront what is the size of disk that needs to be attached while processing bigger datasets. 3 Data partitioning is not synced with catalog right now - Check it once. 4 Validation/Error Message when user changes partition strategy in yaml in subsequent runs. 5 Bring variety of data in from different sources. 6 Incremental data pulling needs to be enhanced. Rakesh/Deenkar were not satisfied with the current implementation. 7 Strategy to bring data in from multiple tables of some depot. Can we make it smart? 8 Fix intermittent timeout issue in metiswherever. 9 Verify wrangler once more. 10 Verify stuff generated by flare assertions. 11 Better error handling at cli/poros in scenarios like pod is not getting up and things like that. (e.g. image pull failed.) 12 Enhance scanner for more depot types. 13 Enhance metis to make things composable instead of inheritance. 14 Materialized view feature is running behind. Do we still need that if yes we need to revisit it. 15 Apache pulsar firstclass support in spark 3 verification(we can use kafka flavour of pulsar meanwhile if it works fine with spark3). 16 Redsfhit/Snowflake support proper validation and testing with decent data. 17 Accepting bad record dir as dataos address for inputs. 18 Some sugar things around error data handling in flare. 19 Use mounted secrets in toolkit and scanner. 20 Securing credentials in jupyter notebook. 21 Polish jupyter notebook. 22 Deal with a variety of stuff with benthos. 23 Revisit information sent in audit(Piyush) 24 Do we have a first class connector for presto/trino. Using jdbc we kind of stringify the object type of fields. 25 Flare - Dataset registration via listener (on a low burner). 26 Scanner - PowerBI/Tableau/Trino support. 27 Metis - Add support to index dashboards/entities. 28 Flare - Column level lineage tracker. 29 Topology publishing from all runnables. 30 Reading/Writing stuff via spark jdbc/odbc server. 31 Bring flink into dataos ecosystem. 32 Add support to make dashboard and other stuff declarative. So that we can have all such stuff ready after installation. 33 Review modern help.","title":"Prodreadiness2"},{"location":"depots/pulsar/","text":"Apache Pulsar \u00b6 DataOS allows you to create a Depot of type 'PULSAR' to read the topics/messages stored in Pulsar. The created Depot enables you to read the published topics and process incoming stream of messages. Requirements \u00b6 To connect to Pulsar, you need: Admin url Service url Template \u00b6 To create a Depot of type \u2018PULSAR\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : PULSAR description : <description> external : true spec : # Data source specific configurations adminUrl : <admin-url> serviceUrl : <service-url>","title":"Apache Pulsar"},{"location":"depots/pulsar/#apache-pulsar","text":"DataOS allows you to create a Depot of type 'PULSAR' to read the topics/messages stored in Pulsar. The created Depot enables you to read the published topics and process incoming stream of messages.","title":"Apache Pulsar"},{"location":"depots/pulsar/#requirements","text":"To connect to Pulsar, you need: Admin url Service url","title":"Requirements"},{"location":"depots/pulsar/#template","text":"To create a Depot of type \u2018PULSAR\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : PULSAR description : <description> external : true spec : # Data source specific configurations adminUrl : <admin-url> serviceUrl : <service-url>","title":"Template"},{"location":"depots/redis/","text":"Redis \u00b6 DataOS allows you to connect to Redis using Depot. The Depot then enables querying of live data stored in Redis and provides access to all of Redis' data structures - String, Hash, List, Set and Sorted Set. You can also read DataFrames. Requirements \u00b6 To connect to Redis, you need: Host name Port DB name Table Password Template \u00b6 To create a Depot of type \u2018REDIS\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : REDIS description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw values : password : <password> - acl : r values : password : <password> spec : # Data source specific configurations host : \"localhost\" port : 5432 db : 10 table : \"user\" # @Dev , do we not need scema name here","title":"Redis"},{"location":"depots/redis/#redis","text":"DataOS allows you to connect to Redis using Depot. The Depot then enables querying of live data stored in Redis and provides access to all of Redis' data structures - String, Hash, List, Set and Sorted Set. You can also read DataFrames.","title":"Redis"},{"location":"depots/redis/#requirements","text":"To connect to Redis, you need: Host name Port DB name Table Password","title":"Requirements"},{"location":"depots/redis/#template","text":"To create a Depot of type \u2018REDIS\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : REDIS description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw values : password : <password> - acl : r values : password : <password> spec : # Data source specific configurations host : \"localhost\" port : 5432 db : 10 table : \"user\" # @Dev , do we not need scema name here","title":"Template"},{"location":"depots/snowflake/","text":"Snowflake \u00b6 DataOS allows you to connect to Snowflake to read data from Snowflakes tables using Depots. Snowflake database is a purely cloud-based data storage and analytics data warehouse provided as a Software-as-a-Service (SaaS). An entirely new SQL database engine (similar to ANSI SQL syntax and features) is designed to work with cloud infrastructure to access Snowflake database. Requirements \u00b6 To connect to Snowflake, you need: URL of your Snowflake account Snowflake user name, typically your login user Snowflake user password Snowflake Database name Database schema where your table belongs Template \u00b6 To create a Depot of type \u2018SNOWFLAKE\u2018, use the following template: version : v1beta1 name : \"snowflake01\" type : depot tags : - dropzone - snowflake layer : user depot : type : SNOWFLAKE description : \"Snowflake Sample data\" spec : url : MUA15126.snowflakecomputing.com database : \"SF_TUTS\" external : true connectionSecret : - acl : rw type : key-value-properties data : username : tmdc password : Rubik@123","title":"Snowflake"},{"location":"depots/snowflake/#snowflake","text":"DataOS allows you to connect to Snowflake to read data from Snowflakes tables using Depots. Snowflake database is a purely cloud-based data storage and analytics data warehouse provided as a Software-as-a-Service (SaaS). An entirely new SQL database engine (similar to ANSI SQL syntax and features) is designed to work with cloud infrastructure to access Snowflake database.","title":"Snowflake"},{"location":"depots/snowflake/#requirements","text":"To connect to Snowflake, you need: URL of your Snowflake account Snowflake user name, typically your login user Snowflake user password Snowflake Database name Database schema where your table belongs","title":"Requirements"},{"location":"depots/snowflake/#template","text":"To create a Depot of type \u2018SNOWFLAKE\u2018, use the following template: version : v1beta1 name : \"snowflake01\" type : depot tags : - dropzone - snowflake layer : user depot : type : SNOWFLAKE description : \"Snowflake Sample data\" spec : url : MUA15126.snowflakecomputing.com database : \"SF_TUTS\" external : true connectionSecret : - acl : rw type : key-value-properties data : username : tmdc password : Rubik@123","title":"Template"},{"location":"depots/typeofdepots/","text":"Type of Depots and their configurations \u00b6 This article lists the type of Depots that can be created in DataOS and describes their detailed configuration settings. Type of Depots \u00b6 The type of the Depot provides the information about the location of the data source which is being accessed. You can create the following type of Depots in DataOS to access data stored in various data sources. Amazon S3 Apache Pulsar Azure Blob Storage Azure Data Lake Gen2 Bigquery Elasticsearch File Google Cloud Storage JDBC Kafka MYSQL Oracle PostgreSQL Presto Redis Snowflake Configuration properties \u00b6 General \u00b6 While creating a yaml config file for the Depot, some properties that define the Depot are common to all the Depot types. For example: version : v1beta1 name : \"nameofdepo\" type : depot tags : -dropzone -bigquery owner : bob_tmdc_io layer : user depot : type : description : external : true Connection secrets \u00b6 In this section, provide credentials/passwords and permissions to access the data source. These properies are specific to the Depot type for underlying data source. You may need to provide the following: acl : You can create multiple connections for a single project with different access levels such as read & write or only read permissions. data : Provide the credentials and additional information needed to connect with the data source such as accesskeyid, secretkeyid. project name and email. files : You can store sensitive credential information in json file. Provide the absolute path to the JSON credentials file. Spec \u00b6 In this section, provide configuration options based on Depot type for specific data source. For example, for KAFKA type depo, you need to give brokerlist whereas for S3 type Depot, you have to provide bucket and relative path in the native storage system. Connection secret examples \u00b6 The following example shows configuration of 'BIGQUERY' type Depot with a 'read' and 'read&write' access to a bigquery project. Here credentials are given in json file. version : v1beta1 name : \"crmbq\" type : depot tags : - dropzone - bigquery layer : user depot : type : BIGQUERY description : \"Google Cloud BigQuery\" external : true connectionSecret : - acl : rw type : key-value-properties files : json_keyfile : secrets/gcp-demo-sa.json - acl : r type : key-value-properties files : json_keyfile : secrets/gcp-demo-sa.json spec : project : dataos-ck-res-yak-dev 2. The following example shows creation of 'S3' type Depot specifying access key id and secret key. The credentials you use in this properties file need to have access to the S3 bucket we will read from. version : v1beta1 name : \"poss3\" type : depot tags : - dropzone - bigquery layer : user depot : type : S3 description : \"AWS S3 Bucket for POS Data\" external : true connectionSecret : - acl : rw type : key-value-properties data : awsaccesskeyid : ${AWS_ACCESS_KEY_ID} awsbucketname:${bucket-name} awssecretaccesskey : ${AWS_SECRET_ACCESS_KEY} spec : bucket : \"tmdc-dataos\" relativePath : \"/demo-mockdata\" 3. The following example shows creation of 'GCS' type Depot specifying secrets in a json file. version : v1beta1 name : \"syndicationgcs\" type : depot tags : - dropzone - bigquery layer : user depot : type : GCS description : \"GC Storage Bucket for Syndication Data\" external : true connectionSecret : - acl : rw type : key-value-properties data : gcskey_json : secrets/gcp-demo-sa.json projectid : dataos-ck-res-yak-dev email : ds-demo-reader@dataos-ck-res-yak-dev.iam.gserviceaccount.com spec : bucket : \"ds-demo-syndication-output\" relativePath : \"outgoing\" Spec examples \u00b6 In DataOS, you access the data in the following way: dataos://depot:collection/dataset depot \u2192 collection \u2192dataset So you have to map the corresponding storage structure of external data sources to access the data within DataOS in a desired way.","title":"Type of Depots"},{"location":"depots/typeofdepots/#type-of-depots-and-their-configurations","text":"This article lists the type of Depots that can be created in DataOS and describes their detailed configuration settings.","title":"Type of  Depots and their configurations"},{"location":"depots/typeofdepots/#type-of-depots","text":"The type of the Depot provides the information about the location of the data source which is being accessed. You can create the following type of Depots in DataOS to access data stored in various data sources. Amazon S3 Apache Pulsar Azure Blob Storage Azure Data Lake Gen2 Bigquery Elasticsearch File Google Cloud Storage JDBC Kafka MYSQL Oracle PostgreSQL Presto Redis Snowflake","title":"Type of Depots"},{"location":"depots/typeofdepots/#configuration-properties","text":"","title":"Configuration properties"},{"location":"depots/typeofdepots/#general","text":"While creating a yaml config file for the Depot, some properties that define the Depot are common to all the Depot types. For example: version : v1beta1 name : \"nameofdepo\" type : depot tags : -dropzone -bigquery owner : bob_tmdc_io layer : user depot : type : description : external : true","title":"General"},{"location":"depots/typeofdepots/#connection-secrets","text":"In this section, provide credentials/passwords and permissions to access the data source. These properies are specific to the Depot type for underlying data source. You may need to provide the following: acl : You can create multiple connections for a single project with different access levels such as read & write or only read permissions. data : Provide the credentials and additional information needed to connect with the data source such as accesskeyid, secretkeyid. project name and email. files : You can store sensitive credential information in json file. Provide the absolute path to the JSON credentials file.","title":"Connection secrets"},{"location":"depots/typeofdepots/#spec","text":"In this section, provide configuration options based on Depot type for specific data source. For example, for KAFKA type depo, you need to give brokerlist whereas for S3 type Depot, you have to provide bucket and relative path in the native storage system.","title":"Spec"},{"location":"depots/typeofdepots/#connection-secret-examples","text":"The following example shows configuration of 'BIGQUERY' type Depot with a 'read' and 'read&write' access to a bigquery project. Here credentials are given in json file. version : v1beta1 name : \"crmbq\" type : depot tags : - dropzone - bigquery layer : user depot : type : BIGQUERY description : \"Google Cloud BigQuery\" external : true connectionSecret : - acl : rw type : key-value-properties files : json_keyfile : secrets/gcp-demo-sa.json - acl : r type : key-value-properties files : json_keyfile : secrets/gcp-demo-sa.json spec : project : dataos-ck-res-yak-dev 2. The following example shows creation of 'S3' type Depot specifying access key id and secret key. The credentials you use in this properties file need to have access to the S3 bucket we will read from. version : v1beta1 name : \"poss3\" type : depot tags : - dropzone - bigquery layer : user depot : type : S3 description : \"AWS S3 Bucket for POS Data\" external : true connectionSecret : - acl : rw type : key-value-properties data : awsaccesskeyid : ${AWS_ACCESS_KEY_ID} awsbucketname:${bucket-name} awssecretaccesskey : ${AWS_SECRET_ACCESS_KEY} spec : bucket : \"tmdc-dataos\" relativePath : \"/demo-mockdata\" 3. The following example shows creation of 'GCS' type Depot specifying secrets in a json file. version : v1beta1 name : \"syndicationgcs\" type : depot tags : - dropzone - bigquery layer : user depot : type : GCS description : \"GC Storage Bucket for Syndication Data\" external : true connectionSecret : - acl : rw type : key-value-properties data : gcskey_json : secrets/gcp-demo-sa.json projectid : dataos-ck-res-yak-dev email : ds-demo-reader@dataos-ck-res-yak-dev.iam.gserviceaccount.com spec : bucket : \"ds-demo-syndication-output\" relativePath : \"outgoing\"","title":"Connection secret examples"},{"location":"depots/typeofdepots/#spec-examples","text":"In DataOS, you access the data in the following way: dataos://depot:collection/dataset depot \u2192 collection \u2192dataset So you have to map the corresponding storage structure of external data sources to access the data within DataOS in a desired way.","title":"Spec examples"},{"location":"depots/windowsazurstorageblob/","text":"Azure Data Lake Storage Gen2 \u00b6 DataOS allows you to create a Depot of type 'WASBS' to support reading data stored in an Azure Data Lake Storage. This Depot enables all access through a storage account. This storage account may have multiple containers. A container is a grouping of multiple blobs. Define separate Depot for each container. Requirements \u00b6 To connect to Azure Data Lake Storage, you need: Storage account name Storage account key Container Relative path Format Template \u00b6 To create a Depot of type \u2018WASBS\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : WASBS description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : azurestorageaccountname : <account-name> azurestorageaccountkey : <account-key> - acl : r type : key-value-properties data : azurestorageaccountname : <account-name> azurestorageaccountkey : <account-key> spec : # Data source specific configurations account : <account-name> container : <container-name> relativePath : <relative-path> format : <format>","title":"Azure Data Lake Storage Gen2"},{"location":"depots/windowsazurstorageblob/#azure-data-lake-storage-gen2","text":"DataOS allows you to create a Depot of type 'WASBS' to support reading data stored in an Azure Data Lake Storage. This Depot enables all access through a storage account. This storage account may have multiple containers. A container is a grouping of multiple blobs. Define separate Depot for each container.","title":"Azure Data Lake Storage Gen2"},{"location":"depots/windowsazurstorageblob/#requirements","text":"To connect to Azure Data Lake Storage, you need: Storage account name Storage account key Container Relative path Format","title":"Requirements"},{"location":"depots/windowsazurstorageblob/#template","text":"To create a Depot of type \u2018WASBS\u2018, use the following template: version : v1beta1 name : <depot-name> type : depot tags : - <tag1> - <tag2> owner : <owner-name> layer : user depot : type : WASBS description : <description> external : true connectionSecret : # Data source specific configurations - acl : rw type : key-value-properties data : azurestorageaccountname : <account-name> azurestorageaccountkey : <account-key> - acl : r type : key-value-properties data : azurestorageaccountname : <account-name> azurestorageaccountkey : <account-key> spec : # Data source specific configurations account : <account-name> container : <container-name> relativePath : <relative-path> format : <format>","title":"Template"},{"location":"flare/","text":"DataOS Flare \u00b6 Introduction \u00b6 Flare is a declarative stack for Apache Spark workflows/ jobs. Flare makes creating and triggering a job much easier and enables the engineer to create fairly complex workflows to carry out the data processing tasks. The workflows can have one or more jobs, and are defined in the form of a sequential yaml. Essential concepts and terminology \u00b6 YAML, essentially a data serialization language, works here as a file holding key-value pairs needed to run the workflow. The yaml for a Flare workflow involves many inputs which you have to provide in key-value pair. Sample YAML: version : v1beta1 name : connect-ny-taxi type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and write with partitioning on vendor_id workflow : title : Connect NY Taxi dag : - name : customer title : Customer Dimension Ingester Data Iceberg description : The job ingests customer data from dropzone into raw zone using hive catalog spec : tags : - Connect - Customer stack : flare:1.0 tier : connect flare : driver : coreLimit : 1200m cores : 1 memory : 1024m executor : coreLimit : 1200m cores : 1 instances : 1 memory : 1024m job : explain : true inputs : - name : transactions_connect dataset : /datadir/transactions?acl=r format : json - name : querydata dataset : /datadir/querydata?acl=r format : json # format: csv # schemaPath: dataos://thirdparty01:default/schemas/avsc/customer.avsc logLevel : ERROR outputs : - name : output01 depot : dataos://raw01:default?acl=rw steps : - sink : - sequenceName : customers datasetName : customer_iceberg_02 outputName : output01 outputType : Iceberg description : Customer data ingested from external csv to Iceberg using hive catalog outputOptions : saveMode : overwrite Property Value version Holds the current version of Flare name Name of the job/ workflow type The type could be a workflow, job, Depot etc tags Any attributes qualifying the workflow/ job description Text describing the workflow/ job owner The ID of the author of the job workflow An array of essential key-value pairs dag Directed acyclic graph, since the workflow is acyclic stack Flare stack to run the workflow tier Job type; Connect/ Rio/ Syndicate driver The controller node of the operation executor The worker node of the operation inputs This section can be multiple in number, an array of 3 or more key-value pairs, multiple datasets can be passed in the same format. Following are the key-value pairs name: Name of the dataset, this name will be used for reference later, this is mandatory format: File format of the dataset, parquet is considered by default dataset: Address of the dataset outputs An array of 2 key-value pairs. Following are the key-value pairs name: Name to be provided to the new dataset; the name can be used for reference later depot: A Depot is a container which can have multiple datasets. Here, the address of the target Depot(where the dataset will belong) is passed. steps The steps of the operation; these steps can be referenced via a separate yaml as well. This involves multiple key-value pairs sequence: Order or operations, further has two key-value pairs: name: Sequence name, to be used as reference for the output of the SQL query it has, in the form of a table or a view sql: SQL query of the operation in Spark SQL format sink Details of the output dataset, following are the key-value pairs for the sink sequenceName: The name of the sequence relevant for the sink datasetName: Name assigned to the new dataset outputName: Name defined in the outputs section above outputType: New dataset's file format outputOptions: Different options depending on different outputTypes partitionBy: Column used for partitioning the data. A good practice dictates partitioning at the Timestamp column","title":"Introduction"},{"location":"flare/#dataos-flare","text":"","title":"DataOS Flare"},{"location":"flare/#introduction","text":"Flare is a declarative stack for Apache Spark workflows/ jobs. Flare makes creating and triggering a job much easier and enables the engineer to create fairly complex workflows to carry out the data processing tasks. The workflows can have one or more jobs, and are defined in the form of a sequential yaml.","title":"Introduction"},{"location":"flare/#essential-concepts-and-terminology","text":"YAML, essentially a data serialization language, works here as a file holding key-value pairs needed to run the workflow. The yaml for a Flare workflow involves many inputs which you have to provide in key-value pair. Sample YAML: version : v1beta1 name : connect-ny-taxi type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and write with partitioning on vendor_id workflow : title : Connect NY Taxi dag : - name : customer title : Customer Dimension Ingester Data Iceberg description : The job ingests customer data from dropzone into raw zone using hive catalog spec : tags : - Connect - Customer stack : flare:1.0 tier : connect flare : driver : coreLimit : 1200m cores : 1 memory : 1024m executor : coreLimit : 1200m cores : 1 instances : 1 memory : 1024m job : explain : true inputs : - name : transactions_connect dataset : /datadir/transactions?acl=r format : json - name : querydata dataset : /datadir/querydata?acl=r format : json # format: csv # schemaPath: dataos://thirdparty01:default/schemas/avsc/customer.avsc logLevel : ERROR outputs : - name : output01 depot : dataos://raw01:default?acl=rw steps : - sink : - sequenceName : customers datasetName : customer_iceberg_02 outputName : output01 outputType : Iceberg description : Customer data ingested from external csv to Iceberg using hive catalog outputOptions : saveMode : overwrite Property Value version Holds the current version of Flare name Name of the job/ workflow type The type could be a workflow, job, Depot etc tags Any attributes qualifying the workflow/ job description Text describing the workflow/ job owner The ID of the author of the job workflow An array of essential key-value pairs dag Directed acyclic graph, since the workflow is acyclic stack Flare stack to run the workflow tier Job type; Connect/ Rio/ Syndicate driver The controller node of the operation executor The worker node of the operation inputs This section can be multiple in number, an array of 3 or more key-value pairs, multiple datasets can be passed in the same format. Following are the key-value pairs name: Name of the dataset, this name will be used for reference later, this is mandatory format: File format of the dataset, parquet is considered by default dataset: Address of the dataset outputs An array of 2 key-value pairs. Following are the key-value pairs name: Name to be provided to the new dataset; the name can be used for reference later depot: A Depot is a container which can have multiple datasets. Here, the address of the target Depot(where the dataset will belong) is passed. steps The steps of the operation; these steps can be referenced via a separate yaml as well. This involves multiple key-value pairs sequence: Order or operations, further has two key-value pairs: name: Sequence name, to be used as reference for the output of the SQL query it has, in the form of a table or a view sql: SQL query of the operation in Spark SQL format sink Details of the output dataset, following are the key-value pairs for the sink sequenceName: The name of the sequence relevant for the sink datasetName: Name assigned to the new dataset outputName: Name defined in the outputs section above outputType: New dataset's file format outputOptions: Different options depending on different outputTypes partitionBy: Column used for partitioning the data. A good practice dictates partitioning at the Timestamp column","title":"Essential concepts and terminology"},{"location":"flare/compactingfiles/","text":"Merging Small Files \u00b6 Overview \u00b6 This use case describes Flare job to efficiently perform metadata and data compaction on Iceberg tables. To write values from streaming query to Iceberg table, these queries can create new table versions quickly, which creates lots of table metadata to track those versions. The amount of data written in a micro batch is typically small, which can cause the table metadata to track lots of small files. You can write a Flare job for compacting small files into larger files to reduce the metadata overhead and runtime file open cost. While running at petabyte scale, it is important to increase storage as well as compute efficiency. Solution approach \u00b6 Iceberg tracks each data file in a table. More data files leads to more metadata stored in manifest files, and small data files causes an unnecessary amount of metadata and less efficient queries. This example Flare job will compact data/metadata files in Iceberg with the rewriteDataFiles action. Implementation details \u00b6 The follwoing actions are implemented in the re-write scenario. A workflow is defined with a job that compacts NY taxi data. You can configure YAML for the following actions. Rewrite manifests \u00b6 Iceberg may write the new snapshot with a \u201cfast\u201d append that does not automatically compact manifests which could lead to lots of small manifest files. Manifests can be rewritten to optimize queries and to compact. --- version : v1beta1 name : com-nytaxi-01 type : workflow tags : - Compression - NyTaxi description : The job Compress NyTaxi data workflow : title : Compression NyTaxi dag : - name : com-nytaxi01 title : Compression NyTaxi description : The job Compress NyTaxi data spec : tags : - Compression - NyTaxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://icebase:raw01/ny_taxi_01?acl=r logLevel : INFO actions : - name : rewrite_manifest options : specId : \"\" # optional stagingLocation : \"\" # optional useCaching : false # optional outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw Expire snapshots \u00b6 In Iceberg, each micro-batch written to a table produces a new snapshot, which are tracked in table metadata. Snapshots accumulate quickly with frequent commits, so it is highly recommended that tables written by streaming queries are regularly maintained. This action will remove old snapshots and related data files that are no longer needed. --- version : v1beta1 name : com-nytaxi-01 type : workflow tags : - Compression - NyTaxi description : The job Compress NyTaxi data workflow : title : Compression NyTaxi dag : - name : com-nytaxi01 title : Compression NyTaxi description : The job Compress NyTaxi data spec : tags : - Compression - NyTaxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://icebase:raw01/ny_taxi_01?acl=r logLevel : INFO actions : - name : expire_snapshots options : expireOlderThan : \"1627500040443\" # expire all snapshots older than this timestamp millis. expireSnapshotId : 1343444444 # specific snapshot to expire. retainLast : 3000 # retail most recent 10 snapshts. streamDeleteResults : false #By default, all files to delete are brought to the driver at once which may be an issue with very long file lists. Set this to true to use toLocalIterator if you are running into memory issues when collecting the list of files to be deleted. outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw Remove orphan files \u00b6 This action will remove files which are not referenced in any metadata files of an Iceberg table and can thus be considered \u201corphaned\u201d. --- version : v1beta1 name : com-nytaxi-01 type : workflow tags : - Compression - NyTaxi description : The job Compress NyTaxi data workflow : title : Compression NyTaxi dag : - name : com-nytaxi01 title : Compression NyTaxi description : The job Compress NyTaxi data spec : tags : - Compression - NyTaxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://icebase:raw01/ny_taxi_01?acl=r logLevel : INFO actions : - name : remove_orphans options : olderThan : \"1627500040443\" # older than this time. location : \"\" # Removes orphan files in the given location. outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw Rewrite dataset \u00b6 This action will rewrite and compact the generated small data files. --- version : v1beta1 name : com-nytaxi-01 type : workflow tags : - Compression - NyTaxi description : The job Compress NyTaxi data workflow : title : Compression NyTaxi dag : - name : com-nytaxi01 title : Compression NyTaxi description : The job Compress NyTaxi data spec : tags : - Compression - NyTaxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://icebase:raw01/ny_taxi_01?acl=r logLevel : INFO actions : - name : rewrite_dataset options : filter : \"fiter expression here\" splitOpenFileCost : 3434343 # Specify the minimum file size to count to pack into one \"bin\" splitLookback : 3 targetSizeInBytes : 54545 # Specify the target rewrite data file size in bytes outputSpecId : 3 # Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite caseSensitive : false # Is it case sensitive outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw Outcomes \u00b6 if there are many files in some partition or non partitioned data, it will merge all small files into a large one.","title":"Merging Small Files"},{"location":"flare/compactingfiles/#merging-small-files","text":"","title":"Merging Small Files"},{"location":"flare/compactingfiles/#overview","text":"This use case describes Flare job to efficiently perform metadata and data compaction on Iceberg tables. To write values from streaming query to Iceberg table, these queries can create new table versions quickly, which creates lots of table metadata to track those versions. The amount of data written in a micro batch is typically small, which can cause the table metadata to track lots of small files. You can write a Flare job for compacting small files into larger files to reduce the metadata overhead and runtime file open cost. While running at petabyte scale, it is important to increase storage as well as compute efficiency.","title":"Overview"},{"location":"flare/compactingfiles/#solution-approach","text":"Iceberg tracks each data file in a table. More data files leads to more metadata stored in manifest files, and small data files causes an unnecessary amount of metadata and less efficient queries. This example Flare job will compact data/metadata files in Iceberg with the rewriteDataFiles action.","title":"Solution approach"},{"location":"flare/compactingfiles/#implementation-details","text":"The follwoing actions are implemented in the re-write scenario. A workflow is defined with a job that compacts NY taxi data. You can configure YAML for the following actions.","title":"Implementation details"},{"location":"flare/compactingfiles/#rewrite-manifests","text":"Iceberg may write the new snapshot with a \u201cfast\u201d append that does not automatically compact manifests which could lead to lots of small manifest files. Manifests can be rewritten to optimize queries and to compact. --- version : v1beta1 name : com-nytaxi-01 type : workflow tags : - Compression - NyTaxi description : The job Compress NyTaxi data workflow : title : Compression NyTaxi dag : - name : com-nytaxi01 title : Compression NyTaxi description : The job Compress NyTaxi data spec : tags : - Compression - NyTaxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://icebase:raw01/ny_taxi_01?acl=r logLevel : INFO actions : - name : rewrite_manifest options : specId : \"\" # optional stagingLocation : \"\" # optional useCaching : false # optional outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw","title":"Rewrite manifests"},{"location":"flare/compactingfiles/#expire-snapshots","text":"In Iceberg, each micro-batch written to a table produces a new snapshot, which are tracked in table metadata. Snapshots accumulate quickly with frequent commits, so it is highly recommended that tables written by streaming queries are regularly maintained. This action will remove old snapshots and related data files that are no longer needed. --- version : v1beta1 name : com-nytaxi-01 type : workflow tags : - Compression - NyTaxi description : The job Compress NyTaxi data workflow : title : Compression NyTaxi dag : - name : com-nytaxi01 title : Compression NyTaxi description : The job Compress NyTaxi data spec : tags : - Compression - NyTaxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://icebase:raw01/ny_taxi_01?acl=r logLevel : INFO actions : - name : expire_snapshots options : expireOlderThan : \"1627500040443\" # expire all snapshots older than this timestamp millis. expireSnapshotId : 1343444444 # specific snapshot to expire. retainLast : 3000 # retail most recent 10 snapshts. streamDeleteResults : false #By default, all files to delete are brought to the driver at once which may be an issue with very long file lists. Set this to true to use toLocalIterator if you are running into memory issues when collecting the list of files to be deleted. outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw","title":"Expire snapshots"},{"location":"flare/compactingfiles/#remove-orphan-files","text":"This action will remove files which are not referenced in any metadata files of an Iceberg table and can thus be considered \u201corphaned\u201d. --- version : v1beta1 name : com-nytaxi-01 type : workflow tags : - Compression - NyTaxi description : The job Compress NyTaxi data workflow : title : Compression NyTaxi dag : - name : com-nytaxi01 title : Compression NyTaxi description : The job Compress NyTaxi data spec : tags : - Compression - NyTaxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://icebase:raw01/ny_taxi_01?acl=r logLevel : INFO actions : - name : remove_orphans options : olderThan : \"1627500040443\" # older than this time. location : \"\" # Removes orphan files in the given location. outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw","title":"Remove orphan files"},{"location":"flare/compactingfiles/#rewrite-dataset","text":"This action will rewrite and compact the generated small data files. --- version : v1beta1 name : com-nytaxi-01 type : workflow tags : - Compression - NyTaxi description : The job Compress NyTaxi data workflow : title : Compression NyTaxi dag : - name : com-nytaxi01 title : Compression NyTaxi description : The job Compress NyTaxi data spec : tags : - Compression - NyTaxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://icebase:raw01/ny_taxi_01?acl=r logLevel : INFO actions : - name : rewrite_dataset options : filter : \"fiter expression here\" splitOpenFileCost : 3434343 # Specify the minimum file size to count to pack into one \"bin\" splitLookback : 3 targetSizeInBytes : 54545 # Specify the target rewrite data file size in bytes outputSpecId : 3 # Pass a PartitionSpec id to specify which PartitionSpec should be used in DataFile rewrite caseSensitive : false # Is it case sensitive outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw","title":"Rewrite dataset"},{"location":"flare/compactingfiles/#outcomes","text":"if there are many files in some partition or non partitioned data, it will merge all small files into a large one.","title":"Outcomes"},{"location":"flare/concurrent/","text":"Concurrent writes \u00b6 Overview \u00b6 This use case tests the scenario where multiple jobs are concurrently writing at the same location. Solution approach \u00b6 This scenario works only for Iceberg as it supports multiple concurrent writes. A workflow with two jobs is defined to write at the same data location. Implementation Details \u00b6 This use case is tested on NY-Taxi data. A workflow is created with two jobs (in one dag) to perform write operation for different vendors (data was filtered on vendor level). The workflow is to be submitted for both the modes: - append save mode For append mode, the data should be written from both jobs. overwrite save mode When Flare's overwrite mode is dynamic, partitions that have rows produced by the jobs will be replaced on every new write operation. Only the last finished job's data should be seen in this case. Outcomes \u00b6 Queries were run to fetch data to validate the expected behavior. Also, it was possible to query data while data was being written. Code files \u00b6 Job to test concurrent writes \u00b6 # This contains two jobs and save mode as append --- version : v1beta1 name : workflow-ny-taxi-parallel-write type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and conbined them to one file workflow : title : Connect NY Taxi dag : - name : nytaxi-vendor-one title : NY-taxi data ingester-parallel description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data?acl=r format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_04 outputName : output01 outputType : Iceberg outputOptions : saveMode : append iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip #overwrite-mode: dynamic # this was used only when one partition data is need to be replaced with saveMode as Overwrite that job was seperate if need will send that as well partitionSpec : - type : month # identity partitioning was used at vendor_id level column : date_col # col name = vendor_id name : month tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi where vendor_id = 1 - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat - name : nytaxi-vendor-two title : NY-taxi data ingester-parallel description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data?acl=r format : json isStream : false logLevel : INFO outputs : - name : output02 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_04 outputName : output02 outputType : Iceberg outputOptions : saveMode : append iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : month column : date_col name : month tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi where vendor_id = 2 - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat DataOS Tool Yaml \u00b6 version : v1beta1 name : dataos-tool-ny-taxi-parallel-write type : workflow workflow : dag : - name : dataos-tool spec : stack : alpha envs : LOG_LEVEL : debug alpha : image : rubiklabs/dataos-tool:0.0.26 arguments : - dataset - set-metadata - --address - dataos://icebase:raw01/ny_taxi_04 - --version - latest","title":"Concurrent Writes"},{"location":"flare/concurrent/#concurrent-writes","text":"","title":"Concurrent writes"},{"location":"flare/concurrent/#overview","text":"This use case tests the scenario where multiple jobs are concurrently writing at the same location.","title":"Overview"},{"location":"flare/concurrent/#solution-approach","text":"This scenario works only for Iceberg as it supports multiple concurrent writes. A workflow with two jobs is defined to write at the same data location.","title":"Solution approach"},{"location":"flare/concurrent/#implementation-details","text":"This use case is tested on NY-Taxi data. A workflow is created with two jobs (in one dag) to perform write operation for different vendors (data was filtered on vendor level). The workflow is to be submitted for both the modes: - append save mode For append mode, the data should be written from both jobs. overwrite save mode When Flare's overwrite mode is dynamic, partitions that have rows produced by the jobs will be replaced on every new write operation. Only the last finished job's data should be seen in this case.","title":"Implementation Details"},{"location":"flare/concurrent/#outcomes","text":"Queries were run to fetch data to validate the expected behavior. Also, it was possible to query data while data was being written.","title":"Outcomes"},{"location":"flare/concurrent/#code-files","text":"","title":"Code files"},{"location":"flare/concurrent/#job-to-test-concurrent-writes","text":"# This contains two jobs and save mode as append --- version : v1beta1 name : workflow-ny-taxi-parallel-write type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and conbined them to one file workflow : title : Connect NY Taxi dag : - name : nytaxi-vendor-one title : NY-taxi data ingester-parallel description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data?acl=r format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_04 outputName : output01 outputType : Iceberg outputOptions : saveMode : append iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip #overwrite-mode: dynamic # this was used only when one partition data is need to be replaced with saveMode as Overwrite that job was seperate if need will send that as well partitionSpec : - type : month # identity partitioning was used at vendor_id level column : date_col # col name = vendor_id name : month tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi where vendor_id = 1 - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat - name : nytaxi-vendor-two title : NY-taxi data ingester-parallel description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data?acl=r format : json isStream : false logLevel : INFO outputs : - name : output02 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_04 outputName : output02 outputType : Iceberg outputOptions : saveMode : append iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : month column : date_col name : month tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi where vendor_id = 2 - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat","title":"Job to test concurrent writes"},{"location":"flare/concurrent/#dataos-tool-yaml","text":"version : v1beta1 name : dataos-tool-ny-taxi-parallel-write type : workflow workflow : dag : - name : dataos-tool spec : stack : alpha envs : LOG_LEVEL : debug alpha : image : rubiklabs/dataos-tool:0.0.26 arguments : - dataset - set-metadata - --address - dataos://icebase:raw01/ny_taxi_04 - --version - latest","title":"DataOS Tool Yaml"},{"location":"flare/datareplay/","text":"Data Replay \u00b6 Overview \u00b6 This use case describes the data replay scenario where you need to re-write a data segment for various reasons, such as incomplete or corrupted data. You can configure your data ingestion jobs to fix corrupted records without rewriting all the partitions. This hugely helps to avoid expensive complete data re-write. Solution approach \u00b6 This use case configures jobs for replacing one partition of data. The behavior was to overwrite partitions dynamically. Implementation details \u00b6 Data replay scenario is tested by first ingesting the NY-taxi data at vendor level partitioning and then one vendor data was replaced using the following properties: saveMode: overwrite overwrite-mode: dynamic The test validation is done with timestamp column by comparing the values written at the time of first write and the second time when the data is written only for the one vendor data. Data replay is tested by writing data with partitioning and one partition of data was replaced by another job. Outcomes \u00b6 The files are stored in the folders based on the partition criterion defined and can be viewed in workbench or storage locaions. The accuracy of the output was tested by running queries accessing data from the modified partition and confirmed with the timestamp values. Code files \u00b6 ### this job is for only changing one partition of dataset --- version : v1beta1 name : workflow-ny-taxi-partitioned-vendor type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and write with partitioning on vendor_id workflow : title : Connect NY Taxi dag : - name : nytaxi title : NY-taxi data ingester description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data?acl=r format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_05 outputName : output01 outputType : Iceberg outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip overwrite-mode : dynamic # this was used only when one partition data is need to replace with saveMode as Overwrite partitionSpec : - type : identity column : vendor_id tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned Vendor sequence : - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi where vendor_id = 1 ## data written for only one vendor","title":"Data Replay"},{"location":"flare/datareplay/#data-replay","text":"","title":"Data Replay"},{"location":"flare/datareplay/#overview","text":"This use case describes the data replay scenario where you need to re-write a data segment for various reasons, such as incomplete or corrupted data. You can configure your data ingestion jobs to fix corrupted records without rewriting all the partitions. This hugely helps to avoid expensive complete data re-write.","title":"Overview"},{"location":"flare/datareplay/#solution-approach","text":"This use case configures jobs for replacing one partition of data. The behavior was to overwrite partitions dynamically.","title":"Solution approach"},{"location":"flare/datareplay/#implementation-details","text":"Data replay scenario is tested by first ingesting the NY-taxi data at vendor level partitioning and then one vendor data was replaced using the following properties: saveMode: overwrite overwrite-mode: dynamic The test validation is done with timestamp column by comparing the values written at the time of first write and the second time when the data is written only for the one vendor data. Data replay is tested by writing data with partitioning and one partition of data was replaced by another job.","title":"Implementation details"},{"location":"flare/datareplay/#outcomes","text":"The files are stored in the folders based on the partition criterion defined and can be viewed in workbench or storage locaions. The accuracy of the output was tested by running queries accessing data from the modified partition and confirmed with the timestamp values.","title":"Outcomes"},{"location":"flare/datareplay/#code-files","text":"### this job is for only changing one partition of dataset --- version : v1beta1 name : workflow-ny-taxi-partitioned-vendor type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and write with partitioning on vendor_id workflow : title : Connect NY Taxi dag : - name : nytaxi title : NY-taxi data ingester description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data?acl=r format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_05 outputName : output01 outputType : Iceberg outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip overwrite-mode : dynamic # this was used only when one partition data is need to replace with saveMode as Overwrite partitionSpec : - type : identity column : vendor_id tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned Vendor sequence : - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi where vendor_id = 1 ## data written for only one vendor","title":"Code files"},{"location":"flare/datasources/","text":"Reading and Writing Data \u00b6 Flare can process data from various data management/storage systems such as file systems, relational databases, Icebase, stream data. It supports reading and writing from different data sources (structured, semi-structured and unstructured) available in Apache Spark. Unstructured: TXT, CSV Semi structured: JSON, XML Structured: AVRO, PARQUET, ORC, XLSX This article will give you a better understanding of how you perform read and write operations in your Flare workflows. You can easily specify the configuration settings in YAML for reading and writing data. Define them under inputs and outputs section in the YAML file of your Flare job. You can also use specific options available for the various data sources based on their capabilities. Inputs section \u00b6 While reading data from any data source, you need to define the following under inputs section. name: Name to refer the dataset dataset: Address of the dataset to be read format: [optional] Format of data to be read. If a dataset is registered and indexed with Metis then format will be automatically identified. schema/schema path: [optional] If a data source accepts a user-specific schema, apply them. options: [optional] Key-value configuration for a specific data source to parameterize how data has to be read. If no option is supplied, default values for the options will be used. Note : Your Flare workflow can read from multiple data sources. You have to provide an array of data source definitions in such a scenario, as shown in the following example. Inputs section example \u00b6 inputs : - name : sample_csv dataset : dataos://thirdparty01:none/sample_city.csv format : csv schemaPath : dataos://thirdparty01:default/schemas/avsc/city.avsc schemaType : options : key1:value1 # Data source specific options key2:value2 - name : sample_states dataset : dataos://thirdparty01:none/states format : csv schema : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"country_code\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"country_id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"latitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"longitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\" - name : my_input_view dataset : dataos://kafka:default/cloudevents01 options : key1:value1 # Data source specific options key2:value2 - name : transaction_abfss dataset : dataos://abfss01:default/transactions_abfss_01 format : avro options : key1:value1 # Data source specific options key2:value2 - name : input_customers format : iceberg dataset : dataos://icebase:retail/customer Outputs section \u00b6 Currently, Flare supports two output formats- parquet and iceberg. For writing data, define the details where the dataset is to be stored, what is going to be its name etc. Note : It is possible to write the output of a Flare job to multiple locations. You can simply write the output and dataset information multiple times. In the outputs section, provide the reference and location information for the output. name: Reference for the output depot: to the dataset and destination path of the data.Specify Depot you want to write data to. In the steps section, describe the steps to generate the output. you also need to provide the details of the ouput dataset and the order in which multiple operations will be executed to create the desired dataset. sink : Provide the details of the output dataset. sequenceName: The name of the sequence relevant for the sink datasetName: Name assigned to the new dataset title: Title for the output dataset description: Description for the output dataset tags: Labels attached to the output dataset outputName: Name defined in the outputs section above outputType: New dataset's file format outputOptions: Different options depending on different outputTypes supported in Spark partitionBy: Column used for partitioning the data. sort: Scope and columns for the sorting, sort order. sequence : Order of operations, further has two key-value pairs: name: Sequence name to be used as a reference for the output of the SQL query it has, in the form of a table or a view. sql: SQL query of the operation in Spark SQL format SQL query for the execution of the operation in Spark. Note : The steps section can also be referenced via a separate YAML. YAML for file-based output \u00b6 The following YAML shows the 'outputs' section for the file-based output type. In outputOptions , you can provide additional data source-specific options to customize the behavior of writing. outputs : - name : output01 depot : dataos://filebase:raw01 steps : - sink : - sequenceName : seq_01 datasetName : retail_dataset title : Retail Data description : ingested retail data tags : - Connect - retail outputName : output01 outputType : Parquet outputOptions : saveMode : overwrite partitionBy : - cust_id sequence : - name : seq_00 sql : select *, to_timestamp(pickup_datetime/1000) as date_col from retail_data - name : seq_01 sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_retail_data FROM seq_00 YAML for Iceberg output type \u00b6 In case of the Iceberg output type, provide some of the iceberg specific properties such as write format, compression method, partition specs under iceberg section. outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - sink : - sequenceName : cities datasetName : city title : City Source Data description : City data ingested from external csv tags : - Connect - City outputName : output01 outputType : Iceberg outputOptions : saveMode : append sort : mode : partition columns : - name : cityId order : desc iceberg : # Iceberg specific properties properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : cityId sequence : - name : cities doc : Pick all columns from cities and add version as yyyyMMddHHmm formatted timestamp. sqlFile : flare/connect-city/city.sql Note : It is important to define partitionSpec under iceberg section to apply the partition strategy effectively. Supported data sources \u00b6 This section describes the Apache Spark data sources you can use in Flare for reading and writing data. Flare supports reading/writing from the following data sources. Data Source Scope Properties AVRO Read Avro options CSV Read CSV options ICEBERG Read/Write Iceberg Spark Configurations JSON Read JSON options ORC Read ORC options PARQUET Read/Write Parquet options TEXT Read Text options XLSX Read Spark Excel options XML Read Spark XML options Recipes \u00b6 This section describes the data source-specific options you can use in Flare to read and write data. Reading data \u00b6 The following examples show how to read data from supported data sources. AVRO \u00b6 Avro format is a row-based storage format for Hadoop, which is widely used as a serialization platform. Avro facilitates the exchange of big data between programs written in any language. Avro supports evolutionary schemas, which supports compatibility checks and allows evolving your data over time. Sample YAML file inputs : - name : sample_avro dataset : dataos://thirdparty01:sampledata/avro format : avro schemaPath : /datadir/data/sample_avro.avsc options : avroSchema : none datetimeRebaseMode : EXCEPTION/CORRECTED/LEGACY positionalFieldMatching : true/false Note : To understand more about each of the options for AVRO file, see the Avro options . CSV \u00b6 Flare allows you to read a file or directory of files in CSV format. You can provide various options to customize the behavior of reading, such as controlling the behavior of the header, delimiter character, character set, and so on. The following YAML file describes how you define various options when you want to read from a CSV file. inputs : - name : sample_csv dataset : /datadir/data/sample_csv.csv # complete file path format : csv options : header : false/true inferSchema : true delimiter : \" \" nullValue : \"GREAL\" enforceSchema : false timestampFormat : yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX] mode : PERMISSIVE/DROPMALFORMED/FAILFAST - name : multiple_csvdata dataset : /datadir/data # complete folder path format : csv options : header : false/true inferSchema : true Note : To understand more about each of the options for CSV file, see the CSV options . JSON \u00b6 Flare can automatically infer the schema of a JSON file and load it as a DataOS dataset. Each line in JSON file must contain a separate, self-contained valid JSON object. For a regular multi-line JSON file, set the multiLine option to true. The following YAML file describes how you define various options for reading from a JSON file. Sample YAML file inputs : - name : sample_json dataset : dataos://thirdparty01:sampledata/json format : json options : primitivesAsString : true/false prefersDecimal : true/false allowComments : true/false mode : PERMISSIVE/DROPMALFORMED/FAILFAST timestampFormat : yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]. dateFormat : yyyy-MM-dd. multiLine : true/false Note : To understand more, see the JSON options . PARQUET \u00b6 Parquet is a columnar format supported by many other data processing systems. Flare provides support for both reading and writing Parquet files that automatically preserve the original data schema. Parquet also supports schema evolution. Users can start with a simple schema and gradually add more columns to the schema as needed. In this way, users may have multiple Parquet files with different but mutually compatible schemas. The Parquet data source can now automatically detect this case and merge schemas of all these files. While writing to the parquet file, you need to provide configuration settings for the 'Save' mode and 'Compression' method. This example YAML file shows how you define various options for reading from a Parquet file. Sample YAML inputs : - name : sample_parquet dataset : dataos://thirdparty01:sampledata/parquet format : parquet schemaPath : /datadir/data/sample_avro.avsc options : mergeSchema : false/true datetimeRebaseMode : EXCEPTION/CORRECTED/LEGACY int96RebaseMode : EXCEPTION/CORRECTED/LEGACY Note : To understand more, see the Parquet options . ORC \u00b6 Apache ORC (Optimized Row Columnar) is a free and open-source column-oriented data storage format of the Apache Hadoop ecosystem. Flare allows you to read a file in orc format. This example file shows how you define various options for reading from an ORC file. inputs : - name : sample_orc dataset : /datadir/data/sample_orc.orc format : orc options : mergeSchema : false/true # Sets whether we should merge schemas collected from all ORC part-files. Note : To understand more, see the ORC options . TEXT \u00b6 Flare allows you to read a text file or directory of text files into a DataOS dataset. When reading a text file, each line becomes each row with a string \"value\" column by default. In YAML, you can specify various options to customize reading behavior, such as controlling the behavior of the line separator. inputs : - name : sample_txt dataset : dataos://thirdparty01:sampledata/sample.txt format : text options : wholetext : true lineSep : \\r, \\r\\n, \\n - name : sample_txt_files dataset : dataos://thirdparty01:sampledata/txt format : text options : wholetext : true lineSep : \\r, \\r\\n, \\n Note : To understand more, see the Text options . XLSX \u00b6 Flare allows you to construct a dataset by reading a data file in XLSX format. You can configure several options for XLSX file and customize the behavior of reading, such as location, sheet names, cell range, workbook password, etc. Flare supports reading multiple XLSX files kept in a folder. This example YAML file shows how you define various options for reading from an XLSX file. inputs : - name : sample_xlsx dataset : dataos://thirdparty01:sampledata/xlsx/returns.xlsx # pass complete file path format : xlsx sheetName : aa header : false/true workbookPassword : password inferSchema : true # If column types to be inferred when reading the file. - name : sample_xlsx_files dataset : dataos://thirdparty01:sampledata/xlsx # pass complete path for the folder where files are kept format : xlsx Note : To understand more about the XLSX options, see the Spark Excel options . XML \u00b6 Flare allows you to read a file in XML format. You can provide various options to customize the behavior of reading, such as how you want to deal with corrupt records, specify the path to an XSD file that is used to validate the XML for each row individually, whether to exclude attributes, etc. This example YAML file shows how you define various options for reading from a XML file. inputs : - name : sample_xml dataset : dataos://thirdparty01:sampledata/xml format : xml schemaPath : dataos://thirdparty01:none/schemas/avsc/csv.avsc options : path : Location of files excludeAttribute : false inferSchema : true columnNameOfCorruptRecord : _corrupt_record attributePrefix : _ valueTag : _VALUE charset : 'UTF-8' ignoreNamespace : false timestampFormat : UTC dateFormat : ISO_DATE Note : To understand more about the options for XML file, see the Spark XML options . Writing data \u00b6 The following examples show how you can specify various options while writing data to the supported data sources in Flare. PARQUET \u00b6 outputs : - name : output01 depot : dataos://filebase:raw01 steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_parquet_06 outputName : output01 outputType : Parquet outputOptions : saveMode : overwrite partitionBy : - store_and_fwd_flag tags : - Connect - NY-Taxi title : NY-Taxi Data sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat ICEBERG \u00b6 outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - sink : - sequenceName : cities datasetName : city outputName : output01 outputType : Iceberg description : City data ingested from external csv tags : - Connect - City title : City Source Data outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : version sequence : - name : cities doc : Pick all columns from cities and add version as yyyyMMddHHmm formatted timestamp. sqlFile : flare/connect-city/city.sql Note : To understand more, see the Iceberg Spark Configurations .","title":"Reading and Writing Data"},{"location":"flare/datasources/#reading-and-writing-data","text":"Flare can process data from various data management/storage systems such as file systems, relational databases, Icebase, stream data. It supports reading and writing from different data sources (structured, semi-structured and unstructured) available in Apache Spark. Unstructured: TXT, CSV Semi structured: JSON, XML Structured: AVRO, PARQUET, ORC, XLSX This article will give you a better understanding of how you perform read and write operations in your Flare workflows. You can easily specify the configuration settings in YAML for reading and writing data. Define them under inputs and outputs section in the YAML file of your Flare job. You can also use specific options available for the various data sources based on their capabilities.","title":"Reading and Writing Data"},{"location":"flare/datasources/#inputs-section","text":"While reading data from any data source, you need to define the following under inputs section. name: Name to refer the dataset dataset: Address of the dataset to be read format: [optional] Format of data to be read. If a dataset is registered and indexed with Metis then format will be automatically identified. schema/schema path: [optional] If a data source accepts a user-specific schema, apply them. options: [optional] Key-value configuration for a specific data source to parameterize how data has to be read. If no option is supplied, default values for the options will be used. Note : Your Flare workflow can read from multiple data sources. You have to provide an array of data source definitions in such a scenario, as shown in the following example.","title":"Inputs section"},{"location":"flare/datasources/#inputs-section-example","text":"inputs : - name : sample_csv dataset : dataos://thirdparty01:none/sample_city.csv format : csv schemaPath : dataos://thirdparty01:default/schemas/avsc/city.avsc schemaType : options : key1:value1 # Data source specific options key2:value2 - name : sample_states dataset : dataos://thirdparty01:none/states format : csv schema : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"country_code\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"country_id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"latitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"longitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\" - name : my_input_view dataset : dataos://kafka:default/cloudevents01 options : key1:value1 # Data source specific options key2:value2 - name : transaction_abfss dataset : dataos://abfss01:default/transactions_abfss_01 format : avro options : key1:value1 # Data source specific options key2:value2 - name : input_customers format : iceberg dataset : dataos://icebase:retail/customer","title":"Inputs section example"},{"location":"flare/datasources/#outputs-section","text":"Currently, Flare supports two output formats- parquet and iceberg. For writing data, define the details where the dataset is to be stored, what is going to be its name etc. Note : It is possible to write the output of a Flare job to multiple locations. You can simply write the output and dataset information multiple times. In the outputs section, provide the reference and location information for the output. name: Reference for the output depot: to the dataset and destination path of the data.Specify Depot you want to write data to. In the steps section, describe the steps to generate the output. you also need to provide the details of the ouput dataset and the order in which multiple operations will be executed to create the desired dataset. sink : Provide the details of the output dataset. sequenceName: The name of the sequence relevant for the sink datasetName: Name assigned to the new dataset title: Title for the output dataset description: Description for the output dataset tags: Labels attached to the output dataset outputName: Name defined in the outputs section above outputType: New dataset's file format outputOptions: Different options depending on different outputTypes supported in Spark partitionBy: Column used for partitioning the data. sort: Scope and columns for the sorting, sort order. sequence : Order of operations, further has two key-value pairs: name: Sequence name to be used as a reference for the output of the SQL query it has, in the form of a table or a view. sql: SQL query of the operation in Spark SQL format SQL query for the execution of the operation in Spark. Note : The steps section can also be referenced via a separate YAML.","title":"Outputs section"},{"location":"flare/datasources/#yaml-for-file-based-output","text":"The following YAML shows the 'outputs' section for the file-based output type. In outputOptions , you can provide additional data source-specific options to customize the behavior of writing. outputs : - name : output01 depot : dataos://filebase:raw01 steps : - sink : - sequenceName : seq_01 datasetName : retail_dataset title : Retail Data description : ingested retail data tags : - Connect - retail outputName : output01 outputType : Parquet outputOptions : saveMode : overwrite partitionBy : - cust_id sequence : - name : seq_00 sql : select *, to_timestamp(pickup_datetime/1000) as date_col from retail_data - name : seq_01 sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_retail_data FROM seq_00","title":"YAML for file-based output"},{"location":"flare/datasources/#yaml-for-iceberg-output-type","text":"In case of the Iceberg output type, provide some of the iceberg specific properties such as write format, compression method, partition specs under iceberg section. outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - sink : - sequenceName : cities datasetName : city title : City Source Data description : City data ingested from external csv tags : - Connect - City outputName : output01 outputType : Iceberg outputOptions : saveMode : append sort : mode : partition columns : - name : cityId order : desc iceberg : # Iceberg specific properties properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : cityId sequence : - name : cities doc : Pick all columns from cities and add version as yyyyMMddHHmm formatted timestamp. sqlFile : flare/connect-city/city.sql Note : It is important to define partitionSpec under iceberg section to apply the partition strategy effectively.","title":"YAML for Iceberg output type"},{"location":"flare/datasources/#supported-data-sources","text":"This section describes the Apache Spark data sources you can use in Flare for reading and writing data. Flare supports reading/writing from the following data sources. Data Source Scope Properties AVRO Read Avro options CSV Read CSV options ICEBERG Read/Write Iceberg Spark Configurations JSON Read JSON options ORC Read ORC options PARQUET Read/Write Parquet options TEXT Read Text options XLSX Read Spark Excel options XML Read Spark XML options","title":"Supported data sources"},{"location":"flare/datasources/#recipes","text":"This section describes the data source-specific options you can use in Flare to read and write data.","title":"Recipes"},{"location":"flare/datasources/#reading-data","text":"The following examples show how to read data from supported data sources.","title":"Reading data"},{"location":"flare/datasources/#avro","text":"Avro format is a row-based storage format for Hadoop, which is widely used as a serialization platform. Avro facilitates the exchange of big data between programs written in any language. Avro supports evolutionary schemas, which supports compatibility checks and allows evolving your data over time. Sample YAML file inputs : - name : sample_avro dataset : dataos://thirdparty01:sampledata/avro format : avro schemaPath : /datadir/data/sample_avro.avsc options : avroSchema : none datetimeRebaseMode : EXCEPTION/CORRECTED/LEGACY positionalFieldMatching : true/false Note : To understand more about each of the options for AVRO file, see the Avro options .","title":"AVRO"},{"location":"flare/datasources/#csv","text":"Flare allows you to read a file or directory of files in CSV format. You can provide various options to customize the behavior of reading, such as controlling the behavior of the header, delimiter character, character set, and so on. The following YAML file describes how you define various options when you want to read from a CSV file. inputs : - name : sample_csv dataset : /datadir/data/sample_csv.csv # complete file path format : csv options : header : false/true inferSchema : true delimiter : \" \" nullValue : \"GREAL\" enforceSchema : false timestampFormat : yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX] mode : PERMISSIVE/DROPMALFORMED/FAILFAST - name : multiple_csvdata dataset : /datadir/data # complete folder path format : csv options : header : false/true inferSchema : true Note : To understand more about each of the options for CSV file, see the CSV options .","title":"CSV"},{"location":"flare/datasources/#json","text":"Flare can automatically infer the schema of a JSON file and load it as a DataOS dataset. Each line in JSON file must contain a separate, self-contained valid JSON object. For a regular multi-line JSON file, set the multiLine option to true. The following YAML file describes how you define various options for reading from a JSON file. Sample YAML file inputs : - name : sample_json dataset : dataos://thirdparty01:sampledata/json format : json options : primitivesAsString : true/false prefersDecimal : true/false allowComments : true/false mode : PERMISSIVE/DROPMALFORMED/FAILFAST timestampFormat : yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]. dateFormat : yyyy-MM-dd. multiLine : true/false Note : To understand more, see the JSON options .","title":"JSON"},{"location":"flare/datasources/#parquet","text":"Parquet is a columnar format supported by many other data processing systems. Flare provides support for both reading and writing Parquet files that automatically preserve the original data schema. Parquet also supports schema evolution. Users can start with a simple schema and gradually add more columns to the schema as needed. In this way, users may have multiple Parquet files with different but mutually compatible schemas. The Parquet data source can now automatically detect this case and merge schemas of all these files. While writing to the parquet file, you need to provide configuration settings for the 'Save' mode and 'Compression' method. This example YAML file shows how you define various options for reading from a Parquet file. Sample YAML inputs : - name : sample_parquet dataset : dataos://thirdparty01:sampledata/parquet format : parquet schemaPath : /datadir/data/sample_avro.avsc options : mergeSchema : false/true datetimeRebaseMode : EXCEPTION/CORRECTED/LEGACY int96RebaseMode : EXCEPTION/CORRECTED/LEGACY Note : To understand more, see the Parquet options .","title":"PARQUET"},{"location":"flare/datasources/#orc","text":"Apache ORC (Optimized Row Columnar) is a free and open-source column-oriented data storage format of the Apache Hadoop ecosystem. Flare allows you to read a file in orc format. This example file shows how you define various options for reading from an ORC file. inputs : - name : sample_orc dataset : /datadir/data/sample_orc.orc format : orc options : mergeSchema : false/true # Sets whether we should merge schemas collected from all ORC part-files. Note : To understand more, see the ORC options .","title":"ORC"},{"location":"flare/datasources/#text","text":"Flare allows you to read a text file or directory of text files into a DataOS dataset. When reading a text file, each line becomes each row with a string \"value\" column by default. In YAML, you can specify various options to customize reading behavior, such as controlling the behavior of the line separator. inputs : - name : sample_txt dataset : dataos://thirdparty01:sampledata/sample.txt format : text options : wholetext : true lineSep : \\r, \\r\\n, \\n - name : sample_txt_files dataset : dataos://thirdparty01:sampledata/txt format : text options : wholetext : true lineSep : \\r, \\r\\n, \\n Note : To understand more, see the Text options .","title":"TEXT"},{"location":"flare/datasources/#xlsx","text":"Flare allows you to construct a dataset by reading a data file in XLSX format. You can configure several options for XLSX file and customize the behavior of reading, such as location, sheet names, cell range, workbook password, etc. Flare supports reading multiple XLSX files kept in a folder. This example YAML file shows how you define various options for reading from an XLSX file. inputs : - name : sample_xlsx dataset : dataos://thirdparty01:sampledata/xlsx/returns.xlsx # pass complete file path format : xlsx sheetName : aa header : false/true workbookPassword : password inferSchema : true # If column types to be inferred when reading the file. - name : sample_xlsx_files dataset : dataos://thirdparty01:sampledata/xlsx # pass complete path for the folder where files are kept format : xlsx Note : To understand more about the XLSX options, see the Spark Excel options .","title":"XLSX"},{"location":"flare/datasources/#xml","text":"Flare allows you to read a file in XML format. You can provide various options to customize the behavior of reading, such as how you want to deal with corrupt records, specify the path to an XSD file that is used to validate the XML for each row individually, whether to exclude attributes, etc. This example YAML file shows how you define various options for reading from a XML file. inputs : - name : sample_xml dataset : dataos://thirdparty01:sampledata/xml format : xml schemaPath : dataos://thirdparty01:none/schemas/avsc/csv.avsc options : path : Location of files excludeAttribute : false inferSchema : true columnNameOfCorruptRecord : _corrupt_record attributePrefix : _ valueTag : _VALUE charset : 'UTF-8' ignoreNamespace : false timestampFormat : UTC dateFormat : ISO_DATE Note : To understand more about the options for XML file, see the Spark XML options .","title":"XML"},{"location":"flare/datasources/#writing-data","text":"The following examples show how you can specify various options while writing data to the supported data sources in Flare.","title":"Writing data"},{"location":"flare/datasources/#parquet_1","text":"outputs : - name : output01 depot : dataos://filebase:raw01 steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_parquet_06 outputName : output01 outputType : Parquet outputOptions : saveMode : overwrite partitionBy : - store_and_fwd_flag tags : - Connect - NY-Taxi title : NY-Taxi Data sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat","title":"PARQUET"},{"location":"flare/datasources/#iceberg","text":"outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - sink : - sequenceName : cities datasetName : city outputName : output01 outputType : Iceberg description : City data ingested from external csv tags : - Connect - City title : City Source Data outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : version sequence : - name : cities doc : Pick all columns from cities and add version as yyyyMMddHHmm formatted timestamp. sqlFile : flare/connect-city/city.sql Note : To understand more, see the Iceberg Spark Configurations .","title":"ICEBERG"},{"location":"flare/flarejoballproperties/","text":"Flare Job Properties \u00b6 Here is the skeleton of the Flare job. YAML is a simple but robust data serialization language. It has just two data structures: sequences (a list) mappings (key and value pairs). Sequence (lists) \u00b6 Linux BSD Illumos Mapping (key value pairs) \u00b6 version: v1beta1 name: product-demo-01 type: workflow These structures can be combined and embedded. Sequence of mappings (list of pairs) - CPU: AMD RAM: \u201816 GB\u2019 - CPU: Intel RAM: \u201816 GB\u2019 Mapping sequences (key with many values) \u00b6 tags: - Connect - City functions: - name: rename column: id asColumn: country_id - name: rename column: name asColumn: country_name Sequence of sequences (a list of lists) \u00b6 pineapple coconut \u00b6 umbrella raincoat Mapping of mappings Joey: age: 22 sex: M Laura: age: 24 sex: F job: explain: true logLevel: INFO --- version : v1beta1 name : cnt-city-demo-01 type : workflow tags : - Connect - City description : The job ingests city data from dropzone into raw zone #owner: itspiyush workflow : title : Connect City dag : - name : city-01 title : City Dimension Ingester description : The job ingests city data from dropzone into raw zone spec : tags : - Connect - City stack : flare:1.0 # persistentVolume: # name: persistent-v # directory: connectCity flare : job : explain : true inputs : - name : city_connect dataset : dataos://thirdparty01:none/city format : csv schemaPath : dataos://thirdparty01:none/schemas/avsc/city.avsc logLevel : INFO outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - sink : - sequenceName : cities datasetName : city outputName : output01 outputType : Iceberg description : City data ingested from external csv outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : version tags : - Connect - City title : City Source Data sequence : - name : cities doc : Pick all columns from cities and add version as yyyyMMddHHmm formatted timestamp. sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_city FROM city_connect version : v1beta1 name : wf-ingest-city-state-countries type : workflow tags : - ingest-data - cities - states - countries - enriched description : This ingest city state and countries data and enriched data owner : rakeshvishvakarma21 workflow : dag : - name : ingest-and-enrich-city-state-country title : city state and countries workflow description : This ingest city state and countries data and data enrichment spec : tags : - cities - states - countries - enriched stack : flare:1.0 flare : job : explain : true inputs : - name : cities dataset : dataos://thirdparty01:none/cities format : csv schema : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"country_code\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"country_id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"latitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"longitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"state_code\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"state_id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\" - name : countries dataset : dataos://thirdparty01:none/countries format : csv schema : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"capital\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"currency\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"currency_symbol\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"emoji\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"emojiU\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"iso2\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"iso3\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"latitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"longitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"native\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"numeric_code\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"phone_code\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"region\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"subregion\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"timezones\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"tld\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\" - name : states dataset : dataos://thirdparty01:none/states format : csv schema : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"country_code\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"country_id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"latitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"longitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"state_code\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\" logLevel : ERROR outputs : - name : output01 # cities depot : dataos://icebase:countries_states_cities?acl=rw - name : output02 # states depot : dataos://icebase:countries_states_cities?acl=rw - name : output03 # countries depot : dataos://icebase:countries_states_cities?acl=rw - name : output04 # joined-data depot : dataos://icebase:countries_states_cities?acl=rw steps : - sequence : - name : countries_uppdated sql : SELECT * FROM countries functions : - name : rename column : id asColumn : country_id - name : rename column : name asColumn : country_name - name : rename column : latitude asColumn : country_latitude - name : rename column : longitude asColumn : country_longitude - name : states_uppdated sql : SELECT * FROM states functions : - name : rename column : id asColumn : state_id - name : rename column : name asColumn : state_name - name : rename column : latitude asColumn : state_latitude - name : rename column : longitude asColumn : state_longitude - name : rename column : country_id asColumn : country_id_in_states - name : cities_uppdated sql : SELECT * FROM cities functions : - name : rename column : id asColumn : city_id - name : rename column : name asColumn : city_name - name : rename column : latitude asColumn : city_latitude - name : rename column : longitude asColumn : city_longitude - name : rename column : state_id asColumn : state_id_in_cities - name : drop columns : - state_code - country_id - country_code - name : joined_states_cities sql : SELECT * FROM cities_uppdated uc left join states_uppdated us on uc.state_id_in_cities = us.state_id functions : - name : drop columns : - state_id_in_cities - name : joined_states_cities_countries sql : SELECT * FROM joined_states_cities jsc left join countries_uppdated ucon on jsc.country_id_in_states = ucon.country_id functions : - name : drop columns : - country_id_in_states sink : - sequenceName : cities datasetName : cities outputName : output01 outputType : Iceberg description : Cities Information With State and Country Code outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip tags : - city-info - database title : Cities Information - sequenceName : states datasetName : states outputName : output02 outputType : Iceberg description : States Information With Country Code outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip tags : - state-info - database title : States Information - sequenceName : countries datasetName : countries outputName : output03 outputType : Iceberg description : Countries Details outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip tags : - country-info - database title : Countries Information - sequenceName : joined_states_cities_countries datasetName : enriched_cities_states_countries outputName : output04 outputType : Iceberg description : Enriched data of cities, states and countries details outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : country_name name : country_name tags : - cities - states - countries - enriched title : Enriched Countries With States and Cities Information --- version : v1beta1 name : cnt-product-demo-01 type : workflow tags : - Connect - Product description : The job ingests product data from dropzone into raw zone # owner: deenkar_rubik workflow : title : Connect Product dag : - name : product-01 title : Product Dimension Ingester description : The job ingests product data from dropzone into raw zone spec : tags : - Connect - Product stack : flare:1.0 flare : job : explain : true inputs : - name : product_connect dataset : dataos://thirdparty01:none/product format : csv schemaPath : dataos://thirdparty01:none/schemas/avsc/product.avsc logLevel : WARN outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - sink : - sequenceName : products datasetName : product outputName : output01 outputType : Iceberg description : Customer data ingested from external csv outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : version tags : - Connect - Product title : Product Source Data sequence : - name : products doc : Pick all columns from products and add version as yyyyMMddHHmm formatted timestamp. sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_product FROM product_connect --- version : v1beta1 name : cust-bq-demo-01 type : workflow tags : - Connect - Customer description : The job ingests customer data from bigquery into raw zone #owner: itspiyush workflow : title : Connect Customer dag : - name : customer title : Customer Dimension Ingester description : The job ingests customer data from bigquery into raw zone spec : tags : - Connect - Customer stack : flare:1.0 flare : job : explain : true inputs : - name : customer_connect dataset : dataos://crmbq:demo/customer_profiles logLevel : WARN outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - sink : - sequenceName : customers datasetName : customer outputName : output01 outputType : Iceberg description : Customer data ingested from bigquery outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : version tags : - Connect - Customer title : Customer Source Data sequence : - name : customers doc : Pick all columns from customers and add version as yyyyMMddHHmm formatted timestamp. sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_customer FROM customer_connect functions : - name : rename column : Id asColumn : id --- version : v1beta1 name : cust-360-01 type : workflow tags : - Customer 360 - Offline Sales description : This job is preparing customer 360 data #owner: deenkar_rubik workflow : title : Customer 360 dag : - name : cust-360-step-01 title : Customer 360 description : This job is preparing customer 360 data spec : tags : - Customer 360 - Offline Sales stack : flare:1.0 tier : rio flare : configs : {} driver : coreLimit : 2400m cores : 2 memory : 3072m executor : coreLimit : 2400m cores : 2 instances : 2 memory : 4096m job : explain : true inputs : - name : input_customers format : iceberg dataset : dataos://icebase:retail/customer - name : input_pos_transactions format : iceberg dataset : dataos://icebase:retail/pos_txn_enric logLevel : INFO outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - flare/customer-360/steps.yaml --- version : v1beta1 name : cnt-store-demo-01 type : workflow tags : - Connect - Store description : The job ingests store data from thirdparty storage into raw #owner: itspiyush workflow : title : Connect Store dag : - name : store title : Store Dimension Ingester description : The job ingests store data from thirdparty storage into raw zone spec : tags : - Connect - Store stack : flare:1.0 # persistentVolume: # name: job-persistence # directory: connectCity # tempVolume: 2Gi flare : job : explain : true inputs : - name : store_connect dataset : dataos://thirdparty01:none/store format : csv schemaPath : dataos://thirdparty01:none/schemas/avsc/store.avsc logLevel : WARN outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - sink : - sequenceName : store datasetName : store outputName : output01 outputType : Iceberg description : Store information ingested from external csv outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : version tags : - Connect - Store title : POS Store Source Data sequence : - name : store doc : Pick all columns from store and add version as yyyyMMddHHmm formatted timestamp. sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_store FROM store_connect --- version : v1beta1 name : cust-snow-demo-01 type : workflow tags : - Connect - Supplier description : The job ingests supplier data from snowflake into raw zone #owner: itspiyush workflow : title : Connect dag : - name : supplier title : Supplier Data Ingester description : The job ingests supplier data from snowflake into raw zone spec : tags : - Connect - Customer stack : flare:1.0 flare : job : explain : true inputs : - name : supplier_connect dataset : dataos://snowflake:tpch_sf10/supplier logLevel : WARN outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : suppliers datasetName : supplier outputName : output01 outputType : Iceberg description : Supplier data ingested from snowflake outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : version tags : - Connect - Supplier title : Supplier Source Data sequence : - name : suppliers doc : Pick all columns from suppliers and add version as yyyyMMddHHmm formatted timestamp. sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_customer FROM supplier_connect --- version : v1beta1 name : write-pulsar-avro type : workflow tags : - pulsar - read description : this jobs reads data from thirdparty and writes to pulsar workflow : dag : - name : pulsar-write-avro title : write avro data to pulsar description : write avro data to pulsar spec : tags : - Connect stack : flare:1.0 envs : ENABLE_TOPIC_CREATION : \"true\" # persistentVolume: # name: persistent-v # directory: connectCity flare : job : explain : true inputs : - name : connect_city dataset : dataos://thirdparty01:none/city format : csv isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://pulsar:default?acl=rw checkpointLocation : dataos://icebase:sys01/checkpoints/connect-city/cl01/city03?acl=rw schemaRegistryUrl : http://schema-registry.caretaker:8081 steps : - sink : - sequenceName : connect_city datasetName : city_avro_pulsar outputName : output01 outputType : PulsarAvro outputOptions : extraOptions : pulsar.client.useKeyStoreTls : true pulsar.client.tlsTrustStorePassword : 3e128220-2b96-4e5d-b4fa-a0ce6ce77d1c pulsar.client.tlsTrustStoreType : JKS pulsar.client.tlsTrustStorePath : /etc/dataos/certs/truststore.jks pulsar.client.allowTlsInsecureConnection : false pulsar.client.enableTlsHostnameVerification : false #pulsar.client.useTls: true #pulsar.client.tlsAllowInsecureConnection: false #pulsar.client.tlsTrustCertsFilePath: /etc/dataos/certs/ca.cert.pem #pulsar.client.tlsEnableHostnameVerification: false tags : - Connect title : City Data AVRO Pulasr --- version : v1beta1 name : syn-off-tx type : workflow tags : - Offline - Syndicate description : This job is Syndicating offline transactions data #owner: itspiyush workflow : dag : - name : syn-off-tx-01-step title : Syndicate Offline transactions description : This job is Syndicating offline transactions data out into csv. spec : tags : - Offline - Syndicate stack : flare:1.0 flare : configs : {} driver : coreLimit : 2400m cores : 2 memory : 3072m executor : coreLimit : 2400m cores : 2 instances : 2 memory : 4096m job : explain : true inputs : - name : processed_transactions format : iceberg dataset : dataos://icebase:set01/pos_txn_enric_01 logLevel : INFO outputs : - name : output01 depot : dataos://syndicationgcs:syndicate?acl=rw steps : - sink : - sequenceName : syndicatePos datasetName : offline_01_csv description : Offline transactions into csv. outputName : output01 outputOptions : file : saveMode : Overwrite outputType : CSV tags : - Offline - Syndicate title : Offline Transactions sequence : - name : transactions sql : SELECT transaction_header.store_id, explode(transaction_line_item) as line_item, store FROM processed_transactions - name : syndicatePos sql : SELECT store_id, line_item.*, store.store_name, store.store_city, store.store_state FROM transactions variables : keepSystemColumns : \"false\"","title":"Flare Job Properties"},{"location":"flare/flarejoballproperties/#flare-job-properties","text":"Here is the skeleton of the Flare job. YAML is a simple but robust data serialization language. It has just two data structures: sequences (a list) mappings (key and value pairs).","title":"Flare Job Properties"},{"location":"flare/flarejoballproperties/#sequence-lists","text":"Linux BSD Illumos","title":"Sequence (lists)"},{"location":"flare/flarejoballproperties/#mapping-key-value-pairs","text":"version: v1beta1 name: product-demo-01 type: workflow These structures can be combined and embedded. Sequence of mappings (list of pairs) - CPU: AMD RAM: \u201816 GB\u2019 - CPU: Intel RAM: \u201816 GB\u2019","title":"Mapping (key value pairs)"},{"location":"flare/flarejoballproperties/#mapping-sequences-key-with-many-values","text":"tags: - Connect - City functions: - name: rename column: id asColumn: country_id - name: rename column: name asColumn: country_name","title":"Mapping sequences (key with many values)"},{"location":"flare/flarejoballproperties/#sequence-of-sequences-a-list-of-lists","text":"pineapple","title":"Sequence of sequences (a list of lists)"},{"location":"flare/flarejoballproperties/#coconut","text":"umbrella raincoat Mapping of mappings Joey: age: 22 sex: M Laura: age: 24 sex: F job: explain: true logLevel: INFO --- version : v1beta1 name : cnt-city-demo-01 type : workflow tags : - Connect - City description : The job ingests city data from dropzone into raw zone #owner: itspiyush workflow : title : Connect City dag : - name : city-01 title : City Dimension Ingester description : The job ingests city data from dropzone into raw zone spec : tags : - Connect - City stack : flare:1.0 # persistentVolume: # name: persistent-v # directory: connectCity flare : job : explain : true inputs : - name : city_connect dataset : dataos://thirdparty01:none/city format : csv schemaPath : dataos://thirdparty01:none/schemas/avsc/city.avsc logLevel : INFO outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - sink : - sequenceName : cities datasetName : city outputName : output01 outputType : Iceberg description : City data ingested from external csv outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : version tags : - Connect - City title : City Source Data sequence : - name : cities doc : Pick all columns from cities and add version as yyyyMMddHHmm formatted timestamp. sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_city FROM city_connect version : v1beta1 name : wf-ingest-city-state-countries type : workflow tags : - ingest-data - cities - states - countries - enriched description : This ingest city state and countries data and enriched data owner : rakeshvishvakarma21 workflow : dag : - name : ingest-and-enrich-city-state-country title : city state and countries workflow description : This ingest city state and countries data and data enrichment spec : tags : - cities - states - countries - enriched stack : flare:1.0 flare : job : explain : true inputs : - name : cities dataset : dataos://thirdparty01:none/cities format : csv schema : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"country_code\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"country_id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"latitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"longitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"state_code\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"state_id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\" - name : countries dataset : dataos://thirdparty01:none/countries format : csv schema : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"capital\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"currency\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"currency_symbol\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"emoji\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"emojiU\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"iso2\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"iso3\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"latitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"longitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"native\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"numeric_code\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"phone_code\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"region\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"subregion\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"timezones\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"tld\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\" - name : states dataset : dataos://thirdparty01:none/states format : csv schema : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"country_code\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"country_id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"latitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"longitude\\\",\\\"type\\\":\\\"double\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}},{\\\"name\\\":\\\"state_code\\\",\\\"type\\\":\\\"string\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\" logLevel : ERROR outputs : - name : output01 # cities depot : dataos://icebase:countries_states_cities?acl=rw - name : output02 # states depot : dataos://icebase:countries_states_cities?acl=rw - name : output03 # countries depot : dataos://icebase:countries_states_cities?acl=rw - name : output04 # joined-data depot : dataos://icebase:countries_states_cities?acl=rw steps : - sequence : - name : countries_uppdated sql : SELECT * FROM countries functions : - name : rename column : id asColumn : country_id - name : rename column : name asColumn : country_name - name : rename column : latitude asColumn : country_latitude - name : rename column : longitude asColumn : country_longitude - name : states_uppdated sql : SELECT * FROM states functions : - name : rename column : id asColumn : state_id - name : rename column : name asColumn : state_name - name : rename column : latitude asColumn : state_latitude - name : rename column : longitude asColumn : state_longitude - name : rename column : country_id asColumn : country_id_in_states - name : cities_uppdated sql : SELECT * FROM cities functions : - name : rename column : id asColumn : city_id - name : rename column : name asColumn : city_name - name : rename column : latitude asColumn : city_latitude - name : rename column : longitude asColumn : city_longitude - name : rename column : state_id asColumn : state_id_in_cities - name : drop columns : - state_code - country_id - country_code - name : joined_states_cities sql : SELECT * FROM cities_uppdated uc left join states_uppdated us on uc.state_id_in_cities = us.state_id functions : - name : drop columns : - state_id_in_cities - name : joined_states_cities_countries sql : SELECT * FROM joined_states_cities jsc left join countries_uppdated ucon on jsc.country_id_in_states = ucon.country_id functions : - name : drop columns : - country_id_in_states sink : - sequenceName : cities datasetName : cities outputName : output01 outputType : Iceberg description : Cities Information With State and Country Code outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip tags : - city-info - database title : Cities Information - sequenceName : states datasetName : states outputName : output02 outputType : Iceberg description : States Information With Country Code outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip tags : - state-info - database title : States Information - sequenceName : countries datasetName : countries outputName : output03 outputType : Iceberg description : Countries Details outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip tags : - country-info - database title : Countries Information - sequenceName : joined_states_cities_countries datasetName : enriched_cities_states_countries outputName : output04 outputType : Iceberg description : Enriched data of cities, states and countries details outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : country_name name : country_name tags : - cities - states - countries - enriched title : Enriched Countries With States and Cities Information --- version : v1beta1 name : cnt-product-demo-01 type : workflow tags : - Connect - Product description : The job ingests product data from dropzone into raw zone # owner: deenkar_rubik workflow : title : Connect Product dag : - name : product-01 title : Product Dimension Ingester description : The job ingests product data from dropzone into raw zone spec : tags : - Connect - Product stack : flare:1.0 flare : job : explain : true inputs : - name : product_connect dataset : dataos://thirdparty01:none/product format : csv schemaPath : dataos://thirdparty01:none/schemas/avsc/product.avsc logLevel : WARN outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - sink : - sequenceName : products datasetName : product outputName : output01 outputType : Iceberg description : Customer data ingested from external csv outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : version tags : - Connect - Product title : Product Source Data sequence : - name : products doc : Pick all columns from products and add version as yyyyMMddHHmm formatted timestamp. sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_product FROM product_connect --- version : v1beta1 name : cust-bq-demo-01 type : workflow tags : - Connect - Customer description : The job ingests customer data from bigquery into raw zone #owner: itspiyush workflow : title : Connect Customer dag : - name : customer title : Customer Dimension Ingester description : The job ingests customer data from bigquery into raw zone spec : tags : - Connect - Customer stack : flare:1.0 flare : job : explain : true inputs : - name : customer_connect dataset : dataos://crmbq:demo/customer_profiles logLevel : WARN outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - sink : - sequenceName : customers datasetName : customer outputName : output01 outputType : Iceberg description : Customer data ingested from bigquery outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : version tags : - Connect - Customer title : Customer Source Data sequence : - name : customers doc : Pick all columns from customers and add version as yyyyMMddHHmm formatted timestamp. sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_customer FROM customer_connect functions : - name : rename column : Id asColumn : id --- version : v1beta1 name : cust-360-01 type : workflow tags : - Customer 360 - Offline Sales description : This job is preparing customer 360 data #owner: deenkar_rubik workflow : title : Customer 360 dag : - name : cust-360-step-01 title : Customer 360 description : This job is preparing customer 360 data spec : tags : - Customer 360 - Offline Sales stack : flare:1.0 tier : rio flare : configs : {} driver : coreLimit : 2400m cores : 2 memory : 3072m executor : coreLimit : 2400m cores : 2 instances : 2 memory : 4096m job : explain : true inputs : - name : input_customers format : iceberg dataset : dataos://icebase:retail/customer - name : input_pos_transactions format : iceberg dataset : dataos://icebase:retail/pos_txn_enric logLevel : INFO outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - flare/customer-360/steps.yaml --- version : v1beta1 name : cnt-store-demo-01 type : workflow tags : - Connect - Store description : The job ingests store data from thirdparty storage into raw #owner: itspiyush workflow : title : Connect Store dag : - name : store title : Store Dimension Ingester description : The job ingests store data from thirdparty storage into raw zone spec : tags : - Connect - Store stack : flare:1.0 # persistentVolume: # name: job-persistence # directory: connectCity # tempVolume: 2Gi flare : job : explain : true inputs : - name : store_connect dataset : dataos://thirdparty01:none/store format : csv schemaPath : dataos://thirdparty01:none/schemas/avsc/store.avsc logLevel : WARN outputs : - name : output01 depot : dataos://icebase:retail?acl=rw steps : - sink : - sequenceName : store datasetName : store outputName : output01 outputType : Iceberg description : Store information ingested from external csv outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : version tags : - Connect - Store title : POS Store Source Data sequence : - name : store doc : Pick all columns from store and add version as yyyyMMddHHmm formatted timestamp. sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_store FROM store_connect --- version : v1beta1 name : cust-snow-demo-01 type : workflow tags : - Connect - Supplier description : The job ingests supplier data from snowflake into raw zone #owner: itspiyush workflow : title : Connect dag : - name : supplier title : Supplier Data Ingester description : The job ingests supplier data from snowflake into raw zone spec : tags : - Connect - Customer stack : flare:1.0 flare : job : explain : true inputs : - name : supplier_connect dataset : dataos://snowflake:tpch_sf10/supplier logLevel : WARN outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : suppliers datasetName : supplier outputName : output01 outputType : Iceberg description : Supplier data ingested from snowflake outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity column : version tags : - Connect - Supplier title : Supplier Source Data sequence : - name : suppliers doc : Pick all columns from suppliers and add version as yyyyMMddHHmm formatted timestamp. sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_customer FROM supplier_connect --- version : v1beta1 name : write-pulsar-avro type : workflow tags : - pulsar - read description : this jobs reads data from thirdparty and writes to pulsar workflow : dag : - name : pulsar-write-avro title : write avro data to pulsar description : write avro data to pulsar spec : tags : - Connect stack : flare:1.0 envs : ENABLE_TOPIC_CREATION : \"true\" # persistentVolume: # name: persistent-v # directory: connectCity flare : job : explain : true inputs : - name : connect_city dataset : dataos://thirdparty01:none/city format : csv isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://pulsar:default?acl=rw checkpointLocation : dataos://icebase:sys01/checkpoints/connect-city/cl01/city03?acl=rw schemaRegistryUrl : http://schema-registry.caretaker:8081 steps : - sink : - sequenceName : connect_city datasetName : city_avro_pulsar outputName : output01 outputType : PulsarAvro outputOptions : extraOptions : pulsar.client.useKeyStoreTls : true pulsar.client.tlsTrustStorePassword : 3e128220-2b96-4e5d-b4fa-a0ce6ce77d1c pulsar.client.tlsTrustStoreType : JKS pulsar.client.tlsTrustStorePath : /etc/dataos/certs/truststore.jks pulsar.client.allowTlsInsecureConnection : false pulsar.client.enableTlsHostnameVerification : false #pulsar.client.useTls: true #pulsar.client.tlsAllowInsecureConnection: false #pulsar.client.tlsTrustCertsFilePath: /etc/dataos/certs/ca.cert.pem #pulsar.client.tlsEnableHostnameVerification: false tags : - Connect title : City Data AVRO Pulasr --- version : v1beta1 name : syn-off-tx type : workflow tags : - Offline - Syndicate description : This job is Syndicating offline transactions data #owner: itspiyush workflow : dag : - name : syn-off-tx-01-step title : Syndicate Offline transactions description : This job is Syndicating offline transactions data out into csv. spec : tags : - Offline - Syndicate stack : flare:1.0 flare : configs : {} driver : coreLimit : 2400m cores : 2 memory : 3072m executor : coreLimit : 2400m cores : 2 instances : 2 memory : 4096m job : explain : true inputs : - name : processed_transactions format : iceberg dataset : dataos://icebase:set01/pos_txn_enric_01 logLevel : INFO outputs : - name : output01 depot : dataos://syndicationgcs:syndicate?acl=rw steps : - sink : - sequenceName : syndicatePos datasetName : offline_01_csv description : Offline transactions into csv. outputName : output01 outputOptions : file : saveMode : Overwrite outputType : CSV tags : - Offline - Syndicate title : Offline Transactions sequence : - name : transactions sql : SELECT transaction_header.store_id, explode(transaction_line_item) as line_item, store FROM processed_transactions - name : syndicatePos sql : SELECT store_id, line_item.*, store.store_name, store.store_city, store.store_state FROM transactions variables : keepSystemColumns : \"false\"","title":"coconut"},{"location":"flare/flareproperties/","text":"Introduction to YAML YAML is a simple but robust data serialization language. It has just two data structures: - sequences (a list) - mappings (key and value pairs). Sequence (lists) \u00b6 Linux BSD Illumos Mapping (key value pairs) \u00b6 version: v1beta1 name: product-demo-01 type: workflow These structures can be combined and embedded. Sequence of mappings (list of pairs) - CPU: AMD RAM: \u201816 GB\u2019 - CPU: Intel RAM: \u201816 GB\u2019 Mapping sequences (key with many values) \u00b6 tags: - Connect - City functions: - name: rename column: id asColumn: country_id - name: rename column: name asColumn: country_name Sequence of sequences (a list of lists) \u00b6 pineapple coconut \u00b6 umbrella raincoat Mapping of mappings Joey: age: 22 sex: M Laura: age: 24 sex: F job: explain: true logLevel: INFO YAML for Flare Workflow \u00b6 Here is the skeleton of the YAML for the Flare workflow. \u00b6 version: name: type: tags: tag1 tag2 description: workflow: title: title of the workflow dag: name: title: description: The job ingests customer data from bigquery into raw zone spec: tags: tag1 tag2 stack: flare:1.0 flare: driver: coreLimit: 2400m cores: 2 memory: 3072m executor: coreLimit: 2400m cores: 2 instances: 2 memory: 4096m job: explain: true inputs: name: customer_connect dataset: dataos://crmbq:demo/customer_profiles logLevel: WARN outputs: name: output01 depot: dataos://icebase:retail?acl=rw steps: sink: sequenceName: customers datasetName: customer outputName: output01 outputType: Iceberg description: Customer data ingested from bigquery outputOptions: saveMode: overwrite iceberg: properties: write.format.default: parquet write.metadata.compression-codec: gzip partitionSpec: - type: identity column: version tags: Connect Customer title: Customer Source Data sequence: - name: customers doc: Pick all columns from customers and add version as yyyyMMddHHmm formatted timestamp. sql: SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_customer FROM customer_connect functions: - name: rename column: Id asColumn: id version \u00b6 Holds the current version of the Flare. Example: version: v1beta1 name: cnt-city-demo-01 type: workflow","title":"Flareproperties"},{"location":"flare/flareproperties/#sequence-lists","text":"Linux BSD Illumos","title":"Sequence (lists)"},{"location":"flare/flareproperties/#mapping-key-value-pairs","text":"version: v1beta1 name: product-demo-01 type: workflow These structures can be combined and embedded. Sequence of mappings (list of pairs) - CPU: AMD RAM: \u201816 GB\u2019 - CPU: Intel RAM: \u201816 GB\u2019","title":"Mapping (key value pairs)"},{"location":"flare/flareproperties/#mapping-sequences-key-with-many-values","text":"tags: - Connect - City functions: - name: rename column: id asColumn: country_id - name: rename column: name asColumn: country_name","title":"Mapping sequences (key with many values)"},{"location":"flare/flareproperties/#sequence-of-sequences-a-list-of-lists","text":"pineapple","title":"Sequence of sequences (a list of lists)"},{"location":"flare/flareproperties/#coconut","text":"umbrella raincoat Mapping of mappings Joey: age: 22 sex: M Laura: age: 24 sex: F job: explain: true logLevel: INFO","title":"coconut"},{"location":"flare/flareproperties/#yaml-for-flare-workflow","text":"","title":"YAML for Flare Workflow"},{"location":"flare/flareproperties/#here-is-the-skeleton-of-the-yaml-for-the-flare-workflow","text":"version: name: type: tags: tag1 tag2 description: workflow: title: title of the workflow dag: name: title: description: The job ingests customer data from bigquery into raw zone spec: tags: tag1 tag2 stack: flare:1.0 flare: driver: coreLimit: 2400m cores: 2 memory: 3072m executor: coreLimit: 2400m cores: 2 instances: 2 memory: 4096m job: explain: true inputs: name: customer_connect dataset: dataos://crmbq:demo/customer_profiles logLevel: WARN outputs: name: output01 depot: dataos://icebase:retail?acl=rw steps: sink: sequenceName: customers datasetName: customer outputName: output01 outputType: Iceberg description: Customer data ingested from bigquery outputOptions: saveMode: overwrite iceberg: properties: write.format.default: parquet write.metadata.compression-codec: gzip partitionSpec: - type: identity column: version tags: Connect Customer title: Customer Source Data sequence: - name: customers doc: Pick all columns from customers and add version as yyyyMMddHHmm formatted timestamp. sql: SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_customer FROM customer_connect functions: - name: rename column: Id asColumn: id","title":"Here is the skeleton of the YAML for the Flare workflow."},{"location":"flare/flareproperties/#version","text":"Holds the current version of the Flare. Example: version: v1beta1 name: cnt-city-demo-01 type: workflow","title":"version"},{"location":"flare/flaretuning/","text":"Flare Job Tuning \u00b6 Overview \u00b6 Flare in DataOS is built on top of Apache Spark to provide a computing platform. It is a declarative stack and defines Spark job processing configuration using YAML. Configuration setting in YAML abstracts the need for programming languages to write data processing jobs. You can easily modify the settings and properties of Spark, defined in YAML, to ensure that the resources are utilized properly and the jobs are executed efficiently. Fine tuning of Flare jobs \u00b6 Apache Spark defaults provide decent performance for large data sets but you can have significant performance gains if able to tune parameters based on resources and job. For your Flare workflows, you can modify Spark configuration parameters such as garbage collector selection, serialization, tweaking number of workers/executors, partitioning key, partition sizes etc. driver section \u00b6 Determine the type and number of instances based on application needs. Define them under driver section in the YAML file of your Flare job. driver : coreLimit : 1200m cores : 1 memory : 1024m executor : coreLimit : 1200m cores : 1 instances : 1 memory : 1024m sparkConf section \u00b6 Set the Spark configuration parameters carefully for Flare job to run successfully. Define them under sparkConf section in the YAML file of your Flare job. sparkConf : - spark.serializer : org.apache.spark.serializer.KryoSerializer - spark.sql.legacy.timeParserPolicy : LEGACY - spark.sql.shuffle.partitions : \"800\" - spark.executor.extraJavaOptions : -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.driver.extraJavaOptions : -XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.memory.storageFraction : \"0.1\" - spark.memory.fraction : \"0.1\" - spark.kryoserializer.buffer.max : 256m - spark.shuffle.memoryFraction : \"0.0\" Spark configuration parameters \u00b6 A brief introduction of the Spark configuration parameters is given below. To understand more about each of the parameters, see the Spark documentation . Parameter Description spark.executor.memory Size of memory to use for each executor that runs the task. spark.executor.cores Number of virtual cores. spark.driver.memory Size of memory to use for the driver. spark.driver.cores Number of virtual cores to use for the driver. spark.executor.instances \u00ad Number of executors. Set this parameter unless spark.dynamicAllocation.enabled is set to true. spark.default.parallelism Default number of partitions in resilient distributed datasets (RDDs) returned by transformations like join, reduceByKey, and parallelize when no partition number is set by the user. spark.network.timeout Timeout for all network transactions. spark.executor.heartbeatInterval Interval between each executor\u2019s heartbeats to the driver. This value should be significantly less than spark.network.timeout. spark.memory.fraction Fraction of JVM heap space used for Spark execution and storage. The lower this is, the more frequently spills and cached data eviction occur. spark.memory.storageFraction Expressed as a fraction of the size of the region set aside by spark.memory.fraction. The higher this is, the less working memory might be available to execution. This means that tasks might spill to disk more often. spark.yarn.scheduler.reporterThread.maxFailures Maximum number executor failures allowed before YARN can fail the application. spark.rdd.compress When set to true, this property can save substantial space at the cost of some extra CPU time by compressing the RDDs. spark.shuffle.compress When set to true, this property compresses the map output to save space. spark.shuffle.spill.compress When set to true, this property compresses the data spilled during shuffles. spark.sql.shuffle.partitions Sets the number of partitions for joins and aggregations. spark.serializer Sets the serializer to serialize or deserialize data. As a serializer, Kyro (org.apache.spark.serializer.KryoSerializer) is preferred, which is faster and more compact than the Java default serializer. coalesce Reduces the number of partitions to allow for less data movement. repartition Reduces or increases the number of partitions and performs full shuffle of data as opposed to coalesce. partitionBy Distributes data horizontally across partitions. bucketBy Decomposes data into more manageable parts (buckets) based on hashed columns. cache/persist Pulls datasets into a clusterwide in-memory cache. Doing this is useful when data is accessed repeatedly, such as when querying a small lookup dataset or when running an iterative algorithm.","title":"Flare Job Tuning"},{"location":"flare/flaretuning/#flare-job-tuning","text":"","title":"Flare Job Tuning"},{"location":"flare/flaretuning/#overview","text":"Flare in DataOS is built on top of Apache Spark to provide a computing platform. It is a declarative stack and defines Spark job processing configuration using YAML. Configuration setting in YAML abstracts the need for programming languages to write data processing jobs. You can easily modify the settings and properties of Spark, defined in YAML, to ensure that the resources are utilized properly and the jobs are executed efficiently.","title":"Overview"},{"location":"flare/flaretuning/#fine-tuning-of-flare-jobs","text":"Apache Spark defaults provide decent performance for large data sets but you can have significant performance gains if able to tune parameters based on resources and job. For your Flare workflows, you can modify Spark configuration parameters such as garbage collector selection, serialization, tweaking number of workers/executors, partitioning key, partition sizes etc.","title":"Fine tuning of Flare jobs"},{"location":"flare/flaretuning/#driver-section","text":"Determine the type and number of instances based on application needs. Define them under driver section in the YAML file of your Flare job. driver : coreLimit : 1200m cores : 1 memory : 1024m executor : coreLimit : 1200m cores : 1 instances : 1 memory : 1024m","title":"driver section"},{"location":"flare/flaretuning/#sparkconf-section","text":"Set the Spark configuration parameters carefully for Flare job to run successfully. Define them under sparkConf section in the YAML file of your Flare job. sparkConf : - spark.serializer : org.apache.spark.serializer.KryoSerializer - spark.sql.legacy.timeParserPolicy : LEGACY - spark.sql.shuffle.partitions : \"800\" - spark.executor.extraJavaOptions : -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.driver.extraJavaOptions : -XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.memory.storageFraction : \"0.1\" - spark.memory.fraction : \"0.1\" - spark.kryoserializer.buffer.max : 256m - spark.shuffle.memoryFraction : \"0.0\"","title":"sparkConf section"},{"location":"flare/flaretuning/#spark-configuration-parameters","text":"A brief introduction of the Spark configuration parameters is given below. To understand more about each of the parameters, see the Spark documentation . Parameter Description spark.executor.memory Size of memory to use for each executor that runs the task. spark.executor.cores Number of virtual cores. spark.driver.memory Size of memory to use for the driver. spark.driver.cores Number of virtual cores to use for the driver. spark.executor.instances \u00ad Number of executors. Set this parameter unless spark.dynamicAllocation.enabled is set to true. spark.default.parallelism Default number of partitions in resilient distributed datasets (RDDs) returned by transformations like join, reduceByKey, and parallelize when no partition number is set by the user. spark.network.timeout Timeout for all network transactions. spark.executor.heartbeatInterval Interval between each executor\u2019s heartbeats to the driver. This value should be significantly less than spark.network.timeout. spark.memory.fraction Fraction of JVM heap space used for Spark execution and storage. The lower this is, the more frequently spills and cached data eviction occur. spark.memory.storageFraction Expressed as a fraction of the size of the region set aside by spark.memory.fraction. The higher this is, the less working memory might be available to execution. This means that tasks might spill to disk more often. spark.yarn.scheduler.reporterThread.maxFailures Maximum number executor failures allowed before YARN can fail the application. spark.rdd.compress When set to true, this property can save substantial space at the cost of some extra CPU time by compressing the RDDs. spark.shuffle.compress When set to true, this property compresses the map output to save space. spark.shuffle.spill.compress When set to true, this property compresses the data spilled during shuffles. spark.sql.shuffle.partitions Sets the number of partitions for joins and aggregations. spark.serializer Sets the serializer to serialize or deserialize data. As a serializer, Kyro (org.apache.spark.serializer.KryoSerializer) is preferred, which is faster and more compact than the Java default serializer. coalesce Reduces the number of partitions to allow for less data movement. repartition Reduces or increases the number of partitions and performs full shuffle of data as opposed to coalesce. partitionBy Distributes data horizontally across partitions. bucketBy Decomposes data into more manageable parts (buckets) based on hashed columns. cache/persist Pulls datasets into a clusterwide in-memory cache. Doing this is useful when data is accessed repeatedly, such as when querying a small lookup dataset or when running an iterative algorithm.","title":"Spark configuration parameters"},{"location":"flare/functions/","text":"Functions \u00b6 Flare allows the user to use various functions for data manipulations at different stages of processing. The following list gives the details about the functions and their usage. any_date \u00b6 Function Description any_date The any_date function converts a date column given in any format like yyyy.mm.dd, dd.mm.yyyy, dd/MM/yy, yyyy/mm/dd and other(total 18 different format) into (yyyy-mm-dd) format. Snippet: functions : - name : any_date column : __inq_date asColumn : inquiry_date rules : - \"\u3010(?<year>\\\\d{4})\\\\W{1}(?<month>\\\\d{1,2})\\\\W{1}(?<day>\\\\d{1,2})[^\\\\d]? \\\\W*(?:at )?(?<hour>\\\\d{1,2}):(?<minute>\\\\d{1,2})(?::(?<second>\\\\d{1,2}))?(?:[.,](?<ns>\\\\d{1,9}))?(?<zero>z)?\u3011\" Note : You can provide explicit regex rules for the column passed in the any_date function. Without an explicit regex, the function will use the default format. any_timestamp \u00b6 Function Description any_timestamp The any_timestamp function converts a timestamp column into the timestamp format as per the specified timezone. Snippet: functions : - name : any_timestamp column : transaction_date asColumn : transaction_date timezone : Asia/Kolkata rules : - \"\u3010(?<year>\\\\d{4})\\\\W{1}(?<month>\\\\d{1,2})\\\\W{1}(?<day>\\\\d{1,2})[^\\\\d]? \\\\W*(?:at )?(?<hour>\\\\d{1,2}):(?<minute>\\\\d{1,2})(?::(?<second>\\\\d{1,2}))?(?:[.,](?<ns>\\\\d{1,9}))?(?<zero>z)?\u3011\" Note : You can provide explicit regex rules for the column passed in the any_timestamp function. Without an explicit regex, the function will use the default format. change_case \u00b6 Function Description change_case The UPPERCASE, LOWERCASE, and TITLECASE functions change the case ( lower/upper/title ) of column values they are applied to. Snippet: functions : - name : change_case case : lower column : asin cleanse_column_names \u00b6 Function Description cleanse_column_names The CLEANSE-COLUMN-NAMES function sanatizes column names, following these rules: 1. Trim leading and trailing spaces 2. Lowercases the column name 3. Replaces any character that are not one of [A-Z][a-z][0-9] or _ with an underscore (_) Snippet: functions : - name : cleanse_column_names cut_character \u00b6 Function Description cut_character The CUT_CHARACTER function selects parts of a string value, accepting standard cut options. Snippet: functions : - name : cut_character column : title asColumn : title_prefix startIndex : 1 endIndex : 5 decode \u00b6 Function Description decode The DECODE function decodes a column value as one of base32 , base64 , or hex following RFC-4648 and adds one more column like - col1_decode_base32 Snippet: functions : - name : decode algo : base64 column : id asColumn : id_1 drop \u00b6 Function Description drop The DROP function is used to drop the specified columns in a record. Snippet: functions : - name : drop columns : - column_1 - column_2 encode \u00b6 Function Description encode The ENCODE function encodes a column value as one of base32 , base64 , or hex following RFC-4648 and adds one more column like - col1_encode_base32 Snippet: functions : - name : encode algo : base64 column : id asColumn : id_2 Function Description diff_date The DIFF_DATE function calculates the difference between two dates. Snippet: functions : - name : diff_date columnA : col_a columnB : col_b asColumn : col_diff_date fill_null_or_empty \u00b6 Function Description fill_null_or_empty The FILL_NULL_OR_EMPTY function fills column value with a fixed value if it is either null or empty (\"\"). If the column does not exist, the function will fail. The defaultValue can only be of type string. Snippet: functions : - name : fill_null_or_empty columns : column1 : value1 column2 : value2 find_and_replace \u00b6 Function Description find_and_replace The FIND_AND_REPLACE function transforms string column values using a \"sed\"-like expression to find and replace text within the same column. Snippet: functions : - name : find_and_replace column : title sedExpression : \"s/regex/replacement/g\" flatten \u00b6 Function Description flatten The FLATTEN function separates the elements in a repeated field into individual records. It is useful for the flexible exploration of repeated data. To maintain the association between each flattened value and the other fields in the record, the FLATTEN directive copies all of the other columns into each new record. Snippet: functions : - name : flatten column : array_holding_column asColumn : new_column_name format_date \u00b6 Function Description format_date The FORMAT_DATE function allows custom patterns for date-time formatting. It formats the date value according to the format string. | Snippet: functions : - name : format_date column : date_column format : \"yyyy-MM-dd'T'HH:mm:ss\" format_unix_date \u00b6 Function Description format_unix_date The FORMAT_UNIX_DATE function allows custom patterns for date-time formatting. Snippet: functions : - name : format_unix_date column : unix_epoch_column format : \"yyyy-MM-dd'T'HH:mm:ss\" generate_uuid \u00b6 Function Description generate_uuid The GENERATE_UUID function generates a universally unique identifier (UUID) of the record. Snippet: functions : - name : generate_uuid asColumn : column_01 hash \u00b6 Function Description hash The HASH function generates a message digest. The column is replaced with the digest created using the supplied algorithm. The type of column is a string. Snippet: functions : - name : hash column : column_to_hash algo : MD5 | SHA-1 | SHA-256 | SHA-384 | SHA-512 increment_variable \u00b6 Function Description increment_variable The INCREMENT_VARIABLE function increments the value of the variable that is local to the input record being processed. Snippet: functions : - name : increment_variable column : column_01 mask_number \u00b6 Function Description mask_number The MASK_NUMBER function applies substitution masking on the column values. The 'column' specifies the name of an existing column to be masked. The 'pattern' is a substitution pattern to be used to mask the column values. Substitution masking is generally used for masking credit cards or social security numbers. The MASK_NUMBER applies substitution masking on the column values. This type of masking is fixed masking, where the pattern is applied on the fixed-length string. These rules are used for the pattern: 1. Use of # will include the digit from the position 2. Use x or any other character to mask the digit at that position e.g. For SSN '000-00-0000' and pattern: 'XXX-XX-####' output would be like: XXX-XX-0000 | Snippet: functions : - name : mask_number column : ssn pattern : XXX-XX-#### merge \u00b6 Function Description merge The MERGE function merges two or more columns by inserting a third column specified as asColumn into a row. The values in the third column are merged values from the specified columns delimited by a specified separator . Snippet: functions : - name : merge separator : \"__\" columns : - first_name - last_name asColumn : full_name parse_as_json \u00b6 Function Description parse_as_json The PARSE_AS_JSON function is for parsing a JSON object. The function can operate on String or JSON object types. It requires spark schema json to parse the json back into dataframe. Snippet: functions : - name : parse_as_json column : json_string_column_name asColumn : column_name sparkSchema : \"<spark_schema_json>\" avroSchema : \"<avro_schema>\" parse_html \u00b6 Function Description parse_html The parse-html function is used to create a new column asColumn by converting the HTML coded string in column. Note : The function works using the jsoup library. You can find more details about this library here: GitHub - jhy/jsoup: jsoup: the Java HTML parser, built for HTML editing, cleaning, scraping, and XSS safety . Snippet: functions : - name : parse_html column : questionText asColumn : parsedText pivot \u00b6 Function Description pivot The Pivot function is used to reorganise data stored in a DataFrame/Dataset. It allows you to rotate unique values from one column in the expression into multiple columns in the output. It allows getting aggregated values based on specific column values. These values will appear as columns in the result. Parameters: aggregate_expression: requires an aggregate function(sum, count, avg etc)along with alias. values(optional): specifies only those values needed after pivot from column. approach(optional): can be set to \u201ctwo-phase\u201d for running an optimised version query on large datasets. Snippet: functions : - name : pivot groupBy : - \"Product\" - \"Country\" column : \"Country\" values : - \"USA\" - \"Mexico\" - \"India\" aggregateExpression : \"sum(Amount) as Amount\" approach : \"two-phase\" rename_all \u00b6 Function Description rename The RENAME_ALL function will rename all supplied columns with the corresponding given column names. Snippet: functions : - name : rename_all columns : column1 : new_column1 column2 : new_column2 select \u00b6 Function Description select The SELECT function is used to keep specified columns from the record. This is the opposite behavior of the DROP function. Snippet: functions : - name : select columns : - column_01 - column_02 - column_03 - column_04 - column_05 - column_06 set_column \u00b6 Function Description set_column The SET_COLUMN function will change name of a supplied in column to value in asColumn . Snippet: functions : - name : set_column column : my_col_name value : \"some value here\" set_type \u00b6 Function Description set_type Convert data type of given columns. Here type can be one of spark data types e.g. int, string, long, double etc. Snippet: functions : - name : set_type columns : column1 : type column2 : type set_variable \u00b6 Function Description set_variable The SET_VARIABLE function evaluates the expression supplied and sets the value in the variable. Snippet: functions : - name : set_variable column : some_new_column expression : \"ROUND(AVG(src_bytes), 2)\" snake_case \u00b6 Function Description snake_case The SNAKE_CASE function converts column names from camel case to snake case and is only applicable for batch dataframe/ job. Snippet: functions : - name : snake_case split_email \u00b6 Function Description split_email The SPLIT_EMAIL function splits an email ID into an account and its domain. The column is a column containing an email address. The function will parse email address into its constituent parts: account and domain. After splitting the email address, the directive will create two new columns, appending to the original column name: column_account column_domain If the email address cannot be parsed correctly, the additional columns will still be generated, but they would be set to null depending on the parts that could not be parsed. Snippet: functions : - name : split_email column : email_column split_url \u00b6 Function Description split_url The SPLIT_URL function splits a URL into protocol, authority, host, port, path, filename, and query. The SPLIT_URL function will parse the URL into its constituents. Upon splitting the URL, the directive creates seven new columns by appending to the original column name: column_authority column_protocol column_host column_port column_path column_filename column_query If the URL cannot be parsed correctly, an exception is throw. If the URL column does not exist, columns with a null value are added to the record. Snippet: functions : - name : split_url column : column_with_url_content swap \u00b6 Function Description swap The SWAP function swaps column names of two columns. Snippet: functions : - name : swap columnA : col_1 columnB : col_2 trim \u00b6 Function Description trim The TRIM function trim whitespace from both sides, left side or right side of string values they are applied to. One can supply method as trim|ltrim|rtrim Snippet: functions : - name : trim column : col_001 method : trim unfurl \u00b6 Function Description unfurl The UNFURL function uses the expression supplied to pull columns from within the nested json attribute out e.g. columnName.* Snippet: functions : - name : unfurl expression : \"col01.*\" - name : unfurl expression : explode(\"col01\")","title":"Functions"},{"location":"flare/functions/#functions","text":"Flare allows the user to use various functions for data manipulations at different stages of processing. The following list gives the details about the functions and their usage.","title":"Functions"},{"location":"flare/functions/#any_date","text":"Function Description any_date The any_date function converts a date column given in any format like yyyy.mm.dd, dd.mm.yyyy, dd/MM/yy, yyyy/mm/dd and other(total 18 different format) into (yyyy-mm-dd) format. Snippet: functions : - name : any_date column : __inq_date asColumn : inquiry_date rules : - \"\u3010(?<year>\\\\d{4})\\\\W{1}(?<month>\\\\d{1,2})\\\\W{1}(?<day>\\\\d{1,2})[^\\\\d]? \\\\W*(?:at )?(?<hour>\\\\d{1,2}):(?<minute>\\\\d{1,2})(?::(?<second>\\\\d{1,2}))?(?:[.,](?<ns>\\\\d{1,9}))?(?<zero>z)?\u3011\" Note : You can provide explicit regex rules for the column passed in the any_date function. Without an explicit regex, the function will use the default format.","title":"any_date"},{"location":"flare/functions/#any_timestamp","text":"Function Description any_timestamp The any_timestamp function converts a timestamp column into the timestamp format as per the specified timezone. Snippet: functions : - name : any_timestamp column : transaction_date asColumn : transaction_date timezone : Asia/Kolkata rules : - \"\u3010(?<year>\\\\d{4})\\\\W{1}(?<month>\\\\d{1,2})\\\\W{1}(?<day>\\\\d{1,2})[^\\\\d]? \\\\W*(?:at )?(?<hour>\\\\d{1,2}):(?<minute>\\\\d{1,2})(?::(?<second>\\\\d{1,2}))?(?:[.,](?<ns>\\\\d{1,9}))?(?<zero>z)?\u3011\" Note : You can provide explicit regex rules for the column passed in the any_timestamp function. Without an explicit regex, the function will use the default format.","title":"any_timestamp"},{"location":"flare/functions/#change_case","text":"Function Description change_case The UPPERCASE, LOWERCASE, and TITLECASE functions change the case ( lower/upper/title ) of column values they are applied to. Snippet: functions : - name : change_case case : lower column : asin","title":"change_case"},{"location":"flare/functions/#cleanse_column_names","text":"Function Description cleanse_column_names The CLEANSE-COLUMN-NAMES function sanatizes column names, following these rules: 1. Trim leading and trailing spaces 2. Lowercases the column name 3. Replaces any character that are not one of [A-Z][a-z][0-9] or _ with an underscore (_) Snippet: functions : - name : cleanse_column_names","title":"cleanse_column_names"},{"location":"flare/functions/#cut_character","text":"Function Description cut_character The CUT_CHARACTER function selects parts of a string value, accepting standard cut options. Snippet: functions : - name : cut_character column : title asColumn : title_prefix startIndex : 1 endIndex : 5","title":"cut_character"},{"location":"flare/functions/#decode","text":"Function Description decode The DECODE function decodes a column value as one of base32 , base64 , or hex following RFC-4648 and adds one more column like - col1_decode_base32 Snippet: functions : - name : decode algo : base64 column : id asColumn : id_1","title":"decode"},{"location":"flare/functions/#drop","text":"Function Description drop The DROP function is used to drop the specified columns in a record. Snippet: functions : - name : drop columns : - column_1 - column_2","title":"drop"},{"location":"flare/functions/#encode","text":"Function Description encode The ENCODE function encodes a column value as one of base32 , base64 , or hex following RFC-4648 and adds one more column like - col1_encode_base32 Snippet: functions : - name : encode algo : base64 column : id asColumn : id_2 Function Description diff_date The DIFF_DATE function calculates the difference between two dates. Snippet: functions : - name : diff_date columnA : col_a columnB : col_b asColumn : col_diff_date","title":"encode"},{"location":"flare/functions/#fill_null_or_empty","text":"Function Description fill_null_or_empty The FILL_NULL_OR_EMPTY function fills column value with a fixed value if it is either null or empty (\"\"). If the column does not exist, the function will fail. The defaultValue can only be of type string. Snippet: functions : - name : fill_null_or_empty columns : column1 : value1 column2 : value2","title":"fill_null_or_empty"},{"location":"flare/functions/#find_and_replace","text":"Function Description find_and_replace The FIND_AND_REPLACE function transforms string column values using a \"sed\"-like expression to find and replace text within the same column. Snippet: functions : - name : find_and_replace column : title sedExpression : \"s/regex/replacement/g\"","title":"find_and_replace"},{"location":"flare/functions/#flatten","text":"Function Description flatten The FLATTEN function separates the elements in a repeated field into individual records. It is useful for the flexible exploration of repeated data. To maintain the association between each flattened value and the other fields in the record, the FLATTEN directive copies all of the other columns into each new record. Snippet: functions : - name : flatten column : array_holding_column asColumn : new_column_name","title":"flatten"},{"location":"flare/functions/#format_date","text":"Function Description format_date The FORMAT_DATE function allows custom patterns for date-time formatting. It formats the date value according to the format string. | Snippet: functions : - name : format_date column : date_column format : \"yyyy-MM-dd'T'HH:mm:ss\"","title":"format_date"},{"location":"flare/functions/#format_unix_date","text":"Function Description format_unix_date The FORMAT_UNIX_DATE function allows custom patterns for date-time formatting. Snippet: functions : - name : format_unix_date column : unix_epoch_column format : \"yyyy-MM-dd'T'HH:mm:ss\"","title":"format_unix_date"},{"location":"flare/functions/#generate_uuid","text":"Function Description generate_uuid The GENERATE_UUID function generates a universally unique identifier (UUID) of the record. Snippet: functions : - name : generate_uuid asColumn : column_01","title":"generate_uuid"},{"location":"flare/functions/#hash","text":"Function Description hash The HASH function generates a message digest. The column is replaced with the digest created using the supplied algorithm. The type of column is a string. Snippet: functions : - name : hash column : column_to_hash algo : MD5 | SHA-1 | SHA-256 | SHA-384 | SHA-512","title":"hash"},{"location":"flare/functions/#increment_variable","text":"Function Description increment_variable The INCREMENT_VARIABLE function increments the value of the variable that is local to the input record being processed. Snippet: functions : - name : increment_variable column : column_01","title":"increment_variable"},{"location":"flare/functions/#mask_number","text":"Function Description mask_number The MASK_NUMBER function applies substitution masking on the column values. The 'column' specifies the name of an existing column to be masked. The 'pattern' is a substitution pattern to be used to mask the column values. Substitution masking is generally used for masking credit cards or social security numbers. The MASK_NUMBER applies substitution masking on the column values. This type of masking is fixed masking, where the pattern is applied on the fixed-length string. These rules are used for the pattern: 1. Use of # will include the digit from the position 2. Use x or any other character to mask the digit at that position e.g. For SSN '000-00-0000' and pattern: 'XXX-XX-####' output would be like: XXX-XX-0000 | Snippet: functions : - name : mask_number column : ssn pattern : XXX-XX-####","title":"mask_number"},{"location":"flare/functions/#merge","text":"Function Description merge The MERGE function merges two or more columns by inserting a third column specified as asColumn into a row. The values in the third column are merged values from the specified columns delimited by a specified separator . Snippet: functions : - name : merge separator : \"__\" columns : - first_name - last_name asColumn : full_name","title":"merge"},{"location":"flare/functions/#parse_as_json","text":"Function Description parse_as_json The PARSE_AS_JSON function is for parsing a JSON object. The function can operate on String or JSON object types. It requires spark schema json to parse the json back into dataframe. Snippet: functions : - name : parse_as_json column : json_string_column_name asColumn : column_name sparkSchema : \"<spark_schema_json>\" avroSchema : \"<avro_schema>\"","title":"parse_as_json"},{"location":"flare/functions/#parse_html","text":"Function Description parse_html The parse-html function is used to create a new column asColumn by converting the HTML coded string in column. Note : The function works using the jsoup library. You can find more details about this library here: GitHub - jhy/jsoup: jsoup: the Java HTML parser, built for HTML editing, cleaning, scraping, and XSS safety . Snippet: functions : - name : parse_html column : questionText asColumn : parsedText","title":"parse_html"},{"location":"flare/functions/#pivot","text":"Function Description pivot The Pivot function is used to reorganise data stored in a DataFrame/Dataset. It allows you to rotate unique values from one column in the expression into multiple columns in the output. It allows getting aggregated values based on specific column values. These values will appear as columns in the result. Parameters: aggregate_expression: requires an aggregate function(sum, count, avg etc)along with alias. values(optional): specifies only those values needed after pivot from column. approach(optional): can be set to \u201ctwo-phase\u201d for running an optimised version query on large datasets. Snippet: functions : - name : pivot groupBy : - \"Product\" - \"Country\" column : \"Country\" values : - \"USA\" - \"Mexico\" - \"India\" aggregateExpression : \"sum(Amount) as Amount\" approach : \"two-phase\"","title":"pivot"},{"location":"flare/functions/#rename_all","text":"Function Description rename The RENAME_ALL function will rename all supplied columns with the corresponding given column names. Snippet: functions : - name : rename_all columns : column1 : new_column1 column2 : new_column2","title":"rename_all"},{"location":"flare/functions/#select","text":"Function Description select The SELECT function is used to keep specified columns from the record. This is the opposite behavior of the DROP function. Snippet: functions : - name : select columns : - column_01 - column_02 - column_03 - column_04 - column_05 - column_06","title":"select"},{"location":"flare/functions/#set_column","text":"Function Description set_column The SET_COLUMN function will change name of a supplied in column to value in asColumn . Snippet: functions : - name : set_column column : my_col_name value : \"some value here\"","title":"set_column"},{"location":"flare/functions/#set_type","text":"Function Description set_type Convert data type of given columns. Here type can be one of spark data types e.g. int, string, long, double etc. Snippet: functions : - name : set_type columns : column1 : type column2 : type","title":"set_type"},{"location":"flare/functions/#set_variable","text":"Function Description set_variable The SET_VARIABLE function evaluates the expression supplied and sets the value in the variable. Snippet: functions : - name : set_variable column : some_new_column expression : \"ROUND(AVG(src_bytes), 2)\"","title":"set_variable"},{"location":"flare/functions/#snake_case","text":"Function Description snake_case The SNAKE_CASE function converts column names from camel case to snake case and is only applicable for batch dataframe/ job. Snippet: functions : - name : snake_case","title":"snake_case"},{"location":"flare/functions/#split_email","text":"Function Description split_email The SPLIT_EMAIL function splits an email ID into an account and its domain. The column is a column containing an email address. The function will parse email address into its constituent parts: account and domain. After splitting the email address, the directive will create two new columns, appending to the original column name: column_account column_domain If the email address cannot be parsed correctly, the additional columns will still be generated, but they would be set to null depending on the parts that could not be parsed. Snippet: functions : - name : split_email column : email_column","title":"split_email"},{"location":"flare/functions/#split_url","text":"Function Description split_url The SPLIT_URL function splits a URL into protocol, authority, host, port, path, filename, and query. The SPLIT_URL function will parse the URL into its constituents. Upon splitting the URL, the directive creates seven new columns by appending to the original column name: column_authority column_protocol column_host column_port column_path column_filename column_query If the URL cannot be parsed correctly, an exception is throw. If the URL column does not exist, columns with a null value are added to the record. Snippet: functions : - name : split_url column : column_with_url_content","title":"split_url"},{"location":"flare/functions/#swap","text":"Function Description swap The SWAP function swaps column names of two columns. Snippet: functions : - name : swap columnA : col_1 columnB : col_2","title":"swap"},{"location":"flare/functions/#trim","text":"Function Description trim The TRIM function trim whitespace from both sides, left side or right side of string values they are applied to. One can supply method as trim|ltrim|rtrim Snippet: functions : - name : trim column : col_001 method : trim","title":"trim"},{"location":"flare/functions/#unfurl","text":"Function Description unfurl The UNFURL function uses the expression supplied to pull columns from within the nested json attribute out e.g. columnName.* Snippet: functions : - name : unfurl expression : \"col01.*\" - name : unfurl expression : explode(\"col01\")","title":"unfurl"},{"location":"flare/gcpsecretpolicy/","text":"GCP Service Account Permissions \u00b6 Overview \u00b6 This use case describes the strategy for getting Required permissions from the client on the Google service account to access the data from their cloud storage. This service account will enable data processing Flare job to access data from the client's Google cloud storage account. Solution approach \u00b6 A service account is a special type of Google account that can be authorized to access data in Cloud Storage. This service account is created and configured during the install process of DataOS on Google cloud. Please note that this service account is a resource with IAM policies attached to it, and these policies determine who can use the service account. We create a Depot definition providing credentials of this service account to enable data access. Flare workflow that will run the data processing job will use this Depot to access data from the cloud storage. In the case of GCP cloud, while creating a Depot, we can not configure credentials per bucket as access to GCP services is enabled on a per-project/account basis. So, when we need to read data from the client's storage(bigquery, big table/storage location), we need to ask for Read permissions on this service account, allowing it to access a client's data resources. In this case, the service account is the identity that is granted permissions for another resource (client's Cloud Storage bucket). Depending on the data processing job, we need to have all the required permissions on the service account to access the data from the client's storage, such as: BigQuery Data Viewer BigQuery Job User BigQuery Metadata Viewer BigQuery Read Session User Storage Admin (For Read & Write access) Storage Object Viewer","title":"GCP Service Account Permissions"},{"location":"flare/gcpsecretpolicy/#gcp-service-account-permissions","text":"","title":"GCP Service Account Permissions"},{"location":"flare/gcpsecretpolicy/#overview","text":"This use case describes the strategy for getting Required permissions from the client on the Google service account to access the data from their cloud storage. This service account will enable data processing Flare job to access data from the client's Google cloud storage account.","title":"Overview"},{"location":"flare/gcpsecretpolicy/#solution-approach","text":"A service account is a special type of Google account that can be authorized to access data in Cloud Storage. This service account is created and configured during the install process of DataOS on Google cloud. Please note that this service account is a resource with IAM policies attached to it, and these policies determine who can use the service account. We create a Depot definition providing credentials of this service account to enable data access. Flare workflow that will run the data processing job will use this Depot to access data from the cloud storage. In the case of GCP cloud, while creating a Depot, we can not configure credentials per bucket as access to GCP services is enabled on a per-project/account basis. So, when we need to read data from the client's storage(bigquery, big table/storage location), we need to ask for Read permissions on this service account, allowing it to access a client's data resources. In this case, the service account is the identity that is granted permissions for another resource (client's Cloud Storage bucket). Depending on the data processing job, we need to have all the required permissions on the service account to access the data from the client's storage, such as: BigQuery Data Viewer BigQuery Job User BigQuery Metadata Viewer BigQuery Read Session User Storage Admin (For Read & Write access) Storage Object Viewer","title":"Solution approach"},{"location":"flare/incremental/","text":"Incremental Data Workload \u00b6 Overview \u00b6 This use case describes a scenario where your data source contains a large amount of data that are continuously updated. Reloading the entire data set can be time-consuming. So you want only to read new data from the data source. The incremental load method is more efficient as compared to full data load when working with a huge volume of data. Solution approach \u00b6 Iceberg has snapshotId and timestamp: timestamp: It is explicitly specified by the user to specify the scope of consumption. As start_timestamp of reading. Iceberg provides the ability to read deltas between start/end commit timestamps on a table and resume reading from the last read end timestamp. So there are scenarios where the timestamp is used for the incremental read. Implementation details \u00b6 To configure your Flare job for the Incremental read, you need to identify what is new data in an evolving data coming from the data source in Iceberg. This new data needs to be captured and stored on a regular interval. Here, we are adding a workflow that reads data incrementally based on commit timestamp. The \u2018start_time\u2019 and \u2018end_time\u2019 timestamps are defined and updated using sql query. Outcomes \u00b6 Code files \u00b6 workflow: dag: - name: incremental-read title: read cloudevent data into usage datasets description: This job transforms cloudevent data into usage datasets spec: tags: - Transformation stack: flare:1.0 tier: connect flare: job: explain: true inputs: - name: querydata dataset: dataos://icebase:sys01/cloudevents?acl=r incremental: # configuration for incremental read context: incrinput # context name sql: select * from incrinput where time > '$|start_time|' AND time <= '$|end_time|' keys: - name: start_time default: \"2021-08-17 00:00:00\" #sql: select to_timestamp(\"2021-08-17 00:00:00\") updateFromKey: end_time - name: end_time sql: select current_timestamp() output: # change save mode to append sink: outputOptions: saveMode: append","title":"Incremental Data Workload"},{"location":"flare/incremental/#incremental-data-workload","text":"","title":"Incremental Data Workload"},{"location":"flare/incremental/#overview","text":"This use case describes a scenario where your data source contains a large amount of data that are continuously updated. Reloading the entire data set can be time-consuming. So you want only to read new data from the data source. The incremental load method is more efficient as compared to full data load when working with a huge volume of data.","title":"Overview"},{"location":"flare/incremental/#solution-approach","text":"Iceberg has snapshotId and timestamp: timestamp: It is explicitly specified by the user to specify the scope of consumption. As start_timestamp of reading. Iceberg provides the ability to read deltas between start/end commit timestamps on a table and resume reading from the last read end timestamp. So there are scenarios where the timestamp is used for the incremental read.","title":"Solution approach"},{"location":"flare/incremental/#implementation-details","text":"To configure your Flare job for the Incremental read, you need to identify what is new data in an evolving data coming from the data source in Iceberg. This new data needs to be captured and stored on a regular interval. Here, we are adding a workflow that reads data incrementally based on commit timestamp. The \u2018start_time\u2019 and \u2018end_time\u2019 timestamps are defined and updated using sql query.","title":"Implementation details"},{"location":"flare/incremental/#outcomes","text":"","title":"Outcomes"},{"location":"flare/incremental/#code-files","text":"workflow: dag: - name: incremental-read title: read cloudevent data into usage datasets description: This job transforms cloudevent data into usage datasets spec: tags: - Transformation stack: flare:1.0 tier: connect flare: job: explain: true inputs: - name: querydata dataset: dataos://icebase:sys01/cloudevents?acl=r incremental: # configuration for incremental read context: incrinput # context name sql: select * from incrinput where time > '$|start_time|' AND time <= '$|end_time|' keys: - name: start_time default: \"2021-08-17 00:00:00\" #sql: select to_timestamp(\"2021-08-17 00:00:00\") updateFromKey: end_time - name: end_time sql: select current_timestamp() output: # change save mode to append sink: outputOptions: saveMode: append","title":"Code files"},{"location":"flare/infolake/","text":"Why adopt MML? Challenges that MML can solve Key considerations for MML What all scenarios where you consider using MML Best Use case How MML works? Compnents How to set up MML The story flow of this article is (THIS SECTION IS FOR INTERNAL CONSUMPTION ONLY) What is preventing the usage of existing data platforms by business and non-technical users ? What have we done in DataOS to make it usable by everyone? What did we introduce to abstract out the technical protocols and need for programming languages ? How can users with even limited SQL skills make the most of DataOS ? In a data-driven business world, for many organisations, a state-of-the-art data and analytics platform is no longer an option but a necessity. These platforms act as a central repository for enterprise-wide data and empowers businesses to translate data into business value. Over the last decade, these organisations have invested a lot in building best data platforms but have often failed in achieving true value from their data at rapid pace. One of the primary reasons for this is that the business users always depended on Technical teams for data because the data platforms demand programming and niche technical expertise. While building DataOS, we had one core agenda in mind - it should be used intuitively by a wide range of users. This means DataOS should make it possible for all users to easily discover and analyze data within the platform, understand the context associated with data, such as column descriptions, history and lineage and derive insights from data with minimal dependencies on the data or IT team. Data processing made easy with DataOS Executing data processing jobs has remained a nightmare for data engineers and with DataOS we wanted to simplify this. To achieve this we have built our own data processing engine called \u2018Rio\u2019. DataOS\u00ae Rio allows data engineers to ingest both batch and stream data processing. Rio Flare is a library that simplifies writing and executing ETL jobs and is built on top of Apache Spark. Typically when you use spark, in data processing, you need to write programs in a programming language like Python,Java or Scala. This requires users to have prior knowledge of one of these programming languages. In DataOS, we have abstracted out the programming interface and turned it into declarative. Flare allows you to declare your Transformation steps in simple YAML configuration files and then runs it on DataOS\u00ae managed Spark Cluster. Example : For a data ingestion job users can simply declare the source system from where the data needs to be read, transformation logic in simple SQL syntax and final destination where the data needs to be written. DataOS Flare abstracts out all the complexities related to different storage system protocols and performs the job for you. Screenshot showing details of input, output and steps in a sample data ingestion flare job So, essentially, we have tried to take out programming languages as a necessary skill and made things easy by introducing SQL as the only required skills for users. Contrary to earlier, users can now work on Spark if they are equipped with SQL, which many users are comfortable with. Data ingestion made easy with DataOS One of the core pain points for data engineers during data ingestion is to ingest data from varied data storage systems. They need to learn and understand the underlying protocols of these systems and deploy these ingestion jobs as per the pre-configured protocols. Organisations these days, store data in multiple sources in varied formats. Now, imagine the time these data engineers spend on understandingRemember, we said that we have abstracted out the need for programming languages while writing data processing jobs, similarly we have abstracted out the concept of varied storage protocols for different storages. Let me explain it with an example. Organisations store data in Redshift using it as DataWarehouse, BigQuery, S3 as blob storage or Snowflake. Now all of these storage systems have different storage protocols, as in how can we query data from these storage systems. What we have essentially done with DataOS is, we have thoroughly understood the underlying technologies and protocols behind these storage systems and created one standard SQL way for users, so that they can query and access data from multiple data source systems. In DataOS, we have made this possible by introducing DataOS \u2018Depots\u2019. More about DataOS Depots DataOS depots enable Data Engineers to write data ingestion jobs irrespective of the source data storage systems. Depots abstract out the challenge of accessing data stored across varied storage systems. Depots contain connection details about the data source and destination systems and the information about the type of data storage systems. Depots encapsulates the complexity and surfaces a simple dataset address and users can access their data using SQL. Backend protocols pertaining to data accessibility of that source system becomes immaterial now. From an end user point of view, accessing a kafka topic or data from a big query table is one and the same. The back end technical protocols of these two will be abstracted and taken care of by DataOS depots. In the next section, let us briefly understand about SQL workbench that brings in immense data capabilities into DataOS. DataOS workbench DataOS Workbench is a SQL based query engine and allows data analysts to write complex SQL queries for managing, manipulating and analysing data in the database. Data analysts can use DataOS workbench to connect to multiple data sources and process SQL queries at rapid pace and scale. DataOS Rio(our inhouse data processing engine) handles these SQL queries to create tables or views. It is simple and easy to use. Want to query your data but not proficient at writing long SQL queries? It is no longer a problem with DataOS. Business users can use SQL studio which is a visual query builder and build even complex queries visually via drag-and-drop even without writing queries. Users need to just select which data you want to get, apply necessary functions and aggregations to them, configure sorting and filtering, and that\u2019s all. All of these can be done using an intuitively built user interface (UI), and users won\u2019t need to type even a single line of code. How cool ! Conclusion Nowadays, almost all data teams are overwhelmed with data requests. They spend sleepless nights trying to make disparate data sets talk to each other, building and maintaining data pipelines, solving data quality and accessibility issues all by themselves. DataOS is one platform that is designed to make the life of data teams easy. DataOS\u2019s intuitive and extensive features encourage users, irrespective of their technical and programming abilities, to experiment with their data. This reduces dependency of business teams on Data teams and improves time to insight drastically. Along with many inbuilt features, DataOS also comes with a swift data processing engine and powerful workbench which require a minimal learning curve. Over the past 12 months, we\u2019ve been working very closely with our beta customers to understand their data goals and showed them how DataOS can help them achieve data goals within 3-6 months. If you\u2019d like to experience your data in a whole new way, manage it and extract value out of it at a rapid pace, email us at hi@tmdc.io or visit us at themoderndatacompany.com Discovery Calls with Potential Customers PRIMARY GOAL Gather information related to the customer\u2019s data goals and data ecosystem so that we can come up with insights around their data needs and offer solutions. APPROACH Business Goals -> Data Goals -> Data Needs -> Solutions -> Capabilities enabled by DataOS -> Features in DataOS The Business Goals are the CEO-level goals. The Data Goals are the CIO-level goals which when accomplished contribute to their Business Goals. The Data Needs are the insights that we need to figure out which when solved will help the customer accomplish their Data Goals. For the customer to consider our solution to the data Needs as valuable, the solution to the needs to either Meet the Data Needs in a way that is both considered as valuable (faster, cheaper, comprehensive) by the customer and is better than the alternatives, OR Make new Data aspirations possible, OR BOTH Page Break QUESTIONS High-level Goals of the Customer What are some of your Business\u2019s goals that you are trying to contribute to? So that we can understand their Business Goals. Some goals we have seen in enterprises Be prepared for Volatility - New ways of doing business \u2013 to adapt to changing market conditions \u2013 user behavior, competition, new entrants. Enhanced User Experience - understand them better \u2013 to retain customers Automation \u2013 business processes automation - to reduce costs and improve accuracy Real Time or near-real time insights \u2013 to react to critical events on-time. Course Correction What are the technology/data goals that you are trying to achieve? So that we understand their Data Goals. Some goals we have seen in enterprises Infrastructure agility Real-time insights Observability \u2013 data pipes and security & governance Multi-cloud security Governance Insights democratization (Shifting power of data/insights from IT to Business users) Enable experimentation / fail fast Data freedom - ability to move data across org freely Inefficiencies with Current Situation What is the status quo way of achieving the Data goals? So that we understand their current data ecosystem and how we can position ourselves to be perceived as a better solution. What are the known shortcomings in the status quo? With few follow up \u201c5 Why\u201d questions. So that we can understand what they consider valuable. From a data ecosystem readiness point of view, how much time/effort/cost did/would it take: to achieve GDPR compliance? to integrate an acquired company/brand? to get a 360 view of your customers? to move data to an external system without getting into data privacy issues? to handle peak season data traffic? to resolve data quality issues? to support near real time use cases? to implement governance on Data lake? to solve data discoverability issues for all the data assets stored in different systems? to monitor data pipelines? to define and manage SLAs (quality, availability, performance, security, and cost-effectiveness)? to manage incidents related to data movement and data management and alert stakeholders? to create compute clusters on demand for varied data processing/querying needs? to version control data artifacts? to deploy, track and monitor ML models in production? to build consensus on definition and calculation of key business metrics? To evolve data partitioning strategy on the fly to improve query performances of data consumers? For each of the above questions, how would the responses be viewed as in your organization - ? Unacceptable Should be improved Acceptable Importance of the problem to the customer What alternatives did you consider? So that we know this is an important goal for them relative to their other goals. Did you try some of the alternative solutions? So that we know they\u2019ll put some money/time/effort in trying out our product. Why did the alternative solutions not satisfy your needs? So that we can understand what they consider valuable \u2013 faster, cheaper, comprehensive. How did you find out about these alternatives? So that we know how to reach our potential customers. Customer\u2019s willingness and ability to pay How do you see budget getting allocated to solving these problems \u2013 new spend or able to retire existing spend? So that we understand there will be able to pay for the solution as per our expectations. Follow Up Who else in your organization should we talk to so we better understand how we can solve your needs? So that we get more calls and better understanding of their needs. How does the approval process work and who are involved in the process? So that we know who else need to be involved in the Sales process. Who else in your organization should we talk to for follow up questions that came out of this conversation? So that we get clarifications on unanswered questions. Page Break PRODUCT \u2013 DataOS DataOS (Data Operating System) is a comprehensive data platform that abstracts out the entire complexities of data engineering by productizing it so that as a business, you can focus on driving value from data instead of managing data. Status quo at several companies is that they are spending lot of time and money on data wrangling than on how to use the data. DataOS provides entire data management in one product \u2013 everything from data ingestion, data governance, data quality, operationalizing data, data discoverability. Current alternatives to get the capabilities provided by DataOS are very suboptimal. The most common available alternative is: Custom build data ecosystem to power the data needs from ground-up by going to an SI (Accenture, Deloitte, etc.), taking a reference architecture, picking multiple vendors of point solutions for ingestion, catalog, quality, governance, processing. These tools are built on different and old stacks that limits their capabilities and integration with other tools. Sum of all these parts results in a very rigid architecture. Inefficiencies experienced Integrate Snowflake on top of existinf infrastructure would take a year. Moving away from Snowflake to Redshift would take 2 or 3 quarters. Data movement from Teradata on-prem to Teradata Cloud takes 2 years. Spend millions of dollars in license fees for Snowflake, PowerBI, Alteryx, etc. and not able to take advantage of the functionalities of those products. Creating each BI report/dashboard takes X weeks. The main gap in the market is that the customers of the companies are expecting experiences that can be provided only with an agile and modern data ecosystem but companies do not have a way to achieve it with the status quo and available alternatives. The rigidity of the data ecosystem and infrastructure that hinders the companies to adapt to dynamic business environment is the key problem that is solved by DataOS. DataOS gives enterprises an agile data infrastructure and future-proofs their infrastructure for next 5-10 years. DataOS CapabilitiesFeatures FeaturesCapabilities Data Management Data interface to access data that is residing in different systems in the organization \u2013 relational, non-relational, streaming, and object data stores. (Depots) Simplification of Data engineering effort by providing re-usable and version-controlled constructs. (Declarative Artifacts) Flexible data warehousing capability by providing ACID-compliant Data Lake based on Open Data formats. (Icebase) Data Processing Ability to process (transform, enrich) data with different volume, velocity needs \u2013 stream, batch, stateful & stateless computation. Orchestration engine to deploy, monitor, schedule, or trigger sophisticated data pipelines. (Workflows) Data Exploration Holistic view of all the data in the organization, their relationships and their usage. (Datanet) Ability to query data from across multiple databases using single SQL interface. (Workbench) Ability for users with no/minimal SQL expertise to access datasets through GUI. (Workbench - Studio) Ability to create on-demand compute clusters for recurring and critical queries for applications such as Dashboards. Data Activation Sending data to external systems (such as Salesforce) to achieve reverse-ETL. (Syndication) Sharing of datasets to enable secure access from external applications using UDLs (Universal Data Link). Ability to create data-powered applications end-to-end including GUI, processing, storage, dashboard. Ability to create REST APIs on demand (conceptually similar to GraphQL), and a runtime for fulfilling request on these APIs on the datasets stored in DataOS. Ability to define and track business KPIs in a consistent manner through MML paradigm; common vocabulary for entities and relationships (Model), map them to data sources (Map) and get data (Load). Data Governance Ability to govern and secure data at highly granular level through ABAC (Attribute-based access control); such as row-level redaction and column-level masking. Visibility for InfoSec team to sensitive data (such as PII) being accessed across the organization to enable data privacy compliance. APPENDIX Tips from the book: THE MOM TEST Tips: Talk about their life instead of your idea. Ask about specifics in the past instead of generics or opinions about the future. Talk less and listen more. Rules of Thumb when framing questions: Opinions are worthless. Anything involving the future is an over-optimistic lie. People will lie to you if they think it\u2019s what you want to hear. You're shooting blind until you understand their goals. Watching someone do a task will show you where the problems and inefficiencies really are, not where the customer thinks they are. If they haven't looked for ways of solving it already, they're not going to look for (or buy) yours. People stop lying when you ask them for money. While it\u2019s rare for someone to tell you precisely what they\u2019ll pay you, they\u2019ll often show you what it\u2019s worth to them. In a B2B context, know where the money comes from Whose budget the purchase will come from Who else within their company holds the power to torpedo the deal. People want to help you. Give them an excuse to do so. \"Who else should I talk to?\" \"Is there anything else I should have asked?\" Hero Section The Modern DataOS Platform Option A: The connective tissue for ALL your data infrastructure Option B: A fully-integrated platform that allows you to programmatically manage and operationalize all your legacy and cloud data. Option C: The connective tissue for your data, ML and analytics infrastructure Option D: An operating system to power your entire data lifecycle. Option E : DataOS is the operating system for a new generation of data,ML and analytics infrastructure. DataOS is the operating system powering a new generation of data and analytics infrastructure. It is the connective tissue between your legacy and cloud data sources delivering quality, governed, secure and reliable data in real-time. Democratize and simplify access to data and insights for technical and business users in your team with DataOS. DataOS policies allow organizations to authorize users, employees and third parties to access company data in a manner that meets security, privacy and compliance requirements. Access Policy Access policy is a security measure which is put in place to regulate the individuals that can view, use, or have access to a restricted DataOS environment/resource.A This is implemented by tags. For example a user with a tag 'operator' can access secrets or specific Depots to connect to data. Data Policies These policies are global policies available in the system which contains all the column level policy. Using these policies, you can: Mask the data or Filter the data. Masking and filtering strategies can be defined in the YAML files. For example, PII data can be shown with appropriate mask, replacing with \"####\" string or with some hash function. You can filter the data based on tags such as some users can not see ' Florida' region data. The following examples show what the masked data might look like after the masking policy is applied. Type Original Value Masked Value Email ID abc.ef@gmail.com - bkfgohrnrtseqq85@bkgiplpsrhsll16.org SSN 987654321 867-92-3415 Credit card number 8671 9211 3415 4546 #### #### #### #### A data catalog is an organized inventory of data assets in the organization. It uses metadata to help organizations manage their data. This metadata helps in data discovery and governance. CTA: Schedule a Demo < Infographic here> Business Value from DataOS Benefit 1: Secure Data Sharing Benefit 2: Data fit for purpose Benefit 3: Data Democratization Benefit 4: Trusted and Reliable Data Benefit 5: Built in governance and security Benefit 6: Automation and Speed DataOS Capabilities One Platform for all your data activities Data Management The DataOS seamlessly connects to all your source and sink systems and transform and curate data with our model-based low-code declarative artifacts. ACID-compliant data lake enables teams to store structured, unstructured and streaming data. Out-of-the-box data quality and profiling functionality lets you get quality and trusted data without additional processing. Leverage our observability principles to create robust alerts across the data lifecycle. Data Exploration - DataOS\u2019s Datanet creates a connective tissue between all your data systems continuously adding usage-based intelligence to your data. Easily Discover, search and find trusted and context-aware data. Understand the journey, relationships and impact of your datasets and make decisions with confidence. DataOS also has powerful workbench that lets you query datasets with simple SQL queries. Data Activation Activate your data and share it across your enterprise. DataOS lets you create customized reports on any datasets in minutes, share and collaborate on them securely. Build trigger alerts and notifications to 10+ destinations including ServiceNow, Jira, Pager Duty, Slack, MS teams, Pager Duty etc. Consume your shared data from SaaS systems like salesforce, zendesk, google ads etc Data Products Build and launch new data products faster. DataOS handles all the infrastructure needed - Run,host,store,process,sync,serve to build your data application so teams can focus on innvoation. Governance & Security DataOS\u2019s Tag-based governance enables flexible and granular policy creation that gives teams an automated and scalable way to ensure governance and compliance for data within your ecosystem. Attribute-based access control future-proofs your org and lets it adapts to changing and new compliance regulations. Easy-to-create access and Data policies let you control, track and audit access to data. Row-level filtering and column masking give you granular control of what data authenticated users can access. Advanced primitives like data masking, data abstraction and differential privacy ensure your teams are working with trusted data at all times. Data Sharing Share data easily across your business ecosystem without copying or moving data. DataOS\u2019s best in class governance enables you to control access in a secured and auditable environment. Share data and collaborate with customers, internal and external partners to unlock newer and richer insights. Data Modeling DataOS\u2019s foundational MML(Map-ModelModel-Map-Load) architecture brings a declarative model-first approach to pipeline creation. It abstracts away the complexity of pipeline building. MML makes it easy to build and manage data pipelines helping data engineering teams to streamline and simplify their ETL processes. Principles of DataOS Model First Engg Abstract away the complexity of data engineering with our model-based approach to building pipelines. Give the control back to business decision makers to get to their insights \u2013 faster and easily. DataOps-friendly Cloud native and agnostic and agile-friendly adopting the principles of DevOps and DataOps. Observability Data observability, with visibility into any anomalies encountered in permissions, roles, policies, or movement of data Use cases Data Engineering - Automate and simplify Data Engineering tasks thru MML (Map-Model-Load). With our declarative model-first pipeline creation, engineering teams can streamline their ETL processes leaving them free to focus on value-add features. Data Governance - Deliver governance and security for all your data from a single, centralized platform. Future-proof your enterprise for governance and compliance requirements with Attribute-based access control. Easy setup of policies to control, observe and audit who and what data they are accessing. Build confidence in your insights by delivering trusted data to your teams. Data Discovery - Discover data, understand the lineage, relationships and impact of your data. Remove barriers to data access for your business decision makers. Empower you technical and business teams with quality and contextual data Data sharing & collaboration \u2013 Collaborate and share data with your internal systems and stakeholders and your external partner ecosystem and customers in a secure, governed and auditable fashion. Share datasets in real time without creating copies or moving it. Unearth richer insights and unlock powerful new use cases by sharing your data within your enterprise and your external partner ecosystem and customers. Data Observability Complete data observability into all your data activities from production to consumption of data, to access and sharing of data including events, metrics and logs","title":"Infolake"},{"location":"flare/installation/","text":"Flare Standalone Installation \u00b6 Requirements \u00b6 Before proceeding, you will need the following: curl utility installed. Access permissions for the BitBucket repository to download the .zip file. Docker download and set up \u00b6 Run the following command from terminal: curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh The above command is to download docker for Linux . For Windows and Mac, visit the following link to download docker desktop. Docker desktop download Log into your docker using the following command and enter the login ID and password. docker login Note : You may need to work with your IT team for this step. Run standalone \u00b6 Download the Flare Standalone zip file. Extract the zip file. This file contains the following: dataos-resource folder with config.yaml datadir containing data files dataout folder for the ouput README.md containing docker command Navigate to the standalone directory, run the following command (given in the the README.md file) to initiate the spawn process: docker run --rm -it \\ -v $PWD/dataos-resource:/etc/standalone \\ -v $PWD:/datadir \\ -v $PWD/dataout:/dataout \\ -e DATAOS_WORKSPACE=public \\ -e DATAOS_RUN_AS_USER=tmdc \\ rubiklabs/flare:5.6.32-dev start Note : Ensure that Docker commnad refers to the latest image. While running it for the first time, it will download all required files, so it may take a few minutes. You should see the following to indicate a successful start: Flare session is available as flare. Welcome to ______ _ | ____| | | | |__ | | __ _ _ __ ___ | __| | | / _` | | '__| / _ \\ | | | | | (_| | | | | __/ |_| |_| \\__,_| |_| \\___| version 1.1.0 Powered by Apache Spark 3.0.1 Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_262) Type in expressions to have them evaluated. Type :help for more information. scala> You can now run various Flare function commands at the prompt. For instance 'tables' will give you the list of all tables in the current directory. You should see the 'enriched_product_order_retailer' listed which is created after the given YAML is successfully run. Flare session is available as flare. Welcome to ______ _ | ____| | | | |__ | | __ _ _ __ ___ | __| | | / _` | | '__| / _ \\ | | | | | (_| | | | | __/ |_| |_| \\__,_| |_| \\___| version 1.1.0 Powered by Apache Spark 3.0.1 Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_262) Type in expressions to have them evaluated. Type :help for more information. scala> tables +--------+---------------------------------+-----------+ |database|tableName |isTemporary| +--------+---------------------------------+-----------+ | |enriched_product_order_retailer |true | | |order_line_item_with_product_info|true | | |product_info |true | | |retail_order_line_item |true | | |retailer_info |true | +--------+---------------------------------+-----------+ You can also run a Spark sql query to verify the data, as follows: scala> spark.sql(\"select product_name, product_id from enriched_product_order_retailer\").show(4) +------------+----------+ |product_name|product_id| +------------+----------+ | Colazal| P1098| | Oxycodone| P1705| | Belviq| P1855| | Kenalog-40| P1079| +------------+----------+ only showing top 4 rows You have now successfully installed Flare standalone and verified the same by ingesting sample data. Sample YAML \u00b6 The following yaml file (config.yaml) is given in the dataos-resource folder to run a sample job in standalone. This job joins product, retailer and order datasets and creates the new enriched dataset. Note : Make the changes to this YAML as per your requirement. version : v1beta1 name : sample-job-standalone type : workflow tags : - getting-data - product - retailer - orders description : This job takes input of product, retailer and order data and joins them all and saves as new dataset. owner : rubik-ai workflow : dag : - name : retail-data-enrichment title : retailer orders enriched data description : This job takes input of product retailer and order data and joins all spec : tags : - Transformation stack : flare:1.0 tier : connect flare : driver : coreLimit : 1200m cores : 1 memory : 1024m executor : coreLimit : 1200m cores : 1 instances : 1 memory : 1024m job : explain : true inputs : - name : product_info dataset : /datadir/product_info format : json - name : retailer_info dataset : /datadir/retailer_info format : json - name : retail_order_line_item dataset : /datadir/retail_order_line_item format : json logLevel : ERROR outputs : - name : output01 depot : depot : /dataout/enriched_retailer steps : - sink : - sequenceName : enriched_product_order_retailer datasetName : enriched_retailer_01 outputName : output01 outputType : Iceberg description : retailer enriched data outputOptions : saveMode : append tags : - retailer - product - enriched-data title : Retailer Enriched Data sequence : - name : order_line_item_with_product_info # Joinin product info with order sql : select * from retail_order_line_item order_i left join product_info product_i on order_i.product_ids = product_i.product_id - name : enriched_product_order_retailer # enriched data sql : select * from order_line_item_with_product_info opi left join retailer_info retail_i on opi.retailer_id = retail_i.retailer_id","title":"Standalone Installation"},{"location":"flare/installation/#flare-standalone-installation","text":"","title":"Flare Standalone Installation"},{"location":"flare/installation/#requirements","text":"Before proceeding, you will need the following: curl utility installed. Access permissions for the BitBucket repository to download the .zip file.","title":"Requirements"},{"location":"flare/installation/#docker-download-and-set-up","text":"Run the following command from terminal: curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh The above command is to download docker for Linux . For Windows and Mac, visit the following link to download docker desktop. Docker desktop download Log into your docker using the following command and enter the login ID and password. docker login Note : You may need to work with your IT team for this step.","title":"Docker download and set up"},{"location":"flare/installation/#run-standalone","text":"Download the Flare Standalone zip file. Extract the zip file. This file contains the following: dataos-resource folder with config.yaml datadir containing data files dataout folder for the ouput README.md containing docker command Navigate to the standalone directory, run the following command (given in the the README.md file) to initiate the spawn process: docker run --rm -it \\ -v $PWD/dataos-resource:/etc/standalone \\ -v $PWD:/datadir \\ -v $PWD/dataout:/dataout \\ -e DATAOS_WORKSPACE=public \\ -e DATAOS_RUN_AS_USER=tmdc \\ rubiklabs/flare:5.6.32-dev start Note : Ensure that Docker commnad refers to the latest image. While running it for the first time, it will download all required files, so it may take a few minutes. You should see the following to indicate a successful start: Flare session is available as flare. Welcome to ______ _ | ____| | | | |__ | | __ _ _ __ ___ | __| | | / _` | | '__| / _ \\ | | | | | (_| | | | | __/ |_| |_| \\__,_| |_| \\___| version 1.1.0 Powered by Apache Spark 3.0.1 Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_262) Type in expressions to have them evaluated. Type :help for more information. scala> You can now run various Flare function commands at the prompt. For instance 'tables' will give you the list of all tables in the current directory. You should see the 'enriched_product_order_retailer' listed which is created after the given YAML is successfully run. Flare session is available as flare. Welcome to ______ _ | ____| | | | |__ | | __ _ _ __ ___ | __| | | / _` | | '__| / _ \\ | | | | | (_| | | | | __/ |_| |_| \\__,_| |_| \\___| version 1.1.0 Powered by Apache Spark 3.0.1 Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_262) Type in expressions to have them evaluated. Type :help for more information. scala> tables +--------+---------------------------------+-----------+ |database|tableName |isTemporary| +--------+---------------------------------+-----------+ | |enriched_product_order_retailer |true | | |order_line_item_with_product_info|true | | |product_info |true | | |retail_order_line_item |true | | |retailer_info |true | +--------+---------------------------------+-----------+ You can also run a Spark sql query to verify the data, as follows: scala> spark.sql(\"select product_name, product_id from enriched_product_order_retailer\").show(4) +------------+----------+ |product_name|product_id| +------------+----------+ | Colazal| P1098| | Oxycodone| P1705| | Belviq| P1855| | Kenalog-40| P1079| +------------+----------+ only showing top 4 rows You have now successfully installed Flare standalone and verified the same by ingesting sample data.","title":"Run standalone"},{"location":"flare/installation/#sample-yaml","text":"The following yaml file (config.yaml) is given in the dataos-resource folder to run a sample job in standalone. This job joins product, retailer and order datasets and creates the new enriched dataset. Note : Make the changes to this YAML as per your requirement. version : v1beta1 name : sample-job-standalone type : workflow tags : - getting-data - product - retailer - orders description : This job takes input of product, retailer and order data and joins them all and saves as new dataset. owner : rubik-ai workflow : dag : - name : retail-data-enrichment title : retailer orders enriched data description : This job takes input of product retailer and order data and joins all spec : tags : - Transformation stack : flare:1.0 tier : connect flare : driver : coreLimit : 1200m cores : 1 memory : 1024m executor : coreLimit : 1200m cores : 1 instances : 1 memory : 1024m job : explain : true inputs : - name : product_info dataset : /datadir/product_info format : json - name : retailer_info dataset : /datadir/retailer_info format : json - name : retail_order_line_item dataset : /datadir/retail_order_line_item format : json logLevel : ERROR outputs : - name : output01 depot : depot : /dataout/enriched_retailer steps : - sink : - sequenceName : enriched_product_order_retailer datasetName : enriched_retailer_01 outputName : output01 outputType : Iceberg description : retailer enriched data outputOptions : saveMode : append tags : - retailer - product - enriched-data title : Retailer Enriched Data sequence : - name : order_line_item_with_product_info # Joinin product info with order sql : select * from retail_order_line_item order_i left join product_info product_i on order_i.product_ids = product_i.product_id - name : enriched_product_order_retailer # enriched data sql : select * from order_line_item_with_product_info opi left join retailer_info retail_i on opi.retailer_id = retail_i.retailer_id","title":"Sample YAML"},{"location":"flare/mml/","text":"What is MML Why adopt MML? Challenges that MML can solve Key considerations for MML What all scenarios where you consider using MML Best Use case How MML works? Components What you require to set up MML How to set up MML Stand alone related. Canarying is a process whereby you partially deploy your service (in this case, the pipeline application) and monitor the results. For a more detailed discussion of canarying, see Canarying Releases. Canarying is tied to the entire pipeline rather than a single process. During a canary phase, you may choose to process the same real production data as the live pipeline but skip writes to production storage; techniques such as two-phase mutation can help (see Idempotent and Two-Phase Mutations). Often, you\u2019ll have to wait for the complete cycle of processing to finish before you can discover any customer-impacting issues. After your dry run (or two-phase mutation), compare the results of your canary pipeline with your live pipeline to confirm their health and check for data differences. A semantic layer is a business representation of corporate data that helps end users access data autonomously using common business terms. A semantic layer maps complex data into familiar business terms such as product, customer, or revenue to offer a unified, consolidated view of data across the organization. I\u2019ve seen implementations try to deliver BI without a semantic layer. They end up building lots of indexes, views, and aggregate tables, just to get their solution to perform. They also write complex SQL to emulate calculated measures and hierarchies. These issues are solved, out of the box, with a good semantic layer. My advice is to definitely include a semantic layer in your solution, even if its unfamiliar to you. The biggest downside of a Semantic Layer is you have to build, maintain and manage it. The layer must be kept in sync with any database changes that occur. Within a cloud environment, compute nodes form a core of resources. They supply the processing, memory, network, and storage that virtual machine instances need. When an instance is created, it is matched to a compute node with the available resources.","title":"Mml"},{"location":"flare/partitionevolution/","text":"Partition Evolution \u00b6 Overview \u00b6 This article describes a use case of partition evolution for Iceberg and shows how seamless it is to query data stored with multiple partition layouts. You can update the Iceberg table partitioning in an existing table because queries do not reference partition values directly. Iceberg makes partitioning simple by implementing hidden partitioning. You need not supply a separate partition filter at query time. The details of partitioning and querying are handled by Iceberg automatically. That is why even when you change the partition spec, your queries do not break. For example, a simple query like this is capable of fetching data from multiple partition spec: Select * from NY-Taxi where date_col > 2010-10-23 AND date_col < 2013-01-01 You do not need to understand the physical table layout to get accurate query results. Iceberg keeps track of the relationship between a column value and its partition. Solution approach \u00b6 When you evolve a partition spec, the old data written with an earlier partition key remains unchanged, and its metadata remains unaffected. New data is written using the new partition key in a new layout. Metadata for each of the partition versions is kept separately. When you query, each partition layout\u2019s respective metadata is used to identify the files it needs to access; this is called split-planning. Implementation details \u00b6 The NY Taxi data is ingested and is partitioned by year. When the new data is appended, the table is updated so that the data is partitioned by day. Both partitioning layouts can coexist in the same table. Iceberg uses hidden partitioning, so you don\u2019t need to write queries for a specific partition layout. Instead, you can write queries that select the data you need, and Iceberg automatically scans files containing matching data referring to partition layouts. Partition evolution is a metadata operation and does not rewrite files. The following steps demonstrate the partition evolution use case. Ingest data with initial partition \u00b6 Run the following Flare job that ingests data into DataOS with the partition on year. --- version : v1beta1 name : workflow-ny-taxi type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data and write with partitioning on year workflow : title : Connect NY Taxi dag : - name : nytaxi-ingest-partition-update title : NY-taxi data ingester description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data/010100.json?acl=r format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_partition_01 outputName : output01 outputType : Iceberg outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : year column : pickup_datetime name : year tags : - NY-Taxi - Connect title : NY-Taxi Data sequence : - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi functions : - name : set_type columns : pickup_datetime : timestamp dropoff_datetime : timestamp Update metadata with data tool \u00b6 Run the following datatool job to update the meta data in DataOS. version : v1beta1 name : dataos-tool-ny-taxi-connect type : workflow workflow : dag : - name : dataos-tool-partition spec : stack : toolbox toolbox : dataset : dataos://icebase:raw01/ny_taxi_partition_01 action : name : set_version value : latest Update partition for new data \u00b6 Run the following datatool job to update the partition spec for the new data to be ingested. version : v1beta1 name : dataos-tool-update-partition type : workflow workflow : dag : - name : dataos-tool-partition-update spec : stack : alpha envs : LOG_LEVEL : debug alpha : image : rubiklabs/dataos-tool:0.2.3 arguments : - dataset - update-partition - --address - dataos://icebase:raw01/ny_taxi_partition_01 - --spec - day:pickup_datetime Append data with new partition spec \u00b6 Run the following Flare job that appends data into DataOS with the updated partition on Day. --- version : v1beta1 name : workflow-ny-taxi-updatepartition type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data and write with updated partitioning on Day workflow : title : Connect NY Taxi dag : - name : nytaxi-ingest-partition-update title : NY-taxi data ingester description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data/010100.json?acl=r format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_01 outputName : output01 outputType : Iceberg outputOptions : saveMode : append iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : day column : pickup_datetime name : day tags : - NY-Taxi - Connect title : NY-Taxi Data sequence : - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi functions : - name : set_type columns : pickup_datetime : timestamp dropoff_datetime : timestamp","title":"Partition Evolution"},{"location":"flare/partitionevolution/#partition-evolution","text":"","title":"Partition Evolution"},{"location":"flare/partitionevolution/#overview","text":"This article describes a use case of partition evolution for Iceberg and shows how seamless it is to query data stored with multiple partition layouts. You can update the Iceberg table partitioning in an existing table because queries do not reference partition values directly. Iceberg makes partitioning simple by implementing hidden partitioning. You need not supply a separate partition filter at query time. The details of partitioning and querying are handled by Iceberg automatically. That is why even when you change the partition spec, your queries do not break. For example, a simple query like this is capable of fetching data from multiple partition spec: Select * from NY-Taxi where date_col > 2010-10-23 AND date_col < 2013-01-01 You do not need to understand the physical table layout to get accurate query results. Iceberg keeps track of the relationship between a column value and its partition.","title":"Overview"},{"location":"flare/partitionevolution/#solution-approach","text":"When you evolve a partition spec, the old data written with an earlier partition key remains unchanged, and its metadata remains unaffected. New data is written using the new partition key in a new layout. Metadata for each of the partition versions is kept separately. When you query, each partition layout\u2019s respective metadata is used to identify the files it needs to access; this is called split-planning.","title":"Solution approach"},{"location":"flare/partitionevolution/#implementation-details","text":"The NY Taxi data is ingested and is partitioned by year. When the new data is appended, the table is updated so that the data is partitioned by day. Both partitioning layouts can coexist in the same table. Iceberg uses hidden partitioning, so you don\u2019t need to write queries for a specific partition layout. Instead, you can write queries that select the data you need, and Iceberg automatically scans files containing matching data referring to partition layouts. Partition evolution is a metadata operation and does not rewrite files. The following steps demonstrate the partition evolution use case.","title":"Implementation details"},{"location":"flare/partitionevolution/#ingest-data-with-initial-partition","text":"Run the following Flare job that ingests data into DataOS with the partition on year. --- version : v1beta1 name : workflow-ny-taxi type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data and write with partitioning on year workflow : title : Connect NY Taxi dag : - name : nytaxi-ingest-partition-update title : NY-taxi data ingester description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data/010100.json?acl=r format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_partition_01 outputName : output01 outputType : Iceberg outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : year column : pickup_datetime name : year tags : - NY-Taxi - Connect title : NY-Taxi Data sequence : - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi functions : - name : set_type columns : pickup_datetime : timestamp dropoff_datetime : timestamp","title":"Ingest data with initial partition"},{"location":"flare/partitionevolution/#update-metadata-with-data-tool","text":"Run the following datatool job to update the meta data in DataOS. version : v1beta1 name : dataos-tool-ny-taxi-connect type : workflow workflow : dag : - name : dataos-tool-partition spec : stack : toolbox toolbox : dataset : dataos://icebase:raw01/ny_taxi_partition_01 action : name : set_version value : latest","title":"Update metadata with data tool"},{"location":"flare/partitionevolution/#update-partition-for-new-data","text":"Run the following datatool job to update the partition spec for the new data to be ingested. version : v1beta1 name : dataos-tool-update-partition type : workflow workflow : dag : - name : dataos-tool-partition-update spec : stack : alpha envs : LOG_LEVEL : debug alpha : image : rubiklabs/dataos-tool:0.2.3 arguments : - dataset - update-partition - --address - dataos://icebase:raw01/ny_taxi_partition_01 - --spec - day:pickup_datetime","title":"Update partition for new data"},{"location":"flare/partitionevolution/#append-data-with-new-partition-spec","text":"Run the following Flare job that appends data into DataOS with the updated partition on Day. --- version : v1beta1 name : workflow-ny-taxi-updatepartition type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data and write with updated partitioning on Day workflow : title : Connect NY Taxi dag : - name : nytaxi-ingest-partition-update title : NY-taxi data ingester description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data/010100.json?acl=r format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_01 outputName : output01 outputType : Iceberg outputOptions : saveMode : append iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : day column : pickup_datetime name : day tags : - NY-Taxi - Connect title : NY-Taxi Data sequence : - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi functions : - name : set_type columns : pickup_datetime : timestamp dropoff_datetime : timestamp","title":"Append data with new partition spec"},{"location":"flare/partitioning/","text":"Partitioning \u00b6 Overview \u00b6 Partitioning is a way to make queries faster by grouping similar rows together when writing. This use case checks how partitioning works for Flare while writing data to Iceberg and non-Iceberg type data storage (Parquet, Kafka) to improve query processing performance. This use case describes data partitioning with partition keys of different data types for Iceberg and Parquet formats. Iceberg can partition timestamps by year, month, day, and hour granularity. It can also use a categorical column of data type- string, integer, long, double(to be checked) , like vendor id in this example, to store rows together and speed up queries. Iceberg produces partition values by taking a column value and optionally transforming it. Iceberg converts timestamp values into a date, and extracts year, month, day, hour, etc. While in non-Iceberg data sink, partitions are explicit and appear as a separate column in the table that must be supplied in every table write. For example, you need to provide the columns explicitly for the year, month, hour as transformation from timestamp column is not automatic. You need to provide catagorical column as a partition criterion for non-Iceberg format such as Parquet. Implementation details \u00b6 The following examples demonstrate the use of various partitioning modes. Iceberg \u00b6 Partition using type identity Integer values String values Partition using type timestamp Year Month Day Hour Example 1: Partitioning is done on identity by taking the vendor_id column. You don't need to give name property if partition field type is identity type. partitionSpec : - type : identity # options tested: identity, year, month, day, hour column : vendor_id # columns used - identity (vendor_id, one string column) & for rest date_col Example 2: Partitioning is done on the year. partitionSpec : - type : year # options tested: identity, year, month, day, hour column : date_col # columns used - identity (vendor_id, one string column) & for rest date_col of type timestamp name : year Example 3: Nested partitioning is done on (identity, year). Here, the vendor_id used for identity should come at the first level. partitionSpec : - type : identity column : vendor_id # columns used - identity (vendor_id, one string column) & for rest date_col - type : year # options tested: identity, year, month, day, hour column : date_col name : year Parquet \u00b6 Partition using type identity Example 1: Partitioning is done on identity by taking the vendor_id column. - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_parquet_06 outputName : output01 outputType : Parquet outputOptions : saveMode : overwrite partitionBy : - vendor_id Outcomes \u00b6 The files will be stored in the folders based on the partition criterion defined, and you can view them in workbench or storage locations. Code files \u00b6 Note : When specifying the partition criterion for Iceberg, the 'partitionSpec' property should be defined as a child property under 'iceberg' in the sink section. Otherwise partioning will not be performed as desired. version : v1beta1 name : workflow-ny-taxi-partitioned-vendor type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and write with partitioning on vendor_id workflow : title : Connect NY Taxi dag : - name : nytaxi title : NY-taxi data ingester description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data?acl=r format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_07 outputName : output01 outputType : Iceberg outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity # options tested: string, integer, long column : vendor_id # columns used - identity (vendor_id, one string column) - type : year # options tested: identity, year, month, day, hour column : date_col # columns used - identity (vendor_id, one string column) & for rest date_col name : year tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned Vendor sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat","title":"Partitioning"},{"location":"flare/partitioning/#partitioning","text":"","title":"Partitioning"},{"location":"flare/partitioning/#overview","text":"Partitioning is a way to make queries faster by grouping similar rows together when writing. This use case checks how partitioning works for Flare while writing data to Iceberg and non-Iceberg type data storage (Parquet, Kafka) to improve query processing performance. This use case describes data partitioning with partition keys of different data types for Iceberg and Parquet formats. Iceberg can partition timestamps by year, month, day, and hour granularity. It can also use a categorical column of data type- string, integer, long, double(to be checked) , like vendor id in this example, to store rows together and speed up queries. Iceberg produces partition values by taking a column value and optionally transforming it. Iceberg converts timestamp values into a date, and extracts year, month, day, hour, etc. While in non-Iceberg data sink, partitions are explicit and appear as a separate column in the table that must be supplied in every table write. For example, you need to provide the columns explicitly for the year, month, hour as transformation from timestamp column is not automatic. You need to provide catagorical column as a partition criterion for non-Iceberg format such as Parquet.","title":"Overview"},{"location":"flare/partitioning/#implementation-details","text":"The following examples demonstrate the use of various partitioning modes.","title":"Implementation details"},{"location":"flare/partitioning/#iceberg","text":"Partition using type identity Integer values String values Partition using type timestamp Year Month Day Hour Example 1: Partitioning is done on identity by taking the vendor_id column. You don't need to give name property if partition field type is identity type. partitionSpec : - type : identity # options tested: identity, year, month, day, hour column : vendor_id # columns used - identity (vendor_id, one string column) & for rest date_col Example 2: Partitioning is done on the year. partitionSpec : - type : year # options tested: identity, year, month, day, hour column : date_col # columns used - identity (vendor_id, one string column) & for rest date_col of type timestamp name : year Example 3: Nested partitioning is done on (identity, year). Here, the vendor_id used for identity should come at the first level. partitionSpec : - type : identity column : vendor_id # columns used - identity (vendor_id, one string column) & for rest date_col - type : year # options tested: identity, year, month, day, hour column : date_col name : year","title":"Iceberg"},{"location":"flare/partitioning/#parquet","text":"Partition using type identity Example 1: Partitioning is done on identity by taking the vendor_id column. - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_parquet_06 outputName : output01 outputType : Parquet outputOptions : saveMode : overwrite partitionBy : - vendor_id","title":"Parquet"},{"location":"flare/partitioning/#outcomes","text":"The files will be stored in the folders based on the partition criterion defined, and you can view them in workbench or storage locations.","title":"Outcomes"},{"location":"flare/partitioning/#code-files","text":"Note : When specifying the partition criterion for Iceberg, the 'partitionSpec' property should be defined as a child property under 'iceberg' in the sink section. Otherwise partioning will not be performed as desired. version : v1beta1 name : workflow-ny-taxi-partitioned-vendor type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and write with partitioning on vendor_id workflow : title : Connect NY Taxi dag : - name : nytaxi title : NY-taxi data ingester description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data?acl=r format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01?acl=rw steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_07 outputName : output01 outputType : Iceberg outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity # options tested: string, integer, long column : vendor_id # columns used - identity (vendor_id, one string column) - type : year # options tested: identity, year, month, day, hour column : date_col # columns used - identity (vendor_id, one string column) & for rest date_col name : year tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned Vendor sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat","title":"Code files"},{"location":"flare/partitioningpractices/","text":"Partitioning Practices \u00b6 Overview \u00b6 Data partitioning is a very common technique by that data stored in a persistent store is segregated into sections for faster queries and easy management. Partitioning your data helps reduce query costs and improve performance by limiting the amount of data query engines such as Minerva, need to scan in order to return the results for a specific query. Since it limits the volume of data scanned, dramatically accelerating queries. Partitioning is a way to make queries faster by grouping similar rows together when writing. This article will cover the data partitioning practices you need to know in order to optimize your analytics infrastructure for performance and to improve the storage of the dataset within DataOS. Data access operations on each partition take place over a smaller volume of data. Provided that the data is partitioned in a suitable way, this is much more efficient. Operations that affect more than one partition can execute in parallel. Data is commonly partitioned by timestamp \u2013 which could mean by hour, by minute or by day \u2013 and the size of the partition should depend on the type of query we intend to run. If most of our queries require data from the last 6 hours, we might want to use hourly partitioning rather than daily in order to scan less data. We will see how to achieve partitioning with some of the existing technologies for large-scale data processing: Designing partitions \u00b6 Data can be partitioned in different ways: horizontally, vertically, or functionally. The strategy you choose depends on the reason for partitioning the data, and the requirements of the applications and services that will access the data. For an enterprise wide Data Lake, data partitioning is an important aspect that needs serious thought and consideration. The three typical strategies for partitioning data are: Horizontal partitioning \u00b6 In this strategy, each partition holds a specific subset of the data, such as all the orders for a specific set of customers in an ecommerce application. In this example, product inventory data is divided into pertitions based on the product key. Each partition holds the data for a contiguous range of partition keys (A-G and H-Z), organized alphabetically. Vertical partitioning \u00b6 In this strategy each partition holds a subset of the fields for items in the data store. The fields are divided according to their pattern of use, such as placing the frequently accessed fields in one vertical partition and the less frequently accessed fields in another. The most common use for vertical partitioning is to reduce the I/O and performance costs associated with fetching the items that are accessed most frequently. Figure 2 shows an overview of an example of vertical partitioning, where different properties for each data item are held in different partitions; the name, description, and price information for products are accessed more frequently than the volume in stock or the last ordered date. Name | Description | Price -------- | -------- | -------- | Volume in Stock | Last ordered Date -------- | -------- | In this example, the application regularly queries the product name, description, and price together when displaying the details of products to customers. The stock level and date when the product was last ordered from the manufacturer are held in a separate partition because these two items are commonly used together. This partitioning scheme has the added advantage that the relatively slow-moving data (product name, description, and price) is separated from the more dynamic data (stock level and last ordered date). Using this partitioning strategy, you can store sensitive (for example credit card )and other data on separate partition to maximize the security of sensitive data. Functional partitioning \u00b6 In this strategy data is aggregated according to distinct business area or service in the application. For example, an ecommerce system that implements separate business functions for invoicing and managing product inventory might store invoice data in one partition and product inventory data in another. It\u2019s important to note that the three strategies described here can be combined. They are not mutually exclusive and you should consider them all when you design a partitioning scheme. For example, you might divide data into shards and then use vertical partitioning to further subdivide the data in each shard. Similarly, the data in a functional partition may be split into shards (which may also be vertically partitioned). You must evaluate and balance when designing a partitioning scheme that meets the overall data processing performance targets for your system. Good practices \u00b6 The most important factor when implementing this partitioning strategy is the choice of partitioning key. It can be difficult to change the key after the system is in operation. The key must ensure that data is partitioned so that the workload is as even as possible across the partitions. The partition key you choose should minimize any future requirements to split large partition into smaller pieces, coalesce small partitions into larger partitions, or change the schema that describes the data stored in a set of partitions. These operations can be very time consuming. Designing partitions for scalability \u00b6 It is vital to consider size and workload for each partition and balance them so that data is distributed to achieve maximum scalability. However, you must also partition the data so that it does not exceed the scaling limits of a single partition store. Follow these steps when designing the partitions for scalability: Analyze the application to understand the data access patterns, such as size of the result set returned by each query, the frequency of access, the inherent latency, and the server-side compute processing requirements. In many cases, a few major entities will demand most of the processing resources. Based on the analysis, determine the current and future scalability targets such as data size and workload, and distribute the data across the partitions to meet the scalability target. In the horizontal partitioning strategy, choosing the appropriate shard key is important to make sure distribution is even. For more information see the Sharding pattern. Make sure that the resources available to each partition are sufficient to handle the scalability requirements in terms of data size and throughput. For example, the node hosting a partition might impose a hard limit on the amount of storage space, processing power, or network bandwidth that it provides. If the data storage and processing requirements are likely to exceed these limits it may be necessary to refine your partitioning strategy or split data out further. For example, one scalability approach might be to separate logging data from the core application features by using separate data stores to prevent the total data storage requirements exceeding the scaling limit of the node. If the total number of data stores exceeds the node limit, it may be necessary to use separate storage nodes. Monitor the system under use to verify that the data is distributed as expected and that the partitions can handle the load imposed on them. It could be possible that the usage does not match that anticipated by the analysis it may be possible to rebalance the partitions. Failing that, it may be necessary to redesign some parts of the system to gain the balance that is required. Designing partitions for query performance \u00b6 When designing and implementing partitions, consider the following factors that affect : Query performance can often be boosted by using smaller data sets and parallel query execution. Each partition should contain a small proportion of the entire data set, and this reduction in volume can improve the performance of queries. Follow these steps when designing the partitions for query performance: Examine the business requirements to determine critical queries that must always perform quickly. Monitor the system to identify any queries that perform slowly. Establish which queries are performed most frequently. Design the partition key in a way that the application can easily find the partition if you are implementing horizontal partitioning. This prevents the query needing to scan through every partition. The selection of partition key and row key values should be driven by the way in which the data is accessed. You should choose a partition key/row key combination that supports the majority of your queries. The most efficient queries will retrieve data by specifying the partition key and the row key. Queries that specify a partition key can be satisfied by scanning a single partition; Queries that don't at least specify the partition key may require storage to scan every partition for your data. Unfortunately, it is hard to decide for a silver bullet partitioning strategy, as future jobs might need to query the data in wildly different ways. Most probably, the engineering team maintaining the Data Lake might have to revisit their data partitioning strategies often. Partitioning in Iceberg \u00b6 What does Iceberg do differently? Other tables formats like Hive support partitioning, but Iceberg supports hidden partitioning. Iceberg handles the tedious and error-prone task of producing partition values for rows in a table. Iceberg avoids reading unnecessary partitions automatically. Consumers don\u2019t need to know how the table is partitioned and add extra filters to their queries. Iceberg partition layouts can evolve as needed. Apache Iceberg is a relatively new, open source table format for storing petabyte-scale data sets. Iceberg fits easily into the existing big data ecosystem and currently has integration with Spark and Presto execution engines. Using a host of metadata kept on each table, Iceberg provides functions that are not traditionally available with other table formats. This includes schema evolution, partition evolution, and table version rollback \u2014 all possible without the need for costly table rewrites or table migration. Iceberg handles all the details of partitioning and querying, and keeps track of the relationship between a column value and its partition without requiring additional columns. Iceberg can partition timestamps by year, month, day, and hour granularity. It can also use a categorical column, like level in this logs example, to store rows together and speed up queries. Iceberg produces partition values by taking a column value and optionally transforming it. Iceberg is responsible for converting event_time into event_date, and keeps track of the relationship. Because Iceberg doesn\u2019t require user-maintained partition columns, it can hide partitioning. Partition values are produced correctly every time and always used to speed up queries, when possible. Producers and consumers wouldn\u2019t even see event_date. Most importantly, queries no longer depend on a table\u2019s physical layout. With a separation between physical and logical, Iceberg tables can evolve partition schemes over time as data volume changes. Misconfigured tables can be fixed without an expensive migration. Partition transform \u00b6 Partition evolution \u00b6","title":"Partitioning Practices"},{"location":"flare/partitioningpractices/#partitioning-practices","text":"","title":"Partitioning Practices"},{"location":"flare/partitioningpractices/#overview","text":"Data partitioning is a very common technique by that data stored in a persistent store is segregated into sections for faster queries and easy management. Partitioning your data helps reduce query costs and improve performance by limiting the amount of data query engines such as Minerva, need to scan in order to return the results for a specific query. Since it limits the volume of data scanned, dramatically accelerating queries. Partitioning is a way to make queries faster by grouping similar rows together when writing. This article will cover the data partitioning practices you need to know in order to optimize your analytics infrastructure for performance and to improve the storage of the dataset within DataOS. Data access operations on each partition take place over a smaller volume of data. Provided that the data is partitioned in a suitable way, this is much more efficient. Operations that affect more than one partition can execute in parallel. Data is commonly partitioned by timestamp \u2013 which could mean by hour, by minute or by day \u2013 and the size of the partition should depend on the type of query we intend to run. If most of our queries require data from the last 6 hours, we might want to use hourly partitioning rather than daily in order to scan less data. We will see how to achieve partitioning with some of the existing technologies for large-scale data processing:","title":"Overview"},{"location":"flare/partitioningpractices/#designing-partitions","text":"Data can be partitioned in different ways: horizontally, vertically, or functionally. The strategy you choose depends on the reason for partitioning the data, and the requirements of the applications and services that will access the data. For an enterprise wide Data Lake, data partitioning is an important aspect that needs serious thought and consideration. The three typical strategies for partitioning data are:","title":"Designing partitions"},{"location":"flare/partitioningpractices/#horizontal-partitioning","text":"In this strategy, each partition holds a specific subset of the data, such as all the orders for a specific set of customers in an ecommerce application. In this example, product inventory data is divided into pertitions based on the product key. Each partition holds the data for a contiguous range of partition keys (A-G and H-Z), organized alphabetically.","title":"Horizontal partitioning"},{"location":"flare/partitioningpractices/#vertical-partitioning","text":"In this strategy each partition holds a subset of the fields for items in the data store. The fields are divided according to their pattern of use, such as placing the frequently accessed fields in one vertical partition and the less frequently accessed fields in another. The most common use for vertical partitioning is to reduce the I/O and performance costs associated with fetching the items that are accessed most frequently. Figure 2 shows an overview of an example of vertical partitioning, where different properties for each data item are held in different partitions; the name, description, and price information for products are accessed more frequently than the volume in stock or the last ordered date. Name | Description | Price -------- | -------- | -------- | Volume in Stock | Last ordered Date -------- | -------- | In this example, the application regularly queries the product name, description, and price together when displaying the details of products to customers. The stock level and date when the product was last ordered from the manufacturer are held in a separate partition because these two items are commonly used together. This partitioning scheme has the added advantage that the relatively slow-moving data (product name, description, and price) is separated from the more dynamic data (stock level and last ordered date). Using this partitioning strategy, you can store sensitive (for example credit card )and other data on separate partition to maximize the security of sensitive data.","title":"Vertical partitioning"},{"location":"flare/partitioningpractices/#functional-partitioning","text":"In this strategy data is aggregated according to distinct business area or service in the application. For example, an ecommerce system that implements separate business functions for invoicing and managing product inventory might store invoice data in one partition and product inventory data in another. It\u2019s important to note that the three strategies described here can be combined. They are not mutually exclusive and you should consider them all when you design a partitioning scheme. For example, you might divide data into shards and then use vertical partitioning to further subdivide the data in each shard. Similarly, the data in a functional partition may be split into shards (which may also be vertically partitioned). You must evaluate and balance when designing a partitioning scheme that meets the overall data processing performance targets for your system.","title":"Functional partitioning"},{"location":"flare/partitioningpractices/#good-practices","text":"The most important factor when implementing this partitioning strategy is the choice of partitioning key. It can be difficult to change the key after the system is in operation. The key must ensure that data is partitioned so that the workload is as even as possible across the partitions. The partition key you choose should minimize any future requirements to split large partition into smaller pieces, coalesce small partitions into larger partitions, or change the schema that describes the data stored in a set of partitions. These operations can be very time consuming.","title":"Good practices"},{"location":"flare/partitioningpractices/#designing-partitions-for-scalability","text":"It is vital to consider size and workload for each partition and balance them so that data is distributed to achieve maximum scalability. However, you must also partition the data so that it does not exceed the scaling limits of a single partition store. Follow these steps when designing the partitions for scalability: Analyze the application to understand the data access patterns, such as size of the result set returned by each query, the frequency of access, the inherent latency, and the server-side compute processing requirements. In many cases, a few major entities will demand most of the processing resources. Based on the analysis, determine the current and future scalability targets such as data size and workload, and distribute the data across the partitions to meet the scalability target. In the horizontal partitioning strategy, choosing the appropriate shard key is important to make sure distribution is even. For more information see the Sharding pattern. Make sure that the resources available to each partition are sufficient to handle the scalability requirements in terms of data size and throughput. For example, the node hosting a partition might impose a hard limit on the amount of storage space, processing power, or network bandwidth that it provides. If the data storage and processing requirements are likely to exceed these limits it may be necessary to refine your partitioning strategy or split data out further. For example, one scalability approach might be to separate logging data from the core application features by using separate data stores to prevent the total data storage requirements exceeding the scaling limit of the node. If the total number of data stores exceeds the node limit, it may be necessary to use separate storage nodes. Monitor the system under use to verify that the data is distributed as expected and that the partitions can handle the load imposed on them. It could be possible that the usage does not match that anticipated by the analysis it may be possible to rebalance the partitions. Failing that, it may be necessary to redesign some parts of the system to gain the balance that is required.","title":"Designing partitions for scalability"},{"location":"flare/partitioningpractices/#designing-partitions-for-query-performance","text":"When designing and implementing partitions, consider the following factors that affect : Query performance can often be boosted by using smaller data sets and parallel query execution. Each partition should contain a small proportion of the entire data set, and this reduction in volume can improve the performance of queries. Follow these steps when designing the partitions for query performance: Examine the business requirements to determine critical queries that must always perform quickly. Monitor the system to identify any queries that perform slowly. Establish which queries are performed most frequently. Design the partition key in a way that the application can easily find the partition if you are implementing horizontal partitioning. This prevents the query needing to scan through every partition. The selection of partition key and row key values should be driven by the way in which the data is accessed. You should choose a partition key/row key combination that supports the majority of your queries. The most efficient queries will retrieve data by specifying the partition key and the row key. Queries that specify a partition key can be satisfied by scanning a single partition; Queries that don't at least specify the partition key may require storage to scan every partition for your data. Unfortunately, it is hard to decide for a silver bullet partitioning strategy, as future jobs might need to query the data in wildly different ways. Most probably, the engineering team maintaining the Data Lake might have to revisit their data partitioning strategies often.","title":"Designing partitions for query performance"},{"location":"flare/partitioningpractices/#partitioning-in-iceberg","text":"What does Iceberg do differently? Other tables formats like Hive support partitioning, but Iceberg supports hidden partitioning. Iceberg handles the tedious and error-prone task of producing partition values for rows in a table. Iceberg avoids reading unnecessary partitions automatically. Consumers don\u2019t need to know how the table is partitioned and add extra filters to their queries. Iceberg partition layouts can evolve as needed. Apache Iceberg is a relatively new, open source table format for storing petabyte-scale data sets. Iceberg fits easily into the existing big data ecosystem and currently has integration with Spark and Presto execution engines. Using a host of metadata kept on each table, Iceberg provides functions that are not traditionally available with other table formats. This includes schema evolution, partition evolution, and table version rollback \u2014 all possible without the need for costly table rewrites or table migration. Iceberg handles all the details of partitioning and querying, and keeps track of the relationship between a column value and its partition without requiring additional columns. Iceberg can partition timestamps by year, month, day, and hour granularity. It can also use a categorical column, like level in this logs example, to store rows together and speed up queries. Iceberg produces partition values by taking a column value and optionally transforming it. Iceberg is responsible for converting event_time into event_date, and keeps track of the relationship. Because Iceberg doesn\u2019t require user-maintained partition columns, it can hide partitioning. Partition values are produced correctly every time and always used to speed up queries, when possible. Producers and consumers wouldn\u2019t even see event_date. Most importantly, queries no longer depend on a table\u2019s physical layout. With a separation between physical and logical, Iceberg tables can evolve partition schemes over time as data volume changes. Misconfigured tables can be fixed without an expensive migration.","title":"Partitioning in Iceberg"},{"location":"flare/partitioningpractices/#partition-transform","text":"","title":"Partition transform"},{"location":"flare/partitioningpractices/#partition-evolution","text":"","title":"Partition evolution"},{"location":"flare/queryinjobprogress/","text":"Query Dataset for Job in Progress \u00b6 Overview \u00b6 This use case describes that Read & Write operations work in isolation for Iceberg formats, not affecting the live table. So when you run a Flare job to write data in Iceberg format, the dataset is available for query while data is being written. Using the snapshot pattern, it will perform a metadata swap only when the Write operation is complete. The use of snapshots also enables time-travel operations as you can perform query operations on different versions of the table by specifying the snapshot to use. Implementation Details \u00b6 While a Flare job is running for writing in Iceberg, we can run the query with the previous snapshot- metadata location set before the job run. If we update the snapshot id during a job run, it picks data from that snapshot. Same as done in Concurrent writes and parallel processing use case . Outcomes \u00b6 This scenario was tested by querying the data while parallel write operation was in progress. Data from the previous snapshot was available to query. Code files \u00b6 Refer code file in Concurrent writes and parallel processing use case .","title":"Query Dataset for Job in Progress"},{"location":"flare/queryinjobprogress/#query-dataset-for-job-in-progress","text":"","title":"Query Dataset for Job in Progress"},{"location":"flare/queryinjobprogress/#overview","text":"This use case describes that Read & Write operations work in isolation for Iceberg formats, not affecting the live table. So when you run a Flare job to write data in Iceberg format, the dataset is available for query while data is being written. Using the snapshot pattern, it will perform a metadata swap only when the Write operation is complete. The use of snapshots also enables time-travel operations as you can perform query operations on different versions of the table by specifying the snapshot to use.","title":"Overview"},{"location":"flare/queryinjobprogress/#implementation-details","text":"While a Flare job is running for writing in Iceberg, we can run the query with the previous snapshot- metadata location set before the job run. If we update the snapshot id during a job run, it picks data from that snapshot. Same as done in Concurrent writes and parallel processing use case .","title":"Implementation Details"},{"location":"flare/queryinjobprogress/#outcomes","text":"This scenario was tested by querying the data while parallel write operation was in progress. Data from the previous snapshot was available to query.","title":"Outcomes"},{"location":"flare/queryinjobprogress/#code-files","text":"Refer code file in Concurrent writes and parallel processing use case .","title":"Code files"},{"location":"flare/readingstreamdata/","text":"Reading Stream Data \u00b6 Overview \u00b6 This use case describes the scenario when you want your Flare job to ingest stream data from Kafka and save it to filesystems, databases, Iceberg, etc. Flare provides a declarative stack to run Spark jobs. Solution approach \u00b6 As Spark does not support writing streaming data, so this is achieved by using Spark Streaming API. Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput stream processing of live data streams. Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka. Internally, a DStream is represented as a sequence of RDDs. Internally, Spark streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches. Implementation details \u00b6 Similar to RDDs, DStreams also allow developers to persist the stream\u2019s data in memory. You need to define checkpointlocation to store the intermediate batches. The batched files are uploaded into a staging storage zone in DataOS, processed, and pushed into the main storage zone, batch by batch. Checkpointing of RDDs incurs the cost of saving to reliable storage. This may cause an increase in the processing time of those batches where RDDs get checkpointed. Hence, the interval of checkpointing needs to be set carefully using dstream.checkpoint(checkpointInterval). Checkpointing with large intervals may cause the lineage and task sizes to grow, which may have detrimental effects. Typically, a checkpoint interval of 5 - 10 seconds of a DStream is recommended. Outcomes \u00b6 The following Flare job saves the data from Kafka to Iceberg tables. Code files \u00b6 dag : - name : cloudevents-data title : stream data to dataset description : This job ingests stream data to Iceberg table spec : stack : flare:1.0 flare : job : explain : true streaming : batchMode : true # to create batches triggerMode : Once #Once for cont checkpointLocation : dataos://icebase:raw01/checkpoints/cloudevents/ce01?acl=rw #triggerDuration: \"20 seconds\" inputs : - name : input_cloudevents dataset : dataos://kafka:default/cloudevents?acl=r schemaRegistryUrl : http://schema-registry.caretaker:8081 isStream : false options : startingOffsets : earliest logLevel : INFO outputs : - name : output01 depot : dataos://icebase:sys01?acl=rw # checkpointLocation: dataos://icebase:raw01/checkpoints/cloudevents/cedev01?acl=rw steps : - sequence : - name : cloudevents sql : SELECT *, time as _timestamp FROM input_cloudevents functions : - name : set_type columns : _timestamp : timestamp time : timestamp sink : - sequenceName : cloudevents datasetName : cloudevents outputName : output01 outputType : Iceberg title : cloudevents data description : cloudevents data outputOptions : saveMode : overwrite # extraOptions: # fanout-enabled: \"true\" # triggerDuration: \"20 seconds\" iceberg : properties : write.metadata.previous-versions-max : \"10\" history.expire.max-snapshot-age-ms : \"7200000\" overwrite-mode : dynamic partitionSpec : - type : identity column : source - type : hour column : _timestamp name : hour tags : - Cloudevents","title":"Reading Stream Data"},{"location":"flare/readingstreamdata/#reading-stream-data","text":"","title":"Reading Stream Data"},{"location":"flare/readingstreamdata/#overview","text":"This use case describes the scenario when you want your Flare job to ingest stream data from Kafka and save it to filesystems, databases, Iceberg, etc. Flare provides a declarative stack to run Spark jobs.","title":"Overview"},{"location":"flare/readingstreamdata/#solution-approach","text":"As Spark does not support writing streaming data, so this is achieved by using Spark Streaming API. Spark Streaming is an extension of the core Spark API that enables scalable, high-throughput stream processing of live data streams. Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. DStreams can be created either from input data streams from sources such as Kafka. Internally, a DStream is represented as a sequence of RDDs. Internally, Spark streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.","title":"Solution approach"},{"location":"flare/readingstreamdata/#implementation-details","text":"Similar to RDDs, DStreams also allow developers to persist the stream\u2019s data in memory. You need to define checkpointlocation to store the intermediate batches. The batched files are uploaded into a staging storage zone in DataOS, processed, and pushed into the main storage zone, batch by batch. Checkpointing of RDDs incurs the cost of saving to reliable storage. This may cause an increase in the processing time of those batches where RDDs get checkpointed. Hence, the interval of checkpointing needs to be set carefully using dstream.checkpoint(checkpointInterval). Checkpointing with large intervals may cause the lineage and task sizes to grow, which may have detrimental effects. Typically, a checkpoint interval of 5 - 10 seconds of a DStream is recommended.","title":"Implementation details"},{"location":"flare/readingstreamdata/#outcomes","text":"The following Flare job saves the data from Kafka to Iceberg tables.","title":"Outcomes"},{"location":"flare/readingstreamdata/#code-files","text":"dag : - name : cloudevents-data title : stream data to dataset description : This job ingests stream data to Iceberg table spec : stack : flare:1.0 flare : job : explain : true streaming : batchMode : true # to create batches triggerMode : Once #Once for cont checkpointLocation : dataos://icebase:raw01/checkpoints/cloudevents/ce01?acl=rw #triggerDuration: \"20 seconds\" inputs : - name : input_cloudevents dataset : dataos://kafka:default/cloudevents?acl=r schemaRegistryUrl : http://schema-registry.caretaker:8081 isStream : false options : startingOffsets : earliest logLevel : INFO outputs : - name : output01 depot : dataos://icebase:sys01?acl=rw # checkpointLocation: dataos://icebase:raw01/checkpoints/cloudevents/cedev01?acl=rw steps : - sequence : - name : cloudevents sql : SELECT *, time as _timestamp FROM input_cloudevents functions : - name : set_type columns : _timestamp : timestamp time : timestamp sink : - sequenceName : cloudevents datasetName : cloudevents outputName : output01 outputType : Iceberg title : cloudevents data description : cloudevents data outputOptions : saveMode : overwrite # extraOptions: # fanout-enabled: \"true\" # triggerDuration: \"20 seconds\" iceberg : properties : write.metadata.previous-versions-max : \"10\" history.expire.max-snapshot-age-ms : \"7200000\" overwrite-mode : dynamic partitionSpec : - type : identity column : source - type : hour column : _timestamp name : hour tags : - Cloudevents","title":"Code files"},{"location":"flare/scenarios/","text":"Use Cases for various scenarios \u00b6 This articles explains all the scenarios with the proper use cases to understand the desired functionality and intented behaviour while writing Flare jobs. It also includes yaml for the respective use cases. Note: You need to run data tool for all the use cases related to iceberg to access the ingested data in the Workbench. Ensure to update the latest data tool image and dataset path in the data tool yaml. Scenario #1- Too many files with few records problem. \u00b6 Overview \u00b6 This use case involves combining a large number of files with very few records. Solution approach \u00b6 Files are combined and compressed. Implementation Details \u00b6 Outcomes \u00b6 if there are many files in some partition or non partitioned data, it will merge all small files into a large one Impediments/fallbacks \u00b6 Code files \u00b6 Scenario #2- Ensure dataset to be available for query while a job is writing data to it. \u00b6 Overview \u00b6 Solution approach \u00b6 Implementation Details \u00b6 Outcomes \u00b6 Tested this- While a job is running and metadata location is set, we can run the query with the previous metadata location set before the job run. If we update the snapshot id during a job run, it picks data from that snapshot Impediments/fallbacks \u00b6 Code files \u00b6 . Scenario #3- Try out with partition keys with different data types \u00b6 Overview \u00b6 Partitioning is a way to make queries faster by grouping similar rows together when writing. This use case checks how partitioning works for Flare while writing data to iceberg and non iceberg type data storage in order to improve query processing performance. Solution approach \u00b6 Iceberg can partition timestamps by year, month, day, and hour granularity. It can also use a categorical column of data type- string, integer, long, double(to be checked) , like vendor id in this example, to store rows together and speed up queries. Iceberg produces partition values by taking a column value and optionally transforming it. Iceberg converts timestamp values into a date, and extracts year, month, day, hour, etc. While in non-iceberg data sink, provide the columns explicitly for the year, month, hour as transformation is not automatic. Implementation details \u00b6 The five partitioning modes were tested ( identity, year, month, day & hour). Partition using type identity Two cases were checked, one for integer values and another for string values. Partition using type timestamp column All four (year, month, day, hour) partitioning cases around timestamp column are working. Example 1: Partitioning is done on identity by taking the vendor_id column. You don't need to give name property if partition field type is identity type. partitionSpec : - type : identity # options tested: identity, year, month, day, hour column : vendor_id # columns used - identity (vendor_id, one string column) & for rest date_col Example 2: Partitioning is done on the year. partitionSpec : - type : year # options tested: identity, year, month, day, hour column : date_col # columns used - identity (vendor_id, one string column) & for rest date_col name : year Example 3: Nested partitioning is done on (identity, year). Here, the vendor_id used for identity should come at the first level. partitionSpec : - type : identity column : vendor_id # columns used - identity (vendor_id, one string column) & for rest date_col - type : year # options tested: identity, year, month, day, hour column : date_col name : year Note: When specifying the partition criterion, the 'partitionSpec' property should be defined as a child property under 'iceberg' in the sink section. Otherwise partioning will not be performed as desired. Outcome \u00b6 The files are stored in the folders based on the partition criterion defined, and you can view them in workbench or storage locations. Impediments/fallouts \u00b6 Code files \u00b6 ``` yaml version : v1beta1 name : workflow-ny-taxi-partitioned-vendor type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and write with partitioning on vendor_id workflow : title : Connect NY Taxi dag : - name : nytaxi title : NY-taxi data ingester description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : driver : coreLimit : 2400m cores : 2 memory : 3072m executor : coreLimit : 2400m cores : 2 instances : 1 memory : 4096m job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01 steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_07 outputName : output01 outputType : Iceberg outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity # options tested: string, integer, long column : vendor_id # columns used - identity (vendor_id, one string column) - type : year # options tested: identity, year, month, day, hour column : date_col # columns used - identity (vendor_id, one string column) & for rest date_col name : year tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned Vendor sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat sparkConf : - spark.serializer : org.apache.spark.serializer.KryoSerializer - spark.sql.shuffle.partitions : \"800\" - spark.executor.extraJavaOptions : -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.driver.extraJavaOptions : -XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.memory.storageFraction : \"0.1\" - spark.memory.fraction : \"0.1\" - spark.kryoserializer.buffer.max : 256m - spark.shuffle.memoryFraction : \"0.0\" Scenario #4- Data Replay \u00b6 Overview \u00b6 This use case describes the data replay scenario where you need to re-write a data segment for various reasons, such as incomplete or corrupted data. You can configure your data ingestion jobs to write the proper partitions to avoid expensive complete data re-write. Solution approach \u00b6 This use case configures jobs for replacing one partition of data. The behavior was to overwrite partitions dynamically. Implementation details \u00b6 Data replay scenario is tested by first ingesting the NY-taxi data at vendor level partitioning and then one vendor data was replaced using the following properties: saveMode: overwrite overwrite-mode: dynamic The test validation is done with timestamp column by comparing the values written at the time of first write and the second time when the data is written only for the one vendor data. Data replay is tested by writing data with partitioning and one partition of data was replaced by another job. Outcomes \u00b6 The files are stored in the folders based on the partition criterion defined and can be viewed in workbench or storage locaions. The accuracy of the output was tested by running queries accessing data from the modified partition and confirmed with the timestamp values. Impediments/fallbacks \u00b6 Code files \u00b6 ### this job is for only changing one partition of dataset --- version : v1beta1 name : workflow-ny-taxi-partitioned-vendor type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and write with partitioning on vendor_id workflow : title : Connect NY Taxi dag : - name : nytaxi title : NY-taxi data ingester description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : driver : coreLimit : 2400m cores : 2 memory : 3072m executor : coreLimit : 2400m cores : 2 instances : 1 memory : 4096m job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01 steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_05 outputName : output01 outputType : Iceberg outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip overwrite-mode : dynamic #overwrite-mode: dynamic # this was used only when one partition data is need to replace with saveMode as Overwrite that job was seperate if need will send that as well partitionSpec : - type : identity column : vendor_id tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned Vendor sequence : - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi where vendor_id = 1 ## data written for only one vendor sparkConf : - spark.serializer : org.apache.spark.serializer.KryoSerializer - spark.sql.shuffle.partitions : \"800\" - spark.executor.extraJavaOptions : -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.driver.extraJavaOptions : -XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.memory.storageFraction : \"0.1\" - spark.memory.fraction : \"0.1\" - spark.kryoserializer.buffer.max : 256m - spark.shuffle.memoryFraction : \"0.0\" Scenario #5- Test concurrent writes and parallel processing \u00b6 Overview \u00b6 This use case tests the scenario where multiple jobs are concurrently writing at the same location. Solution approach \u00b6 This scenario works only for iceberg as it allows parallel writes. A workflow with two jobs are defined to write at the same data location. Implementation Details \u00b6 This use case was tested on NY-Taxi data. Two jobs (in one dag) were created for each vendor (data was filtered on vendor level). The workflow is to be submitted for both the modes: - first, with the append save mode and then When written in append mode, the data should be written from both jobs. overwrite save mode When Flare's overwrite mode is dynamic, partitions that have rows produced by the jobs will be replaced on every new write operation. Only the last finished job's data should be seen in this case. Outcomes \u00b6 Queries were run to validate the expected behavior. Also, it was possible to query data while data was being written. Impediments/fallbacks \u00b6 Code files \u00b6 # This contains two jobs and save mode as append --- version : v1beta1 name : workflow-ny-taxi-parallel-write type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and conbined them to one file workflow : title : Connect NY Taxi dag : - name : nytaxi-vendor-one title : NY-taxi data ingester-parallel description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 # persistentVolume: # name: persistent-v # directory: connectCity flare : driver : coreLimit : 1200m cores : 1 memory : 1024m executor : coreLimit : 1200m cores : 1 instances : 1 memory : 1024m job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01 steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_04 outputName : output01 outputType : Iceberg outputOptions : saveMode : append iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip #overwrite-mode: dynamic # this was used only when one partition data is need to be replaced with saveMode as Overwrite that job was seperate if need will send that as well partitionSpec : - type : month # identity partitioning was used at vendor_id level column : date_col # col name = vendor_id name : month tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi where vendor_id = 1 - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat sparkConf : - spark.serializer : org.apache.spark.serializer.KryoSerializer - spark.sql.shuffle.partitions : \"800\" - spark.executor.extraJavaOptions : -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.driver.extraJavaOptions : -XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.memory.storageFraction : \"0.1\" - spark.memory.fraction : \"0.1\" - spark.kryoserializer.buffer.max : 256m - spark.shuffle.memoryFraction : \"0.0\" - name : nytaxi-vendor-two title : NY-taxi data ingester-parallel description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 # persistentVolume: # name: persistent-v # directory: connectCity flare : driver : coreLimit : 1200m cores : 1 memory : 1024m executor : coreLimit : 1200m cores : 1 instances : 1 memory : 1024m job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data format : json isStream : false logLevel : INFO outputs : - name : output02 depot : dataos://icebase:raw01 steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_04 outputName : output02 outputType : Iceberg outputOptions : saveMode : append iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : month column : date_col name : month tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi where vendor_id = 2 - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat sparkConf : - spark.serializer : org.apache.spark.serializer.KryoSerializer - spark.sql.shuffle.partitions : \"800\" - spark.executor.extraJavaOptions : -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.driver.extraJavaOptions : -XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.memory.storageFraction : \"0.1\" - spark.memory.fraction : \"0.1\" - spark.kryoserializer.buffer.max : 256m - spark.shuffle.memoryFraction : \"0.0\" Scenario #6- Finalize strategy to supply credentials to Flare for GCP. \u00b6 Overview Solution approach We will ask for access to the service account, which is configured during the install process, As a par limitation on the GCP cloud we can set only one credential in the flare job, So we need to ask for permissions on that account only. We need to document this strategy and mentation all required permission. BigQuery Data Viewer BigQuery Job User BigQuery Metadata Viewer BigQuery Read Session User Storage Admin Storage Object Viewer Implementation details \u00b6 Outcomes \u00b6 Impediments/fallbacks \u00b6 Code files \u00b6","title":"Use Cases for various scenarios"},{"location":"flare/scenarios/#use-cases-for-various-scenarios","text":"This articles explains all the scenarios with the proper use cases to understand the desired functionality and intented behaviour while writing Flare jobs. It also includes yaml for the respective use cases. Note: You need to run data tool for all the use cases related to iceberg to access the ingested data in the Workbench. Ensure to update the latest data tool image and dataset path in the data tool yaml.","title":"Use Cases for various scenarios"},{"location":"flare/scenarios/#scenario-1-too-many-files-with-few-records-problem","text":"","title":"Scenario #1- Too many files with few records problem."},{"location":"flare/scenarios/#overview","text":"This use case involves combining a large number of files with very few records.","title":"Overview"},{"location":"flare/scenarios/#solution-approach","text":"Files are combined and compressed.","title":"Solution approach"},{"location":"flare/scenarios/#implementation-details","text":"","title":"Implementation Details"},{"location":"flare/scenarios/#outcomes","text":"if there are many files in some partition or non partitioned data, it will merge all small files into a large one","title":"Outcomes"},{"location":"flare/scenarios/#impedimentsfallbacks","text":"","title":"Impediments/fallbacks"},{"location":"flare/scenarios/#code-files","text":"","title":"Code files"},{"location":"flare/scenarios/#scenario-2-ensure-dataset-to-be-available-for-query-while-a-job-is-writing-data-to-it","text":"","title":"Scenario #2- Ensure dataset to be available for query while a job is writing data to it."},{"location":"flare/scenarios/#overview_1","text":"","title":"Overview"},{"location":"flare/scenarios/#solution-approach_1","text":"","title":"Solution approach"},{"location":"flare/scenarios/#implementation-details_1","text":"","title":"Implementation Details"},{"location":"flare/scenarios/#outcomes_1","text":"Tested this- While a job is running and metadata location is set, we can run the query with the previous metadata location set before the job run. If we update the snapshot id during a job run, it picks data from that snapshot","title":"Outcomes"},{"location":"flare/scenarios/#impedimentsfallbacks_1","text":"","title":"Impediments/fallbacks"},{"location":"flare/scenarios/#code-files_1","text":".","title":"Code files"},{"location":"flare/scenarios/#scenario-3-try-out-with-partition-keys-with-different-data-types","text":"","title":"Scenario #3- Try out with partition keys with different data types"},{"location":"flare/scenarios/#overview_2","text":"Partitioning is a way to make queries faster by grouping similar rows together when writing. This use case checks how partitioning works for Flare while writing data to iceberg and non iceberg type data storage in order to improve query processing performance.","title":"Overview"},{"location":"flare/scenarios/#solution-approach_2","text":"Iceberg can partition timestamps by year, month, day, and hour granularity. It can also use a categorical column of data type- string, integer, long, double(to be checked) , like vendor id in this example, to store rows together and speed up queries. Iceberg produces partition values by taking a column value and optionally transforming it. Iceberg converts timestamp values into a date, and extracts year, month, day, hour, etc. While in non-iceberg data sink, provide the columns explicitly for the year, month, hour as transformation is not automatic.","title":"Solution approach"},{"location":"flare/scenarios/#implementation-details_2","text":"The five partitioning modes were tested ( identity, year, month, day & hour). Partition using type identity Two cases were checked, one for integer values and another for string values. Partition using type timestamp column All four (year, month, day, hour) partitioning cases around timestamp column are working. Example 1: Partitioning is done on identity by taking the vendor_id column. You don't need to give name property if partition field type is identity type. partitionSpec : - type : identity # options tested: identity, year, month, day, hour column : vendor_id # columns used - identity (vendor_id, one string column) & for rest date_col Example 2: Partitioning is done on the year. partitionSpec : - type : year # options tested: identity, year, month, day, hour column : date_col # columns used - identity (vendor_id, one string column) & for rest date_col name : year Example 3: Nested partitioning is done on (identity, year). Here, the vendor_id used for identity should come at the first level. partitionSpec : - type : identity column : vendor_id # columns used - identity (vendor_id, one string column) & for rest date_col - type : year # options tested: identity, year, month, day, hour column : date_col name : year Note: When specifying the partition criterion, the 'partitionSpec' property should be defined as a child property under 'iceberg' in the sink section. Otherwise partioning will not be performed as desired.","title":"Implementation details"},{"location":"flare/scenarios/#outcome","text":"The files are stored in the folders based on the partition criterion defined, and you can view them in workbench or storage locations.","title":"Outcome"},{"location":"flare/scenarios/#impedimentsfallouts","text":"","title":"Impediments/fallouts"},{"location":"flare/scenarios/#code-files_2","text":"``` yaml version : v1beta1 name : workflow-ny-taxi-partitioned-vendor type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and write with partitioning on vendor_id workflow : title : Connect NY Taxi dag : - name : nytaxi title : NY-taxi data ingester description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : driver : coreLimit : 2400m cores : 2 memory : 3072m executor : coreLimit : 2400m cores : 2 instances : 1 memory : 4096m job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01 steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_07 outputName : output01 outputType : Iceberg outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : identity # options tested: string, integer, long column : vendor_id # columns used - identity (vendor_id, one string column) - type : year # options tested: identity, year, month, day, hour column : date_col # columns used - identity (vendor_id, one string column) & for rest date_col name : year tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned Vendor sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat sparkConf : - spark.serializer : org.apache.spark.serializer.KryoSerializer - spark.sql.shuffle.partitions : \"800\" - spark.executor.extraJavaOptions : -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.driver.extraJavaOptions : -XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.memory.storageFraction : \"0.1\" - spark.memory.fraction : \"0.1\" - spark.kryoserializer.buffer.max : 256m - spark.shuffle.memoryFraction : \"0.0\"","title":"Code files"},{"location":"flare/scenarios/#scenario-4-data-replay","text":"","title":"Scenario #4- Data Replay"},{"location":"flare/scenarios/#overview_3","text":"This use case describes the data replay scenario where you need to re-write a data segment for various reasons, such as incomplete or corrupted data. You can configure your data ingestion jobs to write the proper partitions to avoid expensive complete data re-write.","title":"Overview"},{"location":"flare/scenarios/#solution-approach_3","text":"This use case configures jobs for replacing one partition of data. The behavior was to overwrite partitions dynamically.","title":"Solution approach"},{"location":"flare/scenarios/#implementation-details_3","text":"Data replay scenario is tested by first ingesting the NY-taxi data at vendor level partitioning and then one vendor data was replaced using the following properties: saveMode: overwrite overwrite-mode: dynamic The test validation is done with timestamp column by comparing the values written at the time of first write and the second time when the data is written only for the one vendor data. Data replay is tested by writing data with partitioning and one partition of data was replaced by another job.","title":"Implementation details"},{"location":"flare/scenarios/#outcomes_2","text":"The files are stored in the folders based on the partition criterion defined and can be viewed in workbench or storage locaions. The accuracy of the output was tested by running queries accessing data from the modified partition and confirmed with the timestamp values.","title":"Outcomes"},{"location":"flare/scenarios/#impedimentsfallbacks_2","text":"","title":"Impediments/fallbacks"},{"location":"flare/scenarios/#code-files_3","text":"### this job is for only changing one partition of dataset --- version : v1beta1 name : workflow-ny-taxi-partitioned-vendor type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and write with partitioning on vendor_id workflow : title : Connect NY Taxi dag : - name : nytaxi title : NY-taxi data ingester description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 flare : driver : coreLimit : 2400m cores : 2 memory : 3072m executor : coreLimit : 2400m cores : 2 instances : 1 memory : 4096m job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01 steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_05 outputName : output01 outputType : Iceberg outputOptions : saveMode : overwrite iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip overwrite-mode : dynamic #overwrite-mode: dynamic # this was used only when one partition data is need to replace with saveMode as Overwrite that job was seperate if need will send that as well partitionSpec : - type : identity column : vendor_id tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned Vendor sequence : - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi where vendor_id = 1 ## data written for only one vendor sparkConf : - spark.serializer : org.apache.spark.serializer.KryoSerializer - spark.sql.shuffle.partitions : \"800\" - spark.executor.extraJavaOptions : -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.driver.extraJavaOptions : -XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.memory.storageFraction : \"0.1\" - spark.memory.fraction : \"0.1\" - spark.kryoserializer.buffer.max : 256m - spark.shuffle.memoryFraction : \"0.0\"","title":"Code files"},{"location":"flare/scenarios/#scenario-5-test-concurrent-writes-and-parallel-processing","text":"","title":"Scenario #5- Test concurrent writes and parallel processing"},{"location":"flare/scenarios/#overview_4","text":"This use case tests the scenario where multiple jobs are concurrently writing at the same location.","title":"Overview"},{"location":"flare/scenarios/#solution-approach_4","text":"This scenario works only for iceberg as it allows parallel writes. A workflow with two jobs are defined to write at the same data location.","title":"Solution approach"},{"location":"flare/scenarios/#implementation-details_4","text":"This use case was tested on NY-Taxi data. Two jobs (in one dag) were created for each vendor (data was filtered on vendor level). The workflow is to be submitted for both the modes: - first, with the append save mode and then When written in append mode, the data should be written from both jobs. overwrite save mode When Flare's overwrite mode is dynamic, partitions that have rows produced by the jobs will be replaced on every new write operation. Only the last finished job's data should be seen in this case.","title":"Implementation Details"},{"location":"flare/scenarios/#outcomes_3","text":"Queries were run to validate the expected behavior. Also, it was possible to query data while data was being written.","title":"Outcomes"},{"location":"flare/scenarios/#impedimentsfallbacks_3","text":"","title":"Impediments/fallbacks"},{"location":"flare/scenarios/#code-files_4","text":"# This contains two jobs and save mode as append --- version : v1beta1 name : workflow-ny-taxi-parallel-write type : workflow tags : - Connect - NY-Taxi description : The job ingests NY-Taxi data small files and conbined them to one file workflow : title : Connect NY Taxi dag : - name : nytaxi-vendor-one title : NY-taxi data ingester-parallel description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 # persistentVolume: # name: persistent-v # directory: connectCity flare : driver : coreLimit : 1200m cores : 1 memory : 1024m executor : coreLimit : 1200m cores : 1 instances : 1 memory : 1024m job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data format : json isStream : false logLevel : INFO outputs : - name : output01 depot : dataos://icebase:raw01 steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_04 outputName : output01 outputType : Iceberg outputOptions : saveMode : append iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip #overwrite-mode: dynamic # this was used only when one partition data is need to be replaced with saveMode as Overwrite that job was seperate if need will send that as well partitionSpec : - type : month # identity partitioning was used at vendor_id level column : date_col # col name = vendor_id name : month tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi where vendor_id = 1 - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat sparkConf : - spark.serializer : org.apache.spark.serializer.KryoSerializer - spark.sql.shuffle.partitions : \"800\" - spark.executor.extraJavaOptions : -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.driver.extraJavaOptions : -XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.memory.storageFraction : \"0.1\" - spark.memory.fraction : \"0.1\" - spark.kryoserializer.buffer.max : 256m - spark.shuffle.memoryFraction : \"0.0\" - name : nytaxi-vendor-two title : NY-taxi data ingester-parallel description : The job ingests NY-Taxi data from dropzone into raw zone spec : tags : - Connect - NY-Taxi stack : flare:1.0 # persistentVolume: # name: persistent-v # directory: connectCity flare : driver : coreLimit : 1200m cores : 1 memory : 1024m executor : coreLimit : 1200m cores : 1 instances : 1 memory : 1024m job : explain : true inputs : - name : ny_taxi dataset : dataos://thirdparty01:none/ny-taxi-data format : json isStream : false logLevel : INFO outputs : - name : output02 depot : dataos://icebase:raw01 steps : - sink : - sequenceName : ny_taxi_ts datasetName : ny_taxi_04 outputName : output02 outputType : Iceberg outputOptions : saveMode : append iceberg : properties : write.format.default : parquet write.metadata.compression-codec : gzip partitionSpec : - type : month column : date_col name : month tags : - Connect - NY-Taxi title : NY-Taxi Data Partitioned sequence : - name : ny_taxi_changed_dateformat sql : select *, to_timestamp(pickup_datetime/1000) as date_col from ny_taxi where vendor_id = 2 - name : ny_taxi_ts sql : SELECT *, date_format(now(), 'yyyyMMddHHmm') as version, now() as ts_ny_taxi FROM ny_taxi_changed_dateformat sparkConf : - spark.serializer : org.apache.spark.serializer.KryoSerializer - spark.sql.shuffle.partitions : \"800\" - spark.executor.extraJavaOptions : -XX:+PrintFlagsFinal -XX:+PrintReferenceGC -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintAdaptiveSizePolicy -XX:+UnlockDiagnosticVMOptions -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.driver.extraJavaOptions : -XX:+PrintTenuringDistribution -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/hoodie-heapdump.hprof - spark.memory.storageFraction : \"0.1\" - spark.memory.fraction : \"0.1\" - spark.kryoserializer.buffer.max : 256m - spark.shuffle.memoryFraction : \"0.0\"","title":"Code files"},{"location":"flare/scenarios/#scenario-6-finalize-strategy-to-supply-credentials-to-flare-for-gcp","text":"Overview Solution approach We will ask for access to the service account, which is configured during the install process, As a par limitation on the GCP cloud we can set only one credential in the flare job, So we need to ask for permissions on that account only. We need to document this strategy and mentation all required permission. BigQuery Data Viewer BigQuery Job User BigQuery Metadata Viewer BigQuery Read Session User Storage Admin Storage Object Viewer","title":"Scenario #6- Finalize strategy to supply credentials to Flare for GCP."},{"location":"flare/scenarios/#implementation-details_5","text":"","title":"Implementation details"},{"location":"flare/scenarios/#outcomes_4","text":"","title":"Outcomes"},{"location":"flare/scenarios/#impedimentsfallbacks_4","text":"","title":"Impediments/fallbacks"},{"location":"flare/scenarios/#code-files_5","text":"","title":"Code files"},{"location":"flare/schemaevolution/","text":"Overview \u00b6 Evolving table schemas, which constantly change to accommodate new business requirements. Schema Evolution Source schemas change and evolve over time. In the data lake, schema evolution is largely a function of the chosen file format. Engines supporting schema-on-read leverage the file format\u2019s evolution capabilities to handle schema evolution gracefully Solution approach \u00b6 Implementation details \u00b6 Outcomes \u00b6 Code files \u00b6","title":"Schemaevolution"},{"location":"flare/schemaevolution/#overview","text":"Evolving table schemas, which constantly change to accommodate new business requirements. Schema Evolution Source schemas change and evolve over time. In the data lake, schema evolution is largely a function of the chosen file format. Engines supporting schema-on-read leverage the file format\u2019s evolution capabilities to handle schema evolution gracefully","title":"Overview"},{"location":"flare/schemaevolution/#solution-approach","text":"","title":"Solution approach"},{"location":"flare/schemaevolution/#implementation-details","text":"","title":"Implementation details"},{"location":"flare/schemaevolution/#outcomes","text":"","title":"Outcomes"},{"location":"flare/schemaevolution/#code-files","text":"","title":"Code files"},{"location":"flare/streamtobatch/","text":"Read Stream Data as Batch \u00b6 Overview \u00b6 This use case describes the scenario when you want to write an evolving stream data coming from Kafka to Iceberg table. As Iceberg does not support writing streaming data, so this is achieved by providing a buffering point from stream data coming from Kafka and moving data from this buffering point to the Iceberg table. Solution approach \u00b6 Iceberg doesn\u2019t support \u201ccontinuous processing\u201d, as it doesn\u2019t provide the interface to \u201ccommit\u201d the output. Iceberg supports append and complete output modes: append: appends the rows of every micro-batch to the table complete: replaces the table contents every micro-batch We need to buffer our writes as a micro batch. The batched files are uploaded into a staging storage zone in DataOS, processed, and committed into the main storage zone, batch by batch. Implementation details \u00b6 Iceberg has snapshotId and timestamp, corresponding, Kafka has offset and timestamp: offset: It is used for incremental read, such as the state of a checkpoint in a computing system. timestamp: It is explicitly specified by the user to specify the scope of consumption. You need to define checkpointlocation to store the intermediate batches. Outcomes \u00b6 Code files \u00b6 dag : - name : cloudevents-data title : stream data to dataset description : This job ingests stream data to Iceberg table spec : stack : flare:1.0 flare : job : explain : true streaming : batchMode : true # to create batches triggerMode : Once #Once for cont checkpointLocation : dataos://icebase:raw01/checkpoints/cloudevents/ce01?acl=rw #triggerDuration: \"20 seconds\" inputs : - name : input_cloudevents dataset : dataos://kafka:default/cloudevents?acl=r schemaRegistryUrl : http://schema-registry.caretaker:8081 isStream : false options : startingOffsets : earliest logLevel : INFO outputs : - name : output01 depot : dataos://icebase:sys01?acl=rw # checkpointLocation: dataos://icebase:raw01/checkpoints/cloudevents/cedev01 steps : - sequence : - name : cloudevents sql : SELECT *, time as _timestamp FROM input_cloudevents functions : - name : set_type columns : _timestamp : timestamp time : timestamp sink : - sequenceName : cloudevents datasetName : cloudevents outputName : output01 outputType : Iceberg title : cloudevents data description : cloudevents data outputOptions : saveMode : overwrite # extraOptions: # fanout-enabled: \"true\" # triggerDuration: \"20 seconds\" iceberg : properties : write.metadata.previous-versions-max : \"10\" history.expire.max-snapshot-age-ms : \"7200000\" overwrite-mode : dynamic partitionSpec : - type : identity column : source - type : hour column : _timestamp name : hour tags : - Cloudevents","title":"Read Stream Data as Batch"},{"location":"flare/streamtobatch/#read-stream-data-as-batch","text":"","title":"Read Stream Data as Batch"},{"location":"flare/streamtobatch/#overview","text":"This use case describes the scenario when you want to write an evolving stream data coming from Kafka to Iceberg table. As Iceberg does not support writing streaming data, so this is achieved by providing a buffering point from stream data coming from Kafka and moving data from this buffering point to the Iceberg table.","title":"Overview"},{"location":"flare/streamtobatch/#solution-approach","text":"Iceberg doesn\u2019t support \u201ccontinuous processing\u201d, as it doesn\u2019t provide the interface to \u201ccommit\u201d the output. Iceberg supports append and complete output modes: append: appends the rows of every micro-batch to the table complete: replaces the table contents every micro-batch We need to buffer our writes as a micro batch. The batched files are uploaded into a staging storage zone in DataOS, processed, and committed into the main storage zone, batch by batch.","title":"Solution approach"},{"location":"flare/streamtobatch/#implementation-details","text":"Iceberg has snapshotId and timestamp, corresponding, Kafka has offset and timestamp: offset: It is used for incremental read, such as the state of a checkpoint in a computing system. timestamp: It is explicitly specified by the user to specify the scope of consumption. You need to define checkpointlocation to store the intermediate batches.","title":"Implementation details"},{"location":"flare/streamtobatch/#outcomes","text":"","title":"Outcomes"},{"location":"flare/streamtobatch/#code-files","text":"dag : - name : cloudevents-data title : stream data to dataset description : This job ingests stream data to Iceberg table spec : stack : flare:1.0 flare : job : explain : true streaming : batchMode : true # to create batches triggerMode : Once #Once for cont checkpointLocation : dataos://icebase:raw01/checkpoints/cloudevents/ce01?acl=rw #triggerDuration: \"20 seconds\" inputs : - name : input_cloudevents dataset : dataos://kafka:default/cloudevents?acl=r schemaRegistryUrl : http://schema-registry.caretaker:8081 isStream : false options : startingOffsets : earliest logLevel : INFO outputs : - name : output01 depot : dataos://icebase:sys01?acl=rw # checkpointLocation: dataos://icebase:raw01/checkpoints/cloudevents/cedev01 steps : - sequence : - name : cloudevents sql : SELECT *, time as _timestamp FROM input_cloudevents functions : - name : set_type columns : _timestamp : timestamp time : timestamp sink : - sequenceName : cloudevents datasetName : cloudevents outputName : output01 outputType : Iceberg title : cloudevents data description : cloudevents data outputOptions : saveMode : overwrite # extraOptions: # fanout-enabled: \"true\" # triggerDuration: \"20 seconds\" iceberg : properties : write.metadata.previous-versions-max : \"10\" history.expire.max-snapshot-age-ms : \"7200000\" overwrite-mode : dynamic partitionSpec : - type : identity column : source - type : hour column : _timestamp name : hour tags : - Cloudevents","title":"Code files"},{"location":"governance/assertions/","text":"Assertions \u00b6 Sometimes input data sources contain invalid data or incorrect data. You need to validate the captured data to determine whether the data meets business requirements. It is critical for generating valuable, correct insights. Assertions are business-specific validation rules applied to test and evaluate the quality of specific datasets if they are appropriate for the intended purpose. DataOS allows you to define your own assertions with a combination of tests to check the rules. These tests are boolean expressions containing metric functions for aggregated data, such as the average sales price does not exceed some limit. DataOS automatically creates the metrics as per the function used while defining the assertions. You can also define assertions using SQL and regular expressions for more advanced use cases. Getting assertions summary/graph/trend on DataOS UI \u00b6 You can view the list of assertions created for the dataset to monitor the data quality and trends charts for each run. The trend charts also show whether the checks are passed or failed. Sign in to your DataOS instance with your username and password. On the DataOS Datanet page, search for the dataset for which you want to view the profile data. To display the dataset information, click on its name. On the Dataset information page, click on Quality . Creating assertions \u00b6 Provide the following: Property Value column column on which rule is to be defined filter criterion, to apply assertions on the resulting data based on the filter criterion sql sql statement for more complex custom logic that can be evaluated with computed columns regex regular expression if type is regex tests a boolean expression for each test. You can use quality metrics functions such as avg, max ,min etc. to define the rules with threshold values. These tests are expected to evaluate to true if the assertion should pass. Defining YAML \u00b6 The following is the sample YAML file to set up data quality assertions. --- version : v1beta1 name : mtrx-chks-odr-enr-01 type : workflow tags : - Metrics - Checks description : The job performs metrics calculations and checks on order enriched data #owner: itspiyush workflow : title : Metrics and checks dag : - name : metrics-chks-order-enrich title : Metrics and checks description : The job performs metrics calculations and checks on order enriched data spec : stack : flare:1.0 tags : - Metrics title : Metrics and checks description : The job performs metrics calculations and checks on order enriched data flare : driver : coreLimit : 3000m cores : 2 memory : 4000m executor : coreLimit : 6000m cores : 2 instances : 1 memory : 10000m job : explain : true logLevel : INFO #validate single input inputs : - name : source dataset : dataos://icebase:retail/orders_enriched format : iceberg #override outputs, steps with specific template assertions : - column : order_amount tests : - avg > 1000.00 - max < 1000 - max > 1000 - distinct_count > 100 - missing_count < 100 - missing_percentage < 0.5 - column : order_amount filter : brand_name == 'Urbane' tests : - avg > 500 - distinct_count > 100 - missing_count < 100 - column : brand_name validFormat : regex : Awkward tests : - invalid_count < 5 - invalid_percentage < 0.1 - sql : | SELECT AVG(order_amount) AS avg_order_amount, MAX(order_amount) AS max_order_amount FROM source where brand_name = 'Awkward Styles' tests : - avg_order_amount > 1000 - max_order_amount < 1000 Running quality checks job \u00b6 You can run the job defined with the assertions on-demand or schedule it for recurring runs if the data changes or is incremental. Follow the steps to submit the workflow to run Flare job on DataOS CLI . DataOS creates a summary/graphical view containing the results of the assertions. Quality metrics functions \u00b6 Function Description avg The AVG functions returns the average of a column avg_length The AVG_LENGTH function returns the average length of column value distinct_count The DISTINCT_COUNT function returns the count of the distinct values of a column duplicate_count The DUPLICATE_COUNT function returns the count of duplicate values in column. min The MIN function returns the minimum value of a column max The MAX function returns the maximum value of a column max_length The MAX_LENGTH function returns the maximum length of column value min_length The MIN_LENGTH function returns the minimum length of column value missing_count The MISSING_COUNT function returns the count of missing values in column missing_percentage The MISSING_PERCENTAGE function returns the rate of missing values in column sum The SUM function returns the total sum of column value","title":"Assertions"},{"location":"governance/assertions/#assertions","text":"Sometimes input data sources contain invalid data or incorrect data. You need to validate the captured data to determine whether the data meets business requirements. It is critical for generating valuable, correct insights. Assertions are business-specific validation rules applied to test and evaluate the quality of specific datasets if they are appropriate for the intended purpose. DataOS allows you to define your own assertions with a combination of tests to check the rules. These tests are boolean expressions containing metric functions for aggregated data, such as the average sales price does not exceed some limit. DataOS automatically creates the metrics as per the function used while defining the assertions. You can also define assertions using SQL and regular expressions for more advanced use cases.","title":"Assertions"},{"location":"governance/assertions/#getting-assertions-summarygraphtrend-on-dataos-ui","text":"You can view the list of assertions created for the dataset to monitor the data quality and trends charts for each run. The trend charts also show whether the checks are passed or failed. Sign in to your DataOS instance with your username and password. On the DataOS Datanet page, search for the dataset for which you want to view the profile data. To display the dataset information, click on its name. On the Dataset information page, click on Quality .","title":"Getting assertions summary/graph/trend on DataOS UI"},{"location":"governance/assertions/#creating-assertions","text":"Provide the following: Property Value column column on which rule is to be defined filter criterion, to apply assertions on the resulting data based on the filter criterion sql sql statement for more complex custom logic that can be evaluated with computed columns regex regular expression if type is regex tests a boolean expression for each test. You can use quality metrics functions such as avg, max ,min etc. to define the rules with threshold values. These tests are expected to evaluate to true if the assertion should pass.","title":"Creating assertions"},{"location":"governance/assertions/#defining-yaml","text":"The following is the sample YAML file to set up data quality assertions. --- version : v1beta1 name : mtrx-chks-odr-enr-01 type : workflow tags : - Metrics - Checks description : The job performs metrics calculations and checks on order enriched data #owner: itspiyush workflow : title : Metrics and checks dag : - name : metrics-chks-order-enrich title : Metrics and checks description : The job performs metrics calculations and checks on order enriched data spec : stack : flare:1.0 tags : - Metrics title : Metrics and checks description : The job performs metrics calculations and checks on order enriched data flare : driver : coreLimit : 3000m cores : 2 memory : 4000m executor : coreLimit : 6000m cores : 2 instances : 1 memory : 10000m job : explain : true logLevel : INFO #validate single input inputs : - name : source dataset : dataos://icebase:retail/orders_enriched format : iceberg #override outputs, steps with specific template assertions : - column : order_amount tests : - avg > 1000.00 - max < 1000 - max > 1000 - distinct_count > 100 - missing_count < 100 - missing_percentage < 0.5 - column : order_amount filter : brand_name == 'Urbane' tests : - avg > 500 - distinct_count > 100 - missing_count < 100 - column : brand_name validFormat : regex : Awkward tests : - invalid_count < 5 - invalid_percentage < 0.1 - sql : | SELECT AVG(order_amount) AS avg_order_amount, MAX(order_amount) AS max_order_amount FROM source where brand_name = 'Awkward Styles' tests : - avg_order_amount > 1000 - max_order_amount < 1000","title":"Defining YAML"},{"location":"governance/assertions/#running-quality-checks-job","text":"You can run the job defined with the assertions on-demand or schedule it for recurring runs if the data changes or is incremental. Follow the steps to submit the workflow to run Flare job on DataOS CLI . DataOS creates a summary/graphical view containing the results of the assertions.","title":"Running quality checks job"},{"location":"governance/assertions/#quality-metrics-functions","text":"Function Description avg The AVG functions returns the average of a column avg_length The AVG_LENGTH function returns the average length of column value distinct_count The DISTINCT_COUNT function returns the count of the distinct values of a column duplicate_count The DUPLICATE_COUNT function returns the count of duplicate values in column. min The MIN function returns the minimum value of a column max The MAX function returns the maximum value of a column max_length The MAX_LENGTH function returns the maximum length of column value min_length The MIN_LENGTH function returns the minimum length of column value missing_count The MISSING_COUNT function returns the count of missing values in column missing_percentage The MISSING_PERCENTAGE function returns the rate of missing values in column sum The SUM function returns the total sum of column value","title":"Quality metrics functions"},{"location":"governance/datagovernance/","text":"Data Governance in DataOS \u00b6 Data Governance is a framework that you can use to proactively manage organization data and ensure the quality and security of the data used across the organization. Data governance ensures a high quality of data management through all phases of the data lifecycle. It establishes a set of policies, processes and technologies that enforce the availability, usability, integrity, and security of all enterprise data. DataOS enables you to define clear policies for data usage, authorized access and to meet regulatory compliance requirements such as GDPR. This process encompasses the people, process, and technology that is required to ensure that data is fit for its intended purpose. Data Governance deals with the following: Data Ownership DataOS Metis will catalog all Datasource, Datastream, Dataset and Datasinks, and track their ownership over time. Transitive ownerships will be AUTOMATICALLY applied from the Jobs responsible for the Dataset. So, If John D ran a job Enrich Txn With Product which created a Datastream enriched_txn_with_product then John D automatically becomes one of the owners for the Datastream enriched_txn_with_product. Ownership can also be manually curated on DataOS UI Interface. Data Quality Users can use data catalog engine (Metis) to know about their own data and its landscape like quality, profile, lineage, and dictionary, etc. Data Access Metadata of all datasets will be available for anyone in the organisation to consume. DataOS will restrict access only at the Dataset level. You can either read the Dataset or not. You can always create new datasets with filtered columns and grant separate access of them.","title":"Introduction"},{"location":"governance/datagovernance/#data-governance-in-dataos","text":"Data Governance is a framework that you can use to proactively manage organization data and ensure the quality and security of the data used across the organization. Data governance ensures a high quality of data management through all phases of the data lifecycle. It establishes a set of policies, processes and technologies that enforce the availability, usability, integrity, and security of all enterprise data. DataOS enables you to define clear policies for data usage, authorized access and to meet regulatory compliance requirements such as GDPR. This process encompasses the people, process, and technology that is required to ensure that data is fit for its intended purpose. Data Governance deals with the following: Data Ownership DataOS Metis will catalog all Datasource, Datastream, Dataset and Datasinks, and track their ownership over time. Transitive ownerships will be AUTOMATICALLY applied from the Jobs responsible for the Dataset. So, If John D ran a job Enrich Txn With Product which created a Datastream enriched_txn_with_product then John D automatically becomes one of the owners for the Datastream enriched_txn_with_product. Ownership can also be manually curated on DataOS UI Interface. Data Quality Users can use data catalog engine (Metis) to know about their own data and its landscape like quality, profile, lineage, and dictionary, etc. Data Access Metadata of all datasets will be available for anyone in the organisation to consume. DataOS will restrict access only at the Dataset level. You can either read the Dataset or not. You can always create new datasets with filtered columns and grant separate access of them.","title":"Data Governance in DataOS"},{"location":"governance/dataprofile/","text":"Data profiling in DataOS \u00b6 What is data profiling? \u00b6 Data profiling is the process of assessing the quality and structure of the data in the dataset. It examines source data to determine the accuracy, completeness, and validity and summarizes information about that data. Why data profiling? \u00b6 It is crucial to know the completeness and correctness of the data to take full advantage of the value and usefulness of the source data available. Inaccurate and incomplete data used for the analysis can lead to incorrect data-driven decisions for any organization. Analysis and assessment algorithms incorporated in data profiling tools will provide insights into what potential issues exist within a dataset. You can assess your source data quality before using it in critical business scenarios and verify that data columns in your dataset are populated with the right kind of data. How data profiling tool works? \u00b6 Data profiling tool analyzes the structure, content, and relationships within data to uncover patterns and rules, inconsistencies, anomalies, and redundancies to achieve higher data quality. It uses basic statistics to know about the validity of the data. Structure discovery: This validates that your data is consistent and formatted correctly. Structure discovery examines the patterns in the data. For example, pattern matching helps you find the valid formats within the data set if your data contains phone numbers. It also uses basic statistics like the minimum and maximum values, means, medians, modes, and standard deviations to gain insight into the validity of the data. Content discovery: This looks more closely into the individual elements of the dataset to check data quality. It can help you find entries in your dataset that contain null values or values that are incorrect or ambiguous. For example, if there are phone numbers with no area code. Relationship discovery: This discovers how the data columns are interrelated/interdependent/associated. Getting profile data on DataOS UI \u00b6 Sign in to your DataOS instance with your username and password. On the DataOS Datanet page, search for the dataset for which you want to view the profile data. To display the dataset information, click on its name. On the Dataset information page, click on Profile . You will get the following profile data and chart: Summary Correlation matrix Tabular profile data Summary \u00b6 DataOS enables you to have an overview of the data in the profile report. The following information is generated as a summary: Job run details such as job name, who run the job, date & time Sample selection filter - click on the link to see the applied filter (if any) to get the sample data for profiling Query - click on the link to see the generated query Number of rows analyzed Number of columns analyzed Number of columns with type mismatches Number of columns with complete data and names of those columns Number of columns with incomplete data and names of those columns Correlation matrix \u00b6 Data profiling generates a correlation matrix, a table showing correlation between data elements(essentially having numerical values)using color gradients. It helps in quick visual analysis. You can explore the association between two variables and makes inferences about the strength of the relationship. You can discover uncommon associations using this matrix that can help you going ahead with further exploration of data. Tabular profile data \u00b6 DataOS also allows you to have more detailed view and to drill down the data in the profile report. You will get the following information in the tabular form. General Property Value Column Data type of the column, length of the data in the column//check Unique(% / value) Uniqueness Percentage/ Unique count Distinct Number of distinct patterns observed Completeness Number and percentage of records with a null value Statistics Statistical Measures Description Min The smallest mathematical value in the data set Max The largest mathematical value in the data set Mean Average value of a distribution Mode The most frequent value Median Middle value in a sorted set Standard Deviation How much the members of a group differ from the mean value for the group. This is very useful in finding an outliers histogram. Outliers are the abnormal distance from the group, the occurrence of these numbers are uncommon Skewness Measure of symmetry, or more precisely, the lack of symmetry. A data set is symmetric if it looks same to the left and right of the centre point (mean) Coefficient of variation The coefficient of variation shows the extent of variability of data in a sample in relation to the mean of the population First quartile The lower quartile, or first quartile, is denoted as Q1 and is the middle number that falls between the smallest value of the dataset and the median Second quartile The second quartile (Q2) is the median of a data set; thus 50% of the data lies below this point Interpretation of profile data \u00b6 The output from any data profiling job needs to be interpreted before it is useful. Use cases/examples \u00b6 Performing data profiling \u00b6 Data profiling can be done on the entire dataset or a sample /filtered data. You can filter the data by applying appropriate rules. Data profiling is a quite computationally intensive in terms of resources so it is recommended to run it for a sample data in a dataset. Basic structure of profile job \u00b6 Define a YAML file for your prfiling job. To learn more about defining a YAML file, refer to Flare section. There are some additional properties that you need to define specifically for profiling job. Persistent Volume: This is mandatory to define the volume to store intermediate files generated in the process of profiling. The directory given here is predefined. persistentVolume : name : persistent-v directory : fides Filters: You may define the fiter criterion to reduce the data on which you want to perform profiling. if removed, profiling will be performed on the entire dataset. The filter criterion is like a where clause of your SQL query. profile : # basic | intermediate | advanced level : basic filters : - type : expression expression : \"gender='MALE'\" Input: Dataset, on which we are perofmring profiling, should be first in the input list. inputs : - name : profile_input_df dataset : dataos://set01:default/orders_enriched_01?acl=r format : hudi Note : One profile job can have only one dataset. YAML template of profile job \u00b6 version : v1beta1 name : prf-s-odr-er-01 type : workflow tags : - Fides - Offline Sales - has_profile workflow : title : Enriched Order Profiler dag : - name : p-s-odr-er-01 title : Order Enriched Data Profiler description : The job performs profiling on order enriched data spec : stack : flare:1.0 tier : system title : Order Enriched Data Profiler persistentVolume : # define volume name : persistent-v directory : fides flare : job : explain : true inputs : - name : profile_input_df dataset : dataos://icebase:set01/orders_enriched_01?acl=r # dataset name format : iceberg logLevel : WARN profile : # basic | intermediate | advanced level : basic filters : - type : expression expression : \"gender='MALE'\" # Filter expression Running profile job \u00b6 You can run the job for profiling on demand. Follow the steps to submit the workflow to run Flare job on DataOS CLI .","title":"Data Profiling"},{"location":"governance/dataprofile/#data-profiling-in-dataos","text":"","title":"Data profiling in DataOS"},{"location":"governance/dataprofile/#what-is-data-profiling","text":"Data profiling is the process of assessing the quality and structure of the data in the dataset. It examines source data to determine the accuracy, completeness, and validity and summarizes information about that data.","title":"What is data profiling?"},{"location":"governance/dataprofile/#why-data-profiling","text":"It is crucial to know the completeness and correctness of the data to take full advantage of the value and usefulness of the source data available. Inaccurate and incomplete data used for the analysis can lead to incorrect data-driven decisions for any organization. Analysis and assessment algorithms incorporated in data profiling tools will provide insights into what potential issues exist within a dataset. You can assess your source data quality before using it in critical business scenarios and verify that data columns in your dataset are populated with the right kind of data.","title":"Why data profiling?"},{"location":"governance/dataprofile/#how-data-profiling-tool-works","text":"Data profiling tool analyzes the structure, content, and relationships within data to uncover patterns and rules, inconsistencies, anomalies, and redundancies to achieve higher data quality. It uses basic statistics to know about the validity of the data. Structure discovery: This validates that your data is consistent and formatted correctly. Structure discovery examines the patterns in the data. For example, pattern matching helps you find the valid formats within the data set if your data contains phone numbers. It also uses basic statistics like the minimum and maximum values, means, medians, modes, and standard deviations to gain insight into the validity of the data. Content discovery: This looks more closely into the individual elements of the dataset to check data quality. It can help you find entries in your dataset that contain null values or values that are incorrect or ambiguous. For example, if there are phone numbers with no area code. Relationship discovery: This discovers how the data columns are interrelated/interdependent/associated.","title":"How data profiling tool works?"},{"location":"governance/dataprofile/#getting-profile-data-on-dataos-ui","text":"Sign in to your DataOS instance with your username and password. On the DataOS Datanet page, search for the dataset for which you want to view the profile data. To display the dataset information, click on its name. On the Dataset information page, click on Profile . You will get the following profile data and chart: Summary Correlation matrix Tabular profile data","title":"Getting profile data on DataOS UI"},{"location":"governance/dataprofile/#summary","text":"DataOS enables you to have an overview of the data in the profile report. The following information is generated as a summary: Job run details such as job name, who run the job, date & time Sample selection filter - click on the link to see the applied filter (if any) to get the sample data for profiling Query - click on the link to see the generated query Number of rows analyzed Number of columns analyzed Number of columns with type mismatches Number of columns with complete data and names of those columns Number of columns with incomplete data and names of those columns","title":"Summary"},{"location":"governance/dataprofile/#correlation-matrix","text":"Data profiling generates a correlation matrix, a table showing correlation between data elements(essentially having numerical values)using color gradients. It helps in quick visual analysis. You can explore the association between two variables and makes inferences about the strength of the relationship. You can discover uncommon associations using this matrix that can help you going ahead with further exploration of data.","title":"Correlation matrix"},{"location":"governance/dataprofile/#tabular-profile-data","text":"DataOS also allows you to have more detailed view and to drill down the data in the profile report. You will get the following information in the tabular form. General Property Value Column Data type of the column, length of the data in the column//check Unique(% / value) Uniqueness Percentage/ Unique count Distinct Number of distinct patterns observed Completeness Number and percentage of records with a null value Statistics Statistical Measures Description Min The smallest mathematical value in the data set Max The largest mathematical value in the data set Mean Average value of a distribution Mode The most frequent value Median Middle value in a sorted set Standard Deviation How much the members of a group differ from the mean value for the group. This is very useful in finding an outliers histogram. Outliers are the abnormal distance from the group, the occurrence of these numbers are uncommon Skewness Measure of symmetry, or more precisely, the lack of symmetry. A data set is symmetric if it looks same to the left and right of the centre point (mean) Coefficient of variation The coefficient of variation shows the extent of variability of data in a sample in relation to the mean of the population First quartile The lower quartile, or first quartile, is denoted as Q1 and is the middle number that falls between the smallest value of the dataset and the median Second quartile The second quartile (Q2) is the median of a data set; thus 50% of the data lies below this point","title":"Tabular profile data"},{"location":"governance/dataprofile/#interpretation-of-profile-data","text":"The output from any data profiling job needs to be interpreted before it is useful.","title":"Interpretation of profile data"},{"location":"governance/dataprofile/#use-casesexamples","text":"","title":"Use cases/examples"},{"location":"governance/dataprofile/#performing-data-profiling","text":"Data profiling can be done on the entire dataset or a sample /filtered data. You can filter the data by applying appropriate rules. Data profiling is a quite computationally intensive in terms of resources so it is recommended to run it for a sample data in a dataset.","title":"Performing data profiling"},{"location":"governance/dataprofile/#basic-structure-of-profile-job","text":"Define a YAML file for your prfiling job. To learn more about defining a YAML file, refer to Flare section. There are some additional properties that you need to define specifically for profiling job. Persistent Volume: This is mandatory to define the volume to store intermediate files generated in the process of profiling. The directory given here is predefined. persistentVolume : name : persistent-v directory : fides Filters: You may define the fiter criterion to reduce the data on which you want to perform profiling. if removed, profiling will be performed on the entire dataset. The filter criterion is like a where clause of your SQL query. profile : # basic | intermediate | advanced level : basic filters : - type : expression expression : \"gender='MALE'\" Input: Dataset, on which we are perofmring profiling, should be first in the input list. inputs : - name : profile_input_df dataset : dataos://set01:default/orders_enriched_01?acl=r format : hudi Note : One profile job can have only one dataset.","title":"Basic structure of profile job"},{"location":"governance/dataprofile/#yaml-template-of-profile-job","text":"version : v1beta1 name : prf-s-odr-er-01 type : workflow tags : - Fides - Offline Sales - has_profile workflow : title : Enriched Order Profiler dag : - name : p-s-odr-er-01 title : Order Enriched Data Profiler description : The job performs profiling on order enriched data spec : stack : flare:1.0 tier : system title : Order Enriched Data Profiler persistentVolume : # define volume name : persistent-v directory : fides flare : job : explain : true inputs : - name : profile_input_df dataset : dataos://icebase:set01/orders_enriched_01?acl=r # dataset name format : iceberg logLevel : WARN profile : # basic | intermediate | advanced level : basic filters : - type : expression expression : \"gender='MALE'\" # Filter expression","title":"YAML template of profile job"},{"location":"governance/dataprofile/#running-profile-job","text":"You can run the job for profiling on demand. Follow the steps to submit the workflow to run Flare job on DataOS CLI .","title":"Running profile job"},{"location":"governance/fingerprinting/","text":"Data Fingerprinting \u00b6 While you perform the data profiling, the min/max, standard deviation, uniqueness, correlation, etc., help you understand your data and even discover problems like missing and duplicate values. Fingerprinting analyzes the data to know that a column of data has a signature or a fingerprint. By examining the data values in a column, we can identify what type of data is there and determine what business terms or labels can be attached to this data. Examining the data values is important as it allows you to automate identifying sensitive data that needs to be secured or even just cataloged to comply with government regulations like the GDPR. For example, if data contains a column for a name, social security numbers, credit card numbers or phone numbers, you can tag this data as PII. To handle various types of sensitive data and information in increasingly large quantities, data fingerprinting provides a scalable technique for identifying, monitoring, and applying protective controls to data as it moves across the network. Fingerprinting examines the actual data values, whether via regular expression or value-based tagging. It is a powerful but time-consuming process for data monitoring and protection. For this reason, you need to carefully consider the sample amount of data to fingerprint. Fingerprinting report on DataOS UI \u00b6 Sign in to your DataOS instance with your username and password. On the DataOS Datanet page, search for the dataset for which you want to view the profile data. To display the dataset information, click on its name. On the Dataset information page, click on Fingerprints . You will get the following fingerprinting data. a. Column names for which fingerprinting is performed. b. Label showing the labels identified in the data. These labels are defined as system labels or according to business value. c. For each label, you will get an option to accept the label to tag fingerprint to column or discard the label. 4. Once you accept the label, you can view the tagged column in the dictionary. Click on Dictionary . 5. Click on the Pencil icon to get the option to remove this tag. You can also add more tags.","title":"Data Fingerprinting"},{"location":"governance/fingerprinting/#data-fingerprinting","text":"While you perform the data profiling, the min/max, standard deviation, uniqueness, correlation, etc., help you understand your data and even discover problems like missing and duplicate values. Fingerprinting analyzes the data to know that a column of data has a signature or a fingerprint. By examining the data values in a column, we can identify what type of data is there and determine what business terms or labels can be attached to this data. Examining the data values is important as it allows you to automate identifying sensitive data that needs to be secured or even just cataloged to comply with government regulations like the GDPR. For example, if data contains a column for a name, social security numbers, credit card numbers or phone numbers, you can tag this data as PII. To handle various types of sensitive data and information in increasingly large quantities, data fingerprinting provides a scalable technique for identifying, monitoring, and applying protective controls to data as it moves across the network. Fingerprinting examines the actual data values, whether via regular expression or value-based tagging. It is a powerful but time-consuming process for data monitoring and protection. For this reason, you need to carefully consider the sample amount of data to fingerprint.","title":"Data Fingerprinting"},{"location":"governance/fingerprinting/#fingerprinting-report-on-dataos-ui","text":"Sign in to your DataOS instance with your username and password. On the DataOS Datanet page, search for the dataset for which you want to view the profile data. To display the dataset information, click on its name. On the Dataset information page, click on Fingerprints . You will get the following fingerprinting data. a. Column names for which fingerprinting is performed. b. Label showing the labels identified in the data. These labels are defined as system labels or according to business value. c. For each label, you will get an option to accept the label to tag fingerprint to column or discard the label. 4. Once you accept the label, you can view the tagged column in the dictionary. Click on Dictionary . 5. Click on the Pencil icon to get the option to remove this tag. You can also add more tags.","title":"Fingerprinting report on DataOS UI"},{"location":"governance/metricschecks/","text":"Metrics and Checks \u00b6 DataOS allows you to define your own metrics to identify aspects of data that can be measured, such as average order amount, average age of buyers. These metrics are used in combination with checks through which you can validate data quality. Once metrics are defined using simple SQL or specifying column names, you can set checks to configure business-specific rules. These rules can be used to analyze and evaluate the quality of specific data sets if they are appropriate for the intended purpose. Getting metrics and checks on DataOS UI \u00b6 You can view the list of metrics and checks created for the dataset to monitor the data quality and trends charts for each run. The trend charts also show whether the checks are passed or failed. Sign in to your DataOS instance with your username and password. On the DataOS Datanet page, search for the dataset for which you want to view the profile data. To display the dataset information, click on its name. On the Dataset information page, click on Quality . Creating quality metrics \u00b6 Provide the following: Property Value name name of the metrics type data quality metrics function , for example sqltype, regex column column on which metrics is to be defined sql sql statement if type is sql regex regular expression if type is regex Setting up quality checks \u00b6 Provide the following: Property Value name name to refer the check operator data quality checks operator , for example less than, greater_than metric name metric on which checks to be defined threshold a value to check the criterion sql sql statement if operator is custom sql Defining YAML \u00b6 The following is the sample YAML file to Set up data quality metrics and checks. --- version : v1beta1 name : mtrx-chks-odr-enr-01 type : workflow tags : - Metrics - Checks description : The job performs metrics calculations and checks on order enriched data workflow : title : Metrics and checks dag : - name : metrics-chks-order-enrich title : Metrics and checks description : The job performs metrics calculations and checks on order enriched data spec : stack : flare:1.0 tags : - Metrics title : Metrics and checks description : The job performs metrics calculations and checks on order enriched data flare : job : explain : true logLevel : INFO # Single dataset in the input section inputs : - name : input_df dataset : dataos://icebase:set01/orders_enriched_01?acl=r metrics : - name : avg_order_amount type : column_avg column : order_amount - name : distinct_order_amount_count type : column_distinct_count column : order_amount - name : avg_order_amount_custom type : custom_sql sql : select avg(order_amount) from input_df - name : col_mth_ratio type : column_regex_match column : brand_name regex : Awkward Styles checks : - name : avg_order_amount_gt_1000 metricName : avg_order_amount operator : greater_than threshold : 1000.00 - name : avg_order_amount_lt_1000 metricName : avg_order_amount operator : less_than threshold : 1000.00 - name : avg_order_amount_lt_sum_order_amount operator : custom_sql sql : select avg(order_amount) < sum(order_amount) from input_df Running quality checks job \u00b6 You can run the job defined with the quality checks on-demand or schedule it for recurring runs if the data is changing or incremental. Follow the steps to submit the workflow to run Flare job on DataOS CLI . Quality metrics functions \u00b6 Function Description avg The AVG functions returns the average of a column. Snippet metrics : - name : avg_price type : avg column : price distinct_count \u00b6 Function Description distinct_count The DISTINCT_COUNT function returns the count of the distinct values of a column. Snippet metrics : - name : distinct_price_count type : distinct_count column : price sql \u00b6 Function Description sql The SQL function returns the output of sql query. SQL output must be an integer value. Snippet metrics : - name : avg_price_custom type : sql sql : select avg(price) from input_df Function Description regex_match The REGEX_MATCH function calculates the ratio of matching regex on a column. Snippet metrics : - name : regex_on_col type : regex_match column : product_name regex : Western Cowboy min \u00b6 Function Description min The MIN function returns the minimum value of a column. Snippet metrics : - name : min_price type : min column : price max \u00b6 Function Description max The MAX function returns the maximum value of a column. Snippet metrics : - name : max_price type : max column : price avg_length \u00b6 Function Description avg_length The AVG_LENGTH function returns the average length of column value. Snippet metrics : - name : avg_len_product_name type : avg_length column : product_name max_length \u00b6 Function Description max_length The MAX_LENGTH function returns the maximun length of column value. Snippet metrics : - name : max_len_product_name type : max_length column : product_name min_length \u00b6 Function Description min_length The MIN_LENGTH function returns the minimum length of column value. Snippet metrics : - name : min_len_product_name type : min_length column : product_name sum \u00b6 Function Description sum The SUM function returns the total sum of column value. Snippet metrics : - name : total_sale_price type : sum column : sale_price missing_count \u00b6 Function Description missing_count The MISSING_COUNT function returns the count of missing values in column. Snippet metrics : - name : missing_sale_price type : missing_count column : sale_price missing_percentage \u00b6 Function Description missing_percentage The MISSING_PERCENTAGE function returns the rate of missing values in column. Snippet metrics : - name : missing_rate_sale_price type : missing_percentage column : sale_price Quality checks operators \u00b6 greater_than \u00b6 Function Description greater_than The GREATER_THAN function compares the metric value is greater or not with given threshold. Snippet checks : - name : check001 metricName : avg_price operator : greater_than threshold : 1000.00 less_than \u00b6 Function Description less_than The LESS_THAN function compares the metric value is less or not with given threshold. Snippet checks : - name : check002 metricName : avg_price operator : less_than threshold : 1000.00 custom_sql \u00b6 Function Description custom_sql The CUSTOM_SQL function returns the output of sql query. Snippet checks : - name : check003 operator : custom_sql sql : select avg(sale_price) < sum(sale_price) from input_df","title":"Metrics and Checks"},{"location":"governance/metricschecks/#metrics-and-checks","text":"DataOS allows you to define your own metrics to identify aspects of data that can be measured, such as average order amount, average age of buyers. These metrics are used in combination with checks through which you can validate data quality. Once metrics are defined using simple SQL or specifying column names, you can set checks to configure business-specific rules. These rules can be used to analyze and evaluate the quality of specific data sets if they are appropriate for the intended purpose.","title":"Metrics and Checks"},{"location":"governance/metricschecks/#getting-metrics-and-checks-on-dataos-ui","text":"You can view the list of metrics and checks created for the dataset to monitor the data quality and trends charts for each run. The trend charts also show whether the checks are passed or failed. Sign in to your DataOS instance with your username and password. On the DataOS Datanet page, search for the dataset for which you want to view the profile data. To display the dataset information, click on its name. On the Dataset information page, click on Quality .","title":"Getting metrics and checks on DataOS UI"},{"location":"governance/metricschecks/#creating-quality-metrics","text":"Provide the following: Property Value name name of the metrics type data quality metrics function , for example sqltype, regex column column on which metrics is to be defined sql sql statement if type is sql regex regular expression if type is regex","title":"Creating quality metrics"},{"location":"governance/metricschecks/#setting-up-quality-checks","text":"Provide the following: Property Value name name to refer the check operator data quality checks operator , for example less than, greater_than metric name metric on which checks to be defined threshold a value to check the criterion sql sql statement if operator is custom sql","title":"Setting up quality checks"},{"location":"governance/metricschecks/#defining-yaml","text":"The following is the sample YAML file to Set up data quality metrics and checks. --- version : v1beta1 name : mtrx-chks-odr-enr-01 type : workflow tags : - Metrics - Checks description : The job performs metrics calculations and checks on order enriched data workflow : title : Metrics and checks dag : - name : metrics-chks-order-enrich title : Metrics and checks description : The job performs metrics calculations and checks on order enriched data spec : stack : flare:1.0 tags : - Metrics title : Metrics and checks description : The job performs metrics calculations and checks on order enriched data flare : job : explain : true logLevel : INFO # Single dataset in the input section inputs : - name : input_df dataset : dataos://icebase:set01/orders_enriched_01?acl=r metrics : - name : avg_order_amount type : column_avg column : order_amount - name : distinct_order_amount_count type : column_distinct_count column : order_amount - name : avg_order_amount_custom type : custom_sql sql : select avg(order_amount) from input_df - name : col_mth_ratio type : column_regex_match column : brand_name regex : Awkward Styles checks : - name : avg_order_amount_gt_1000 metricName : avg_order_amount operator : greater_than threshold : 1000.00 - name : avg_order_amount_lt_1000 metricName : avg_order_amount operator : less_than threshold : 1000.00 - name : avg_order_amount_lt_sum_order_amount operator : custom_sql sql : select avg(order_amount) < sum(order_amount) from input_df","title":"Defining YAML"},{"location":"governance/metricschecks/#running-quality-checks-job","text":"You can run the job defined with the quality checks on-demand or schedule it for recurring runs if the data is changing or incremental. Follow the steps to submit the workflow to run Flare job on DataOS CLI .","title":"Running quality checks job"},{"location":"governance/metricschecks/#quality-metrics-functions","text":"Function Description avg The AVG functions returns the average of a column. Snippet metrics : - name : avg_price type : avg column : price","title":"Quality metrics functions"},{"location":"governance/metricschecks/#distinct_count","text":"Function Description distinct_count The DISTINCT_COUNT function returns the count of the distinct values of a column. Snippet metrics : - name : distinct_price_count type : distinct_count column : price","title":"distinct_count"},{"location":"governance/metricschecks/#sql","text":"Function Description sql The SQL function returns the output of sql query. SQL output must be an integer value. Snippet metrics : - name : avg_price_custom type : sql sql : select avg(price) from input_df Function Description regex_match The REGEX_MATCH function calculates the ratio of matching regex on a column. Snippet metrics : - name : regex_on_col type : regex_match column : product_name regex : Western Cowboy","title":"sql"},{"location":"governance/metricschecks/#min","text":"Function Description min The MIN function returns the minimum value of a column. Snippet metrics : - name : min_price type : min column : price","title":"min"},{"location":"governance/metricschecks/#max","text":"Function Description max The MAX function returns the maximum value of a column. Snippet metrics : - name : max_price type : max column : price","title":"max"},{"location":"governance/metricschecks/#avg_length","text":"Function Description avg_length The AVG_LENGTH function returns the average length of column value. Snippet metrics : - name : avg_len_product_name type : avg_length column : product_name","title":"avg_length"},{"location":"governance/metricschecks/#max_length","text":"Function Description max_length The MAX_LENGTH function returns the maximun length of column value. Snippet metrics : - name : max_len_product_name type : max_length column : product_name","title":"max_length"},{"location":"governance/metricschecks/#min_length","text":"Function Description min_length The MIN_LENGTH function returns the minimum length of column value. Snippet metrics : - name : min_len_product_name type : min_length column : product_name","title":"min_length"},{"location":"governance/metricschecks/#sum","text":"Function Description sum The SUM function returns the total sum of column value. Snippet metrics : - name : total_sale_price type : sum column : sale_price","title":"sum"},{"location":"governance/metricschecks/#missing_count","text":"Function Description missing_count The MISSING_COUNT function returns the count of missing values in column. Snippet metrics : - name : missing_sale_price type : missing_count column : sale_price","title":"missing_count"},{"location":"governance/metricschecks/#missing_percentage","text":"Function Description missing_percentage The MISSING_PERCENTAGE function returns the rate of missing values in column. Snippet metrics : - name : missing_rate_sale_price type : missing_percentage column : sale_price","title":"missing_percentage"},{"location":"governance/metricschecks/#quality-checks-operators","text":"","title":"Quality checks operators"},{"location":"governance/metricschecks/#greater_than","text":"Function Description greater_than The GREATER_THAN function compares the metric value is greater or not with given threshold. Snippet checks : - name : check001 metricName : avg_price operator : greater_than threshold : 1000.00","title":"greater_than"},{"location":"governance/metricschecks/#less_than","text":"Function Description less_than The LESS_THAN function compares the metric value is less or not with given threshold. Snippet checks : - name : check002 metricName : avg_price operator : less_than threshold : 1000.00","title":"less_than"},{"location":"governance/metricschecks/#custom_sql","text":"Function Description custom_sql The CUSTOM_SQL function returns the output of sql query. Snippet checks : - name : check003 operator : custom_sql sql : select avg(sale_price) < sum(sale_price) from input_df","title":"custom_sql"},{"location":"governance/policies/","text":"Governance Policy in DataOS \u00b6 A DataOS governance policy is a set of rules that help safeguard data and establish standards for its access, use, and integrity. DataOS policies allow organizations to authorize users, employees, and third parties to access enterprise data in a manner that meets security, privacy, and compliance requirements. Definitions \u00b6 Term Description Subject Subject is a user or system identified by a token, that would like to perform a specific predicate on a specific object Predicate Predicate is an action that the subject would like to perform on the specific object Object Object is a target that the subject would like to perform the predicate on. This target can be an API path, a document, a file, or a column Metadata Metadata is an additional information that may be pertinent in the policy evaluation decision of access Policy Policy is a rule defining what tags are associated with subjects, predicates, tags or paths associated with objects, and optional additional conditions on metadata to allow or deny access to DataOS resource/environment Policy Decision Point (PDP) Policy Decision Point (PDP) is the service that evaluates a specific subject-predicate-object-metadata request against the current policies to determine if access to the DataOS resource/environment is allowed or denied Policy Execution Point (PEP) Policy Execution Point (PEP) is the executor of the decision that is returned from the PDP. This occurs within any application that requires a policy decision to be made about access. Example: kong proxy during ingress validates that specific api paths are allowed or denied and does not proxy the request if the PDP returns denied Types of policies \u00b6 There are two types of policies in DataOS, and both are managed through the artifact type of \u201cpolicy\u201d. Access policy Data policy Access policy \u00b6 Access policy is a security measure to regulate the individuals who can view, use, or access a restricted DataOS environment/resource. The Access policy type is implemented using an Attribute-Based Access Control paradigm. More specifically, we leverage the attribute tags of Subjects and the attribute tags or paths of Objects to evaluate a set of policies when determining if a specific Predicate(action) should be allowed or denied. For example, a user with a tag 'user' can access secrets or specific Depots to connect to data, but a user with an 'operator' tag can perform CRUD operations. Example 1 --- name : \"user-access-demo-depots\" version : v1beta1 type : policy layer : user description : \"policy allowing users to read from demo depots\" policy : access : subjects : tags : - - \"dataos:u:user\" predicates : - \"read\" objects : paths : - \"dataos://crmbq**\" - \"dataos://poss3**\" allow : true Example 2 name : \"user-access-demo-syn-depots\" version : v1beta1 type : policy layer : user description : \"policy allowing users to crud demo depots\" policy : access : subjects : tags : - - \"dataos:u:operator\" predicates : - \"create\" - \"read\" - \"update\" - \"delete\" objects : paths : - \"dataos://syndicationgcs**\" allow : true Data policy \u00b6 Data policies are a collection of statements that describes the rules controlling the integrity, security, quality, and use of data during its lifecycle and state change. You can create data policies to guide what data the user sees once they access a dataset. You can set up data policies in the follwoing two ways: Global - covers all the columns based on tags. Will only support column masking Local - covers columns of a specific dataset. Will support column masking and row level filters These policies selectively mask/filter data and provide multiple views for users and groups based on their access and visibility rules. Masking policy \u00b6 A data masking policy defines the logic that replaces(masks) the original sensitive data with fictitious data to maintain the privacy of sensitive data. For example, PII data can be shown with an appropriate mask, replaced with \"####\" string, or with some hash function. The following examples show what the masked data might look like after the masking policy is applied. Type Original Value Masked Value Email ID john.smith@gmail.com bkfgohrnrtseqq85@bkgiplpsrhsll16.com SSN 987654321 867-92-3415 Credit card number 8671 9211 3415 4546 #### #### #### #### Masking strategies \u00b6 Masking strategies can be defined in the YAML files and applied. These strategies may be simple or complex depending on the information security needs of the organization. Here is a list of operators/rules through which you can define masking definitions. Rule Type Description Hashing allow user to pick pre-defined strategies to mask column values Bucketing Numeric - based on the bucket size given by the user, the policy will define lower and upper bound for each bucket based on min & max of values of a column and replace the column value with the lower value of the bucket in which the value lies Date - based on the time bucket (hour/day/week/month) given by the user, the policy will replace the column value with the lower value of the time bucket Regex replace based on regex pattern defined by the user, the policy will replace the matching characters with the string given by user Format preservation based on the format pattern given by user, policy will replace the column values with random strings/numbers preserving the given format Redaction policy will simply redact values of the column based on its type: String - \u2018Redacted\u2019 Number - 0 Date - 01/01/1972 Object - empty Supported column types \u00b6 The checklist for support of masking strategy by column types is given below. Type Text Number Date Object Hashing Y N N N Bucketing N Y Y N Regex replace Y N N N Format preservation Y N N N Redaction Y Y Y N Note : You can override the masking strategies by using a special policy type- passs_through . This policy will allow access to the value of columns tagged with fingerprint pii. Example 1 --- name : demo-pii-hash version : v1beta1 type : policy layer : user description : \"data policy to hash pii columns by name\" owner : policy : data : type : mask priority : 99 selector : user : match : any tags : - \"dataos:u:user\" column : names : - \"first_name\" - \"last_name\" - \"email_id\" mask : # operator: redact operator : hash hash : algo : sha256 Example 2 --- name : pii-pass-through version : v1beta1 type : policy layer : user description : \"data policy to pass-through all finger-prints\" owner : policy : data : type : mask priority : 95 selector : user : match : any tags : - \"dataos:u:operator\" column : tags : - \"dataos:f:pii\" mask : operator : pass_through Example 3 name : demo-reader version : v1beta1 type : policy layer : user description : \"policy to allow users with a demo-reader tag to view\" policy : data : type : mask priority : 90 selector : user : match : any tags : - \"dataos:u:demo-reader\" column : names : - \"first_name\" - \"last_name\" mask : operator : pass_through Filtering policy \u00b6 The filtering policy constraints data visibility for end users. You can define a policy to remove rows from the query's result set based on comparison operators set on a column, such as some users cannot see ' Florida' region data. Filter policy can be defined in the YAML file and applied at the time of the query. Example --- name : filter-to-florida version : v1beta1 type : policy layer : user description : \"data policy to filter just FL data\" owner : policy : data : type : filter priority : 80 selector : user : match : any tags : - \"dataos:u:user\" filters : - column : store_state_code operator : not_equals value : FL depot : raw01 collection : public dataset : store_01 Creating policies \u00b6 DataOS implements a unique PDP based on tags associated with Subjects and Objects. This allows a very flexible approach to policy definition which complements the ever changing set of subjects and objects in a Data Fabric. While creating policy in the YAML file, provide various configuration properties, set permissions, and identify a priority for the policy. Rules \u00b6 AND/OR relationship Subjects are defined using one or more tags with AND/OR relationship. # tag1 OR tag2 - - tag1 - - tag2 # tag1 AND tag2 - - tag1 - tag2 # tag1 OR (tag2 AND tag3) - - tag1 - - tag2 - tag3 Example 1 In this example policy, a subject MUST have the (dataos:u:pii-reader AND dataos:u:user) tags attributed to qualify for this policy to apply. name : subject-example1 version : v1beta1 type : policy layer : user description : example policy policy : access : subjects : tags : - - dataos:u:pii-reader - dataos:u:user predicates : - read objects : tags : - - dataos:f:pii - dataos:type:column allow : true Example 2 In this example policy, a subject MUST have the (dataos:u:pii-reader AND dataos:u:user) OR dataos:u:marketing-manager tags attributed to qualify for this policy to apply. name : subject-example2 version : v1beta1 type : policy layer : user description : example policy policy : access : subjects : tags : - - dataos:u:pii-reader - dataos:u:user - - dataos:u:marketing-manager predicates : - read objects : tags : - - dataos:f:pii - dataos:type:column allow : true 2. Predicates are the string array of actions that the policy will apply to. Predicates are OR relationships only since the PEP is authorizing one action at a time. In this example policy, a predicate MUST be read OR write from the PEP to qualify for this policy to apply. name : predicate-example2 version : v1beta1 type : policy layer : user description : example policy policy : access : subjects : tags : - - dataos:u:pii-reader - dataos:u:user - - dataos:u:marketing-manager predicates : - read - write objects : tags : - - dataos:f:pii - dataos:type:column allow : true 3. Objects are defined on resource paths or using one or more tags that must be an attribute of the requested object. In this example policy, an object MUST have the resource path of /metis/api/v2/workspaces/public OR /metis/api/v2/workspaces/sandbox to qualify for this policy to apply. Example 1 name : object-example1 version : v1beta1 type : policy layer : user description : example policy policy : access : subjects : tags : - - dataos:u:developer - dataos:u:user predicates : - read objects : paths : - /metis/api/v2/workspaces/public - /metis/api/v2/workspaces/sandbox allow : true Example 2 In this example policy, an object MUST have the dataos:f:pii OR dataos:f:sensitive tags attributed to qualify for this policy to apply. name : object-example2 version : v1beta1 type : policy layer : user description : example policy policy : access : subjects : tags : - - dataos:u:developer - dataos:u:user predicates : - read objects : tags : - - dataos:f:pii - - dataos:f:sensitive allow : true Policy enforcement \u00b6 DataOS Heimdall is the Access Control Gatekeeper for all the various touchpoints. The Heimdall application has the following capabilities: Policy Decision Point User and Policy Tag Management User Management Policy Management Token and Secret Provider Getting policy details on DataOS UI \u00b6 Sign in to your DataOS instance with your username and password. On the DataOS Datanet page, click on Policies . For Access policies, Click on Access Policy . Note : You must have 'Operator' previleges to access this information. The following information appears on the screen. Field Description Allow/Deny Permission to access the resource Subject User/system who wants permission to perform the action Predicate Action to be performed such as read, write,get post Object Target to perform action on based on the tags For Masking policies, Click on Mask Policy . The following information appears on the screen. Field Description Priority Permission to access the resource Depot Depot on which access permission is given, ** indicates all depots Collection Collection on which access permission is given, ** indicates all collections Dataset Dataset on which access permission is given, ** indicates all datasets Columns Columns on which masking strategy is applied Name Policy name Formula Formula for masking strategy Users Users or tags who have this policy applied Desc Short description of the policy defined For example, a default PII policy is defined to protect sensitive information in your data. So, any column of any collection and dataset with dataos:f:pii tag is not visible to any user with tag dataos-user (default tag for all logged-in users) as the defined policy hashes the column values. But sometimes, you want some of the users to access this information. So the other policy is defined to override the default PII policy. Any column of any collection and dataset with the tag dataos:f:pii is visible to the user who has dataos:u:pii-reader tag. For Filtering policies, Click on Filter Policy . The following information appears on the screen. Field Description Priority Permission to access the resource Depot Name of the Depot Collection Name of the collection Dataset Name of the dataset Name Policy name Formula Formula for filter criterion Users Users or tags who has this policy applied Desc Short description of the policy defined","title":"Policies"},{"location":"governance/policies/#governance-policy-in-dataos","text":"A DataOS governance policy is a set of rules that help safeguard data and establish standards for its access, use, and integrity. DataOS policies allow organizations to authorize users, employees, and third parties to access enterprise data in a manner that meets security, privacy, and compliance requirements.","title":"Governance Policy in DataOS"},{"location":"governance/policies/#definitions","text":"Term Description Subject Subject is a user or system identified by a token, that would like to perform a specific predicate on a specific object Predicate Predicate is an action that the subject would like to perform on the specific object Object Object is a target that the subject would like to perform the predicate on. This target can be an API path, a document, a file, or a column Metadata Metadata is an additional information that may be pertinent in the policy evaluation decision of access Policy Policy is a rule defining what tags are associated with subjects, predicates, tags or paths associated with objects, and optional additional conditions on metadata to allow or deny access to DataOS resource/environment Policy Decision Point (PDP) Policy Decision Point (PDP) is the service that evaluates a specific subject-predicate-object-metadata request against the current policies to determine if access to the DataOS resource/environment is allowed or denied Policy Execution Point (PEP) Policy Execution Point (PEP) is the executor of the decision that is returned from the PDP. This occurs within any application that requires a policy decision to be made about access. Example: kong proxy during ingress validates that specific api paths are allowed or denied and does not proxy the request if the PDP returns denied","title":"Definitions"},{"location":"governance/policies/#types-of-policies","text":"There are two types of policies in DataOS, and both are managed through the artifact type of \u201cpolicy\u201d. Access policy Data policy","title":"Types of policies"},{"location":"governance/policies/#access-policy","text":"Access policy is a security measure to regulate the individuals who can view, use, or access a restricted DataOS environment/resource. The Access policy type is implemented using an Attribute-Based Access Control paradigm. More specifically, we leverage the attribute tags of Subjects and the attribute tags or paths of Objects to evaluate a set of policies when determining if a specific Predicate(action) should be allowed or denied. For example, a user with a tag 'user' can access secrets or specific Depots to connect to data, but a user with an 'operator' tag can perform CRUD operations. Example 1 --- name : \"user-access-demo-depots\" version : v1beta1 type : policy layer : user description : \"policy allowing users to read from demo depots\" policy : access : subjects : tags : - - \"dataos:u:user\" predicates : - \"read\" objects : paths : - \"dataos://crmbq**\" - \"dataos://poss3**\" allow : true Example 2 name : \"user-access-demo-syn-depots\" version : v1beta1 type : policy layer : user description : \"policy allowing users to crud demo depots\" policy : access : subjects : tags : - - \"dataos:u:operator\" predicates : - \"create\" - \"read\" - \"update\" - \"delete\" objects : paths : - \"dataos://syndicationgcs**\" allow : true","title":"Access policy"},{"location":"governance/policies/#data-policy","text":"Data policies are a collection of statements that describes the rules controlling the integrity, security, quality, and use of data during its lifecycle and state change. You can create data policies to guide what data the user sees once they access a dataset. You can set up data policies in the follwoing two ways: Global - covers all the columns based on tags. Will only support column masking Local - covers columns of a specific dataset. Will support column masking and row level filters These policies selectively mask/filter data and provide multiple views for users and groups based on their access and visibility rules.","title":"Data policy"},{"location":"governance/policies/#masking-policy","text":"A data masking policy defines the logic that replaces(masks) the original sensitive data with fictitious data to maintain the privacy of sensitive data. For example, PII data can be shown with an appropriate mask, replaced with \"####\" string, or with some hash function. The following examples show what the masked data might look like after the masking policy is applied. Type Original Value Masked Value Email ID john.smith@gmail.com bkfgohrnrtseqq85@bkgiplpsrhsll16.com SSN 987654321 867-92-3415 Credit card number 8671 9211 3415 4546 #### #### #### ####","title":"Masking policy"},{"location":"governance/policies/#masking-strategies","text":"Masking strategies can be defined in the YAML files and applied. These strategies may be simple or complex depending on the information security needs of the organization. Here is a list of operators/rules through which you can define masking definitions. Rule Type Description Hashing allow user to pick pre-defined strategies to mask column values Bucketing Numeric - based on the bucket size given by the user, the policy will define lower and upper bound for each bucket based on min & max of values of a column and replace the column value with the lower value of the bucket in which the value lies Date - based on the time bucket (hour/day/week/month) given by the user, the policy will replace the column value with the lower value of the time bucket Regex replace based on regex pattern defined by the user, the policy will replace the matching characters with the string given by user Format preservation based on the format pattern given by user, policy will replace the column values with random strings/numbers preserving the given format Redaction policy will simply redact values of the column based on its type: String - \u2018Redacted\u2019 Number - 0 Date - 01/01/1972 Object - empty","title":"Masking strategies"},{"location":"governance/policies/#supported-column-types","text":"The checklist for support of masking strategy by column types is given below. Type Text Number Date Object Hashing Y N N N Bucketing N Y Y N Regex replace Y N N N Format preservation Y N N N Redaction Y Y Y N Note : You can override the masking strategies by using a special policy type- passs_through . This policy will allow access to the value of columns tagged with fingerprint pii. Example 1 --- name : demo-pii-hash version : v1beta1 type : policy layer : user description : \"data policy to hash pii columns by name\" owner : policy : data : type : mask priority : 99 selector : user : match : any tags : - \"dataos:u:user\" column : names : - \"first_name\" - \"last_name\" - \"email_id\" mask : # operator: redact operator : hash hash : algo : sha256 Example 2 --- name : pii-pass-through version : v1beta1 type : policy layer : user description : \"data policy to pass-through all finger-prints\" owner : policy : data : type : mask priority : 95 selector : user : match : any tags : - \"dataos:u:operator\" column : tags : - \"dataos:f:pii\" mask : operator : pass_through Example 3 name : demo-reader version : v1beta1 type : policy layer : user description : \"policy to allow users with a demo-reader tag to view\" policy : data : type : mask priority : 90 selector : user : match : any tags : - \"dataos:u:demo-reader\" column : names : - \"first_name\" - \"last_name\" mask : operator : pass_through","title":"Supported column types"},{"location":"governance/policies/#filtering-policy","text":"The filtering policy constraints data visibility for end users. You can define a policy to remove rows from the query's result set based on comparison operators set on a column, such as some users cannot see ' Florida' region data. Filter policy can be defined in the YAML file and applied at the time of the query. Example --- name : filter-to-florida version : v1beta1 type : policy layer : user description : \"data policy to filter just FL data\" owner : policy : data : type : filter priority : 80 selector : user : match : any tags : - \"dataos:u:user\" filters : - column : store_state_code operator : not_equals value : FL depot : raw01 collection : public dataset : store_01","title":"Filtering policy"},{"location":"governance/policies/#creating-policies","text":"DataOS implements a unique PDP based on tags associated with Subjects and Objects. This allows a very flexible approach to policy definition which complements the ever changing set of subjects and objects in a Data Fabric. While creating policy in the YAML file, provide various configuration properties, set permissions, and identify a priority for the policy.","title":"Creating policies"},{"location":"governance/policies/#rules","text":"AND/OR relationship Subjects are defined using one or more tags with AND/OR relationship. # tag1 OR tag2 - - tag1 - - tag2 # tag1 AND tag2 - - tag1 - tag2 # tag1 OR (tag2 AND tag3) - - tag1 - - tag2 - tag3 Example 1 In this example policy, a subject MUST have the (dataos:u:pii-reader AND dataos:u:user) tags attributed to qualify for this policy to apply. name : subject-example1 version : v1beta1 type : policy layer : user description : example policy policy : access : subjects : tags : - - dataos:u:pii-reader - dataos:u:user predicates : - read objects : tags : - - dataos:f:pii - dataos:type:column allow : true Example 2 In this example policy, a subject MUST have the (dataos:u:pii-reader AND dataos:u:user) OR dataos:u:marketing-manager tags attributed to qualify for this policy to apply. name : subject-example2 version : v1beta1 type : policy layer : user description : example policy policy : access : subjects : tags : - - dataos:u:pii-reader - dataos:u:user - - dataos:u:marketing-manager predicates : - read objects : tags : - - dataos:f:pii - dataos:type:column allow : true 2. Predicates are the string array of actions that the policy will apply to. Predicates are OR relationships only since the PEP is authorizing one action at a time. In this example policy, a predicate MUST be read OR write from the PEP to qualify for this policy to apply. name : predicate-example2 version : v1beta1 type : policy layer : user description : example policy policy : access : subjects : tags : - - dataos:u:pii-reader - dataos:u:user - - dataos:u:marketing-manager predicates : - read - write objects : tags : - - dataos:f:pii - dataos:type:column allow : true 3. Objects are defined on resource paths or using one or more tags that must be an attribute of the requested object. In this example policy, an object MUST have the resource path of /metis/api/v2/workspaces/public OR /metis/api/v2/workspaces/sandbox to qualify for this policy to apply. Example 1 name : object-example1 version : v1beta1 type : policy layer : user description : example policy policy : access : subjects : tags : - - dataos:u:developer - dataos:u:user predicates : - read objects : paths : - /metis/api/v2/workspaces/public - /metis/api/v2/workspaces/sandbox allow : true Example 2 In this example policy, an object MUST have the dataos:f:pii OR dataos:f:sensitive tags attributed to qualify for this policy to apply. name : object-example2 version : v1beta1 type : policy layer : user description : example policy policy : access : subjects : tags : - - dataos:u:developer - dataos:u:user predicates : - read objects : tags : - - dataos:f:pii - - dataos:f:sensitive allow : true","title":"Rules"},{"location":"governance/policies/#policy-enforcement","text":"DataOS Heimdall is the Access Control Gatekeeper for all the various touchpoints. The Heimdall application has the following capabilities: Policy Decision Point User and Policy Tag Management User Management Policy Management Token and Secret Provider","title":"Policy enforcement"},{"location":"governance/policies/#getting-policy-details-on-dataos-ui","text":"Sign in to your DataOS instance with your username and password. On the DataOS Datanet page, click on Policies . For Access policies, Click on Access Policy . Note : You must have 'Operator' previleges to access this information. The following information appears on the screen. Field Description Allow/Deny Permission to access the resource Subject User/system who wants permission to perform the action Predicate Action to be performed such as read, write,get post Object Target to perform action on based on the tags For Masking policies, Click on Mask Policy . The following information appears on the screen. Field Description Priority Permission to access the resource Depot Depot on which access permission is given, ** indicates all depots Collection Collection on which access permission is given, ** indicates all collections Dataset Dataset on which access permission is given, ** indicates all datasets Columns Columns on which masking strategy is applied Name Policy name Formula Formula for masking strategy Users Users or tags who have this policy applied Desc Short description of the policy defined For example, a default PII policy is defined to protect sensitive information in your data. So, any column of any collection and dataset with dataos:f:pii tag is not visible to any user with tag dataos-user (default tag for all logged-in users) as the defined policy hashes the column values. But sometimes, you want some of the users to access this information. So the other policy is defined to override the default PII policy. Any column of any collection and dataset with the tag dataos:f:pii is visible to the user who has dataos:u:pii-reader tag. For Filtering policies, Click on Filter Policy . The following information appears on the screen. Field Description Priority Permission to access the resource Depot Name of the Depot Collection Name of the collection Dataset Name of the dataset Name Policy name Formula Formula for filter criterion Users Users or tags who has this policy applied Desc Short description of the policy defined","title":"Getting policy details on DataOS UI"},{"location":"integrations/powerbi/","text":"DataOS Integration with Power BI \u00b6 This article will help you to set up the connection between DataOS and Power BI. It provides specific steps needed to fetch data from DataOS into Power BI. Explore your DataOS data with Power BI \u00b6 Power BI is a business analytics service to provide interactive visualizations and business intelligence capabilities. Using Power BI\u2019s simple user interface, you can create your reports and dashboards. DataOS and Power BI seamless integration works to take advantage of that powerful visualization technology, on the data pulled from DataOS. Requirements \u00b6 Power BI Desktop installed on your system - If Power BI is not installed on your system, you can download the latest version from the Power BI website. Simba Presto ODBC Driver - In order to connect to DataOS Catalog, you would have to install this Presto driver. DataOS API token - To authenticate and access DataOS, you will need API token. Download and install Presto driver \u00b6 Download it from Presto ODBC & JDBC Drivers download page . 2. To run the installer, double-click on downloaded Simba Presto XX-bit installer file. Select the 32 or 64 bit version according to your system configurations. Follow the steps for the successful installation. a. Click Next . b. Select the check box to accept the terms of the License Agreement if you agree, and then click Next . c. To change the installation location, click Change, then browse to the desired folder, and then click OK . To accept the installation location, click Next . d. Click Install . e. When the installation completes, click Finish . 3. After successful installation, copy the license file (that you have received in your email) into the \\lib subfolder of the Simba installation folder. Note : Contact your network administrator in case you encounter an error due to not having required admin priviliges. Generate DataOS API Token \u00b6 Sign in to your DataOS instance with your username and password. On the DataOS home page, click on Profile . On the 'Profile' page, click on Tokens . Click on the Add API Key link on the Tokens tab. Type in name for this token and also set the validity period of your token based on the security requirements as per your business needs. Click Save to create one for you. The API key is listed below. Click on the \u201ceye icon\u201d on the right side to make the full API key visible. Click on the APIkey to copy it. You need this API key to configure Simba Presto driver. Configure Presto ODBC DSN \u00b6 To use the Simba Presto ODBC Driver in Power BI application, you need to configure a Data Source Name (DSN) to connect to your data in DataOS. Open ODBC Data Source Administrator (64-bit or 32-bit). Click System DSN tab. In the list of DSNs, select Simba Presto ODBC DSN, and then click Configure . 4. In the DSN Setup dialog box, provide the following inputs: Provide 'Description' for data source name. In the 'Authentication' section Select Authentication type as No Authentication Enter generated API key as username Now in the 'Data Source' section, provide required information. Host (e.g reasonably-welcome-grub.dataos.io) Port (e.g 7432) Catalog (e.g icebase) Schema (optional) 5. In the DSN Setup dialog box, click SSL Options and enable SSL. 6. Click Test . and if successful, press OK to close the Test Results dialog box. 7.Click OK to save your DSN. Note : If you encounter any error in setting up the connection, please check DataOS url, validity of API key and try again or contact your administrator. Access DataOS on Power BI \u00b6 Launch Power BI. Click on the Get Data option in top menu bar and click More . Search for the ODBC from the data source list, then select ODBC and click on Connect `. Select the DSN you tested successfully during the DSN setup process, and click OK Enter API key for username and password both in the dialogue box and click on Connect . On successful connection, you can see the DataOS catalog in the left panel. Select the schema and table and click Load . Now you can explore and visualize this data in Power BI.","title":"PowerBI"},{"location":"integrations/powerbi/#dataos-integration-with-power-bi","text":"This article will help you to set up the connection between DataOS and Power BI. It provides specific steps needed to fetch data from DataOS into Power BI.","title":"DataOS Integration with Power BI"},{"location":"integrations/powerbi/#explore-your-dataos-data-with-power-bi","text":"Power BI is a business analytics service to provide interactive visualizations and business intelligence capabilities. Using Power BI\u2019s simple user interface, you can create your reports and dashboards. DataOS and Power BI seamless integration works to take advantage of that powerful visualization technology, on the data pulled from DataOS.","title":"Explore your DataOS data with Power BI"},{"location":"integrations/powerbi/#requirements","text":"Power BI Desktop installed on your system - If Power BI is not installed on your system, you can download the latest version from the Power BI website. Simba Presto ODBC Driver - In order to connect to DataOS Catalog, you would have to install this Presto driver. DataOS API token - To authenticate and access DataOS, you will need API token.","title":"Requirements"},{"location":"integrations/powerbi/#download-and-install-presto-driver","text":"Download it from Presto ODBC & JDBC Drivers download page . 2. To run the installer, double-click on downloaded Simba Presto XX-bit installer file. Select the 32 or 64 bit version according to your system configurations. Follow the steps for the successful installation. a. Click Next . b. Select the check box to accept the terms of the License Agreement if you agree, and then click Next . c. To change the installation location, click Change, then browse to the desired folder, and then click OK . To accept the installation location, click Next . d. Click Install . e. When the installation completes, click Finish . 3. After successful installation, copy the license file (that you have received in your email) into the \\lib subfolder of the Simba installation folder. Note : Contact your network administrator in case you encounter an error due to not having required admin priviliges.","title":"Download and install Presto driver"},{"location":"integrations/powerbi/#generate-dataos-api-token","text":"Sign in to your DataOS instance with your username and password. On the DataOS home page, click on Profile . On the 'Profile' page, click on Tokens . Click on the Add API Key link on the Tokens tab. Type in name for this token and also set the validity period of your token based on the security requirements as per your business needs. Click Save to create one for you. The API key is listed below. Click on the \u201ceye icon\u201d on the right side to make the full API key visible. Click on the APIkey to copy it. You need this API key to configure Simba Presto driver.","title":"Generate DataOS API Token"},{"location":"integrations/powerbi/#configure-presto-odbc-dsn","text":"To use the Simba Presto ODBC Driver in Power BI application, you need to configure a Data Source Name (DSN) to connect to your data in DataOS. Open ODBC Data Source Administrator (64-bit or 32-bit). Click System DSN tab. In the list of DSNs, select Simba Presto ODBC DSN, and then click Configure . 4. In the DSN Setup dialog box, provide the following inputs: Provide 'Description' for data source name. In the 'Authentication' section Select Authentication type as No Authentication Enter generated API key as username Now in the 'Data Source' section, provide required information. Host (e.g reasonably-welcome-grub.dataos.io) Port (e.g 7432) Catalog (e.g icebase) Schema (optional) 5. In the DSN Setup dialog box, click SSL Options and enable SSL. 6. Click Test . and if successful, press OK to close the Test Results dialog box. 7.Click OK to save your DSN. Note : If you encounter any error in setting up the connection, please check DataOS url, validity of API key and try again or contact your administrator.","title":"Configure Presto ODBC DSN"},{"location":"integrations/powerbi/#access-dataos-on-power-bi","text":"Launch Power BI. Click on the Get Data option in top menu bar and click More . Search for the ODBC from the data source list, then select ODBC and click on Connect `. Select the DSN you tested successfully during the DSN setup process, and click OK Enter API key for username and password both in the dialogue box and click on Connect . On successful connection, you can see the DataOS catalog in the left panel. Select the schema and table and click Load . Now you can explore and visualize this data in Power BI.","title":"Access DataOS on Power BI"},{"location":"integrations/powerbiold/","text":"Integrate with Power BI \u00b6 Purpose: \u00b6 Power BI business analytics solution comprises several products and services, and value and versatility comes from leveraging the individual elements, and taking advantage of how they work together. The following text streamlines the integration with Power BI, to make it quick and straightforward. Download driver: \u00b6 Download and install Simba Presto ODBC Driver from the following link https://www.simba.com/drivers/presto-odbc-jdbc/ Install Simba Presto ODBC driver \u00b6 a) Double-click the downloaded driver file to start the installation process, click Next and follow the instructions. b) After successful installation, copy the license file (that you must have received in your email) into the \\lib subfolder of the installation folder you selected above. In case you encounter an error, you may not have the required admin priviliges. Contact your network administrator to accomplish this. Configure ODBC DSN \u00b6 After successful installation, proceed as follows to configure ODBC DSN: a) ODBC Data Sources (64-bit) Go to ODBC Sources Administrator(64 bit or 32 bit) and open it. b) In the dialogue box select System DSN tab c) Now click Add and select driver name which you entered during driver installation process and click Finish d) A new dialogue box opens up for ODBC driver DSN setup In the DSN Setup section, provide Data Source Name and 'Description' In the Authentication section, Select Authentication type as No Authentication Enter username and the system generated login key Now in the Data Source section, provide required information (example follows) Host (e.g annually-harmless-lark.dataos.io) Port (e.g 7432) Catalog (e.g hive) Schema (optional) -- Generate login key -- \u00b6 Coming soon... e) Perform Test action and successful test results imply a successfull connection to the data source. Connect Presto to Power BI: \u00b6 a) Launch Power BI and click on the Get Data option in top menu bar b) Search for the option of ODBC from the data source list, then select ODBC and click on Connect . c) Select the DSN you tested successfully during the DSN setup process, and click OK d) Enter the username, password in the dialogue box and click on Connect . e) Post a successful connection, you can preview the data. f) Tick the checkbox of the relevant dataset/table in the left menu and click Load . g) Now you are ready to work with Power BI chart visualization activities.","title":"Integrate with Power BI"},{"location":"integrations/powerbiold/#integrate-with-power-bi","text":"","title":"Integrate with Power BI"},{"location":"integrations/powerbiold/#purpose","text":"Power BI business analytics solution comprises several products and services, and value and versatility comes from leveraging the individual elements, and taking advantage of how they work together. The following text streamlines the integration with Power BI, to make it quick and straightforward.","title":"Purpose:"},{"location":"integrations/powerbiold/#download-driver","text":"Download and install Simba Presto ODBC Driver from the following link https://www.simba.com/drivers/presto-odbc-jdbc/","title":"Download driver:"},{"location":"integrations/powerbiold/#install-simba-presto-odbc-driver","text":"a) Double-click the downloaded driver file to start the installation process, click Next and follow the instructions. b) After successful installation, copy the license file (that you must have received in your email) into the \\lib subfolder of the installation folder you selected above. In case you encounter an error, you may not have the required admin priviliges. Contact your network administrator to accomplish this.","title":"Install Simba Presto ODBC driver"},{"location":"integrations/powerbiold/#configure-odbc-dsn","text":"After successful installation, proceed as follows to configure ODBC DSN: a) ODBC Data Sources (64-bit) Go to ODBC Sources Administrator(64 bit or 32 bit) and open it. b) In the dialogue box select System DSN tab c) Now click Add and select driver name which you entered during driver installation process and click Finish d) A new dialogue box opens up for ODBC driver DSN setup In the DSN Setup section, provide Data Source Name and 'Description' In the Authentication section, Select Authentication type as No Authentication Enter username and the system generated login key Now in the Data Source section, provide required information (example follows) Host (e.g annually-harmless-lark.dataos.io) Port (e.g 7432) Catalog (e.g hive) Schema (optional)","title":"Configure ODBC DSN"},{"location":"integrations/powerbiold/#-generate-login-key-","text":"Coming soon... e) Perform Test action and successful test results imply a successfull connection to the data source.","title":"-- Generate login key --"},{"location":"integrations/powerbiold/#connect-presto-to-power-bi","text":"a) Launch Power BI and click on the Get Data option in top menu bar b) Search for the option of ODBC from the data source list, then select ODBC and click on Connect . c) Select the DSN you tested successfully during the DSN setup process, and click OK d) Enter the username, password in the dialogue box and click on Connect . e) Post a successful connection, you can preview the data. f) Tick the checkbox of the relevant dataset/table in the left menu and click Load . g) Now you are ready to work with Power BI chart visualization activities.","title":"Connect Presto to Power BI:"},{"location":"integrations/spss/","text":"DataOS Integration with IBM SPSS Statistics \u00b6 This article will help you to set up the connection between DataOS and SPSS Statistics. It provides specific steps needed to fetch data from DataOS into SPSS Statistics for descriptive analysis and mining. Explore DataOS data with SPSS Statistics \u00b6 SPSS Statistics is a comprehensive statistical analysis and data management solution. SPSS can take data from almost any type of file and use them to generate tabulated reports, charts and plots of distributions and trends, descriptive statistics. It is used by market researchers, survey companies, government, education researchers, marketing organizations and data miners. DataOS and SPSS Statistics integration works to take advantage of powerful statistical analysis, on the data pulled from DataOS. Requirements \u00b6 SPSS Statistics installed on your system - If SPSS Statistics is not installed on your system, you can download the latest version from the IBM SPSS software website Simba Presto ODBC Driver - In order to connect to DataOS Catalog, you would have to install the Presto driver. DataOS API token - To authenticate to and access DataOS, you will need an API token. Download and install Presto driver \u00b6 Download it from Presto ODBC & JDBC Drivers download page . 2. To run the installer, double-click on downloaded Simba Presto XX-bit installer file. Select the 32 or 64 bit version according to your system configurations. Follow the steps for the successful installation. a. Click Next . b. Select the check box to accept the terms of the License Agreement if you agree, and then click Next . c. To change the installation location, click Change, then browse to the desired folder, and then click OK . To accept the installation location, click Next . d. Click Install . e. When the installation completes, click Finish . 3. After successful installation, copy the license file (that you have received in your email) into the \\lib subfolder of the Simba installation folder. Note : Contact your network administrator in case you encounter an error due to not having required admin priviliges. Generate DataOS API token \u00b6 Sign in to your DataOS instance with your username and password. On the DataOS home page, click on ' Profile '. On the 'Profile' page, click on Tokens . Click on the Add API Key link on the Tokens tab: Type in name for this token and also set the validity period of your token based on the security requirements as per your business needs. Click Save to create one for you. The API key is listed below. Click on the \u201ceye icon\u201d on the right side to make the full API key visible. Click on the APIkey to copy it. You need this API key to configure Simba Presto driver. Configure Presto ODBC DSN \u00b6 To use the Simba Presto ODBC Driver in SPSS Statistics application, you need to configure a Data Source Name (DSN) to connect to your data in DataOS. Note : Currently, SPSS does not provide an SPSS Data Access Pack for Mac so you need to use Windows system. For more information, refer to IBM Support. Launch SPSS Statistics. Double-click on the New Database Query . Click on Add ODBC DataSource . In ODBC Data Source Administrator (64-bit or 32-bit) dialog box, click System DSN tab. In the list of DSNs, select Simba Presto ODBC DSN, and then click Configure . In the DSN Setup dialog box, provide the following inputs: Provide 'Description' for data source name. In the 'Authentication' section Select Authentication type as No Authentication Enter generated API key as username Now in the 'Data Source' section, provide required information. Host (e.g reasonably-welcome-grub.dataos.io) Port (e.g 7432) Catalog (e.g icebase) Schema (optional) In the DSN Setup dialog box, click SSL Options and enable SSL. Click Test . and if successful, press OK to close the Test Results dialog box. Click OK to save your DSN. Note : If you encounter any error in setting up the connection, please check DataOS url, validity of API key and try again or contact your administrator. Access DataOS on SPSS Statistics \u00b6 Launch SPSS Statistics. Click on the New Database Query option. Select the DSN you tested successfully during the DSN setup process, and double-click. Enter IBM SPSS credentials in the dialogue box and click on Connect . On successful connection, you can see the DataOS tables in the left panel. Select the table and click the Arrow button. You can see the fields of the selected table. Now you can explore and visualize this data in SPSS Statistics.","title":"IBM SPSS Statistics"},{"location":"integrations/spss/#dataos-integration-with-ibm-spss-statistics","text":"This article will help you to set up the connection between DataOS and SPSS Statistics. It provides specific steps needed to fetch data from DataOS into SPSS Statistics for descriptive analysis and mining.","title":"DataOS Integration with IBM SPSS Statistics"},{"location":"integrations/spss/#explore-dataos-data-with-spss-statistics","text":"SPSS Statistics is a comprehensive statistical analysis and data management solution. SPSS can take data from almost any type of file and use them to generate tabulated reports, charts and plots of distributions and trends, descriptive statistics. It is used by market researchers, survey companies, government, education researchers, marketing organizations and data miners. DataOS and SPSS Statistics integration works to take advantage of powerful statistical analysis, on the data pulled from DataOS.","title":"Explore DataOS data with SPSS Statistics"},{"location":"integrations/spss/#requirements","text":"SPSS Statistics installed on your system - If SPSS Statistics is not installed on your system, you can download the latest version from the IBM SPSS software website Simba Presto ODBC Driver - In order to connect to DataOS Catalog, you would have to install the Presto driver. DataOS API token - To authenticate to and access DataOS, you will need an API token.","title":"Requirements"},{"location":"integrations/spss/#download-and-install-presto-driver","text":"Download it from Presto ODBC & JDBC Drivers download page . 2. To run the installer, double-click on downloaded Simba Presto XX-bit installer file. Select the 32 or 64 bit version according to your system configurations. Follow the steps for the successful installation. a. Click Next . b. Select the check box to accept the terms of the License Agreement if you agree, and then click Next . c. To change the installation location, click Change, then browse to the desired folder, and then click OK . To accept the installation location, click Next . d. Click Install . e. When the installation completes, click Finish . 3. After successful installation, copy the license file (that you have received in your email) into the \\lib subfolder of the Simba installation folder. Note : Contact your network administrator in case you encounter an error due to not having required admin priviliges.","title":"Download and install Presto driver"},{"location":"integrations/spss/#generate-dataos-api-token","text":"Sign in to your DataOS instance with your username and password. On the DataOS home page, click on ' Profile '. On the 'Profile' page, click on Tokens . Click on the Add API Key link on the Tokens tab: Type in name for this token and also set the validity period of your token based on the security requirements as per your business needs. Click Save to create one for you. The API key is listed below. Click on the \u201ceye icon\u201d on the right side to make the full API key visible. Click on the APIkey to copy it. You need this API key to configure Simba Presto driver.","title":"Generate DataOS API token"},{"location":"integrations/spss/#configure-presto-odbc-dsn","text":"To use the Simba Presto ODBC Driver in SPSS Statistics application, you need to configure a Data Source Name (DSN) to connect to your data in DataOS. Note : Currently, SPSS does not provide an SPSS Data Access Pack for Mac so you need to use Windows system. For more information, refer to IBM Support. Launch SPSS Statistics. Double-click on the New Database Query . Click on Add ODBC DataSource . In ODBC Data Source Administrator (64-bit or 32-bit) dialog box, click System DSN tab. In the list of DSNs, select Simba Presto ODBC DSN, and then click Configure . In the DSN Setup dialog box, provide the following inputs: Provide 'Description' for data source name. In the 'Authentication' section Select Authentication type as No Authentication Enter generated API key as username Now in the 'Data Source' section, provide required information. Host (e.g reasonably-welcome-grub.dataos.io) Port (e.g 7432) Catalog (e.g icebase) Schema (optional) In the DSN Setup dialog box, click SSL Options and enable SSL. Click Test . and if successful, press OK to close the Test Results dialog box. Click OK to save your DSN. Note : If you encounter any error in setting up the connection, please check DataOS url, validity of API key and try again or contact your administrator.","title":"Configure Presto ODBC DSN"},{"location":"integrations/spss/#access-dataos-on-spss-statistics","text":"Launch SPSS Statistics. Click on the New Database Query option. Select the DSN you tested successfully during the DSN setup process, and double-click. Enter IBM SPSS credentials in the dialogue box and click on Connect . On successful connection, you can see the DataOS tables in the left panel. Select the table and click the Arrow button. You can see the fields of the selected table. Now you can explore and visualize this data in SPSS Statistics.","title":"Access DataOS on SPSS Statistics"},{"location":"integrations/superset/","text":"Apache Superset \u00b6 Apache Superset is a modern, enterprise-ready business intelligence web application. It is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple pie charts to highly detailed deck.gl geospatial charts. An intuitive interface for visualizing datasets and crafting interactive dashboards A wide array of beautiful visualizations to showcase your data Code-free visualization builder to extract and present datasets A world-class SQL IDE for preparing data for visualization, including a rich metadata browser A lightweight semantic layer which empowers data analysts to quickly define custom dimensions and metrics Out-of-the-box support for most SQL-speaking databases Seamless, in-memory asynchronous caching and queries An extensible security model that allows configuration of very intricate rules on on who can access which product features and datasets. Integration with major authentication backends (database, OpenID, LDAP, OAuth, REMOTE_USER, etc) The ability to add custom visualization plugins An API for programmatic customization A cloud-native architecture designed from the ground up for scale Superset is cloud-native and designed to be highly available. It was designed to scale out to large, distributed environments and works very well inside containers.","title":"Apache Superset"},{"location":"integrations/superset/#apache-superset","text":"Apache Superset is a modern, enterprise-ready business intelligence web application. It is fast, lightweight, intuitive, and loaded with options that make it easy for users of all skill sets to explore and visualize their data, from simple pie charts to highly detailed deck.gl geospatial charts. An intuitive interface for visualizing datasets and crafting interactive dashboards A wide array of beautiful visualizations to showcase your data Code-free visualization builder to extract and present datasets A world-class SQL IDE for preparing data for visualization, including a rich metadata browser A lightweight semantic layer which empowers data analysts to quickly define custom dimensions and metrics Out-of-the-box support for most SQL-speaking databases Seamless, in-memory asynchronous caching and queries An extensible security model that allows configuration of very intricate rules on on who can access which product features and datasets. Integration with major authentication backends (database, OpenID, LDAP, OAuth, REMOTE_USER, etc) The ability to add custom visualization plugins An API for programmatic customization A cloud-native architecture designed from the ground up for scale Superset is cloud-native and designed to be highly available. It was designed to scale out to large, distributed environments and works very well inside containers.","title":"Apache Superset"},{"location":"integrations/tableau/","text":"DataOS Integration with Tableau \u00b6 This article will help you to set up the connection between DataOS and Tableau. It provides specific steps needed to fetch data from DataOS into Tableau. Explore your DataOS data with Tableau \u00b6 Tableau is one of the most powerful business intelligence tools which allows you to pull vast amounts of information from disparate sources, and helps you turning it into advanced visualizations. The dashboard created from these visualizations empowers you with business insights and helps in data-driven decisions. DataOS and Tableau integration works to take advantage of that powerful visualization technology, on the data pulled from DataOS. Requirements \u00b6 Tableau Desktop installed on your system - If Tableau is not installed on your system, you can download the latest version from the Tableau website. Driver - In order to connect to DataOS Catalog, you would have to install the driver. DataOS API token - To authenticate and access DataOS, you will need an API token. Download and install driver \u00b6 Check your Tableau version and follow the steps given below: Tableau: 2021.3, 2021.2.2, 2021.1.5, 2020.4.8, 2020.3.12, and above a. Download the driver (trino-jdbc-361.jar) from the Trino page. Please note that Presto is Trino now. b. Copy the downloaded JAR file to the appropriate location. ~/Library/Tableau/Drivers for MAC. C:\\Program Files\\Tableau\\Drivers for WINDOWS Tableau: 10.0-2020.2 a. Download the driver from Tableau driver download page . b. Select the operating system and bit version according to your system configurations. c. Click on Download button (mac or Windows). d. Double-click on downloaded 'Simba Presto 1.1.pkg' for mac or Windows 64-bit driver to run the installer. e. Click Continue and follow the steps for suceessful installation. Generate DataOS API token \u00b6 Sign in to your DataOS instance with your username and password. On the DataOS home page, click on ' Profile '. On the 'Profile' page, click on Tokens . Click on the Add API Key link on the Tokens tab: Type in name for this token and also set the validity period of your token based on the security requirements as per your business needs. Click Save to create one for you. The API key is listed below. Click on the \u201ceye icon\u201d on the right side to make the full API key visible. Click on the APIkey to copy it. You need this API key to configure Presto driver. Configure driver on Tableau \u00b6 You need to configure the Presto driver in Tableau application to connect to your data in DataOS. Open the Tableau desktop application. Click on More to access the list of all the servers connectors. Search for Presto and click on it. 2. Provide the following values: Property Value Server e.g. reasonably-welcome-grub.dataos.io Port 7432 Catalog e.g. icebase Schema an optional field Authentication username Username Access API Key from DataOS Require SSL Check the box 3. Click Sign In . Note : If you encounter any error in setting up the connection, please check DataOS URL, validity of API key and try again or contact your administrator. Access DataOS on Tableau \u00b6 Once you've completed the driver configuration steps successfully, you can see the DataOS catalog in the left panel in Tableau dialog. To get the list of available schemas, click on Search icon. To select the relevant schema, double-click on it. To bring the data from the table, click on Search icon and you can see all the tables available in your DataOS schema cluster. Double-click to select a table that you want to retrieve data from. Click Upload Now to load the data for preview. Tableau retrieves data from the selected DataOS table and loads it into a worksheet. Now you can explore and visualize this data in Tableau.","title":"Tableau"},{"location":"integrations/tableau/#dataos-integration-with-tableau","text":"This article will help you to set up the connection between DataOS and Tableau. It provides specific steps needed to fetch data from DataOS into Tableau.","title":"DataOS Integration with Tableau"},{"location":"integrations/tableau/#explore-your-dataos-data-with-tableau","text":"Tableau is one of the most powerful business intelligence tools which allows you to pull vast amounts of information from disparate sources, and helps you turning it into advanced visualizations. The dashboard created from these visualizations empowers you with business insights and helps in data-driven decisions. DataOS and Tableau integration works to take advantage of that powerful visualization technology, on the data pulled from DataOS.","title":"Explore your DataOS data with Tableau"},{"location":"integrations/tableau/#requirements","text":"Tableau Desktop installed on your system - If Tableau is not installed on your system, you can download the latest version from the Tableau website. Driver - In order to connect to DataOS Catalog, you would have to install the driver. DataOS API token - To authenticate and access DataOS, you will need an API token.","title":"Requirements"},{"location":"integrations/tableau/#download-and-install-driver","text":"Check your Tableau version and follow the steps given below: Tableau: 2021.3, 2021.2.2, 2021.1.5, 2020.4.8, 2020.3.12, and above a. Download the driver (trino-jdbc-361.jar) from the Trino page. Please note that Presto is Trino now. b. Copy the downloaded JAR file to the appropriate location. ~/Library/Tableau/Drivers for MAC. C:\\Program Files\\Tableau\\Drivers for WINDOWS Tableau: 10.0-2020.2 a. Download the driver from Tableau driver download page . b. Select the operating system and bit version according to your system configurations. c. Click on Download button (mac or Windows). d. Double-click on downloaded 'Simba Presto 1.1.pkg' for mac or Windows 64-bit driver to run the installer. e. Click Continue and follow the steps for suceessful installation.","title":"Download and install driver"},{"location":"integrations/tableau/#generate-dataos-api-token","text":"Sign in to your DataOS instance with your username and password. On the DataOS home page, click on ' Profile '. On the 'Profile' page, click on Tokens . Click on the Add API Key link on the Tokens tab: Type in name for this token and also set the validity period of your token based on the security requirements as per your business needs. Click Save to create one for you. The API key is listed below. Click on the \u201ceye icon\u201d on the right side to make the full API key visible. Click on the APIkey to copy it. You need this API key to configure Presto driver.","title":"Generate DataOS API token"},{"location":"integrations/tableau/#configure-driver-on-tableau","text":"You need to configure the Presto driver in Tableau application to connect to your data in DataOS. Open the Tableau desktop application. Click on More to access the list of all the servers connectors. Search for Presto and click on it. 2. Provide the following values: Property Value Server e.g. reasonably-welcome-grub.dataos.io Port 7432 Catalog e.g. icebase Schema an optional field Authentication username Username Access API Key from DataOS Require SSL Check the box 3. Click Sign In . Note : If you encounter any error in setting up the connection, please check DataOS URL, validity of API key and try again or contact your administrator.","title":"Configure driver on Tableau"},{"location":"integrations/tableau/#access-dataos-on-tableau","text":"Once you've completed the driver configuration steps successfully, you can see the DataOS catalog in the left panel in Tableau dialog. To get the list of available schemas, click on Search icon. To select the relevant schema, double-click on it. To bring the data from the table, click on Search icon and you can see all the tables available in your DataOS schema cluster. Double-click to select a table that you want to retrieve data from. Click Upload Now to load the data for preview. Tableau retrieves data from the selected DataOS table and loads it into a worksheet. Now you can explore and visualize this data in Tableau.","title":"Access DataOS on Tableau"},{"location":"integrations/tableauold/","text":"Integrate with Tableau \u00b6 Purpose \u00b6 Tableau is one of the most commonly used tools across organisations, to extract business insights and gauge intended traction. The visualizations it offers make it one of the most popular tools as well. With this in mind, the following text has been designed to make integrating with Tableau simple and straightforward. Connector driver required: \u00b6 Simba Presto 1.1.pkg or the latest one according to your system configurations. If driver is not downloaded or installed in your computer, download and install it from the following link: https://www.tableau.com/support/drivers?__full-version=20204.21.0114.0916#presto Install driver and setup connection for the Tableau \u00b6 a) Double-click on downloaded Simba Presto ODBC Driver installer file and follow the instructions. b) Open Tableau desktop application, in the Connect section under the To a Server area, select More option c) Fill the dialogue box with the relevant information and click Sign In . Information required: \u00b6 Server : Name of the server that hosts the database you want to connect to e.g annually-harmless-lark.dataos.io Port : e.g 7432 Catalog : e.g hive Schema : an optional field Authentication : Select authentication method as Username and enter the same in the next field. --- How to generate Username/Password key --- \u00b6 Coming soon.......... d) On successful connection, Tableau desktop will open with the relevant connection details showing in the left panel. If you encounter any error in setting up the connection, please check your credentials and try again or contact your administrator. e) In the left panel, search and select the relevant schema. f) Now search and select the relevant schema table. g) Now click Upload now to load data to preview. h) The data is now available for visualization.","title":"Integrate with Tableau"},{"location":"integrations/tableauold/#integrate-with-tableau","text":"","title":"Integrate with Tableau"},{"location":"integrations/tableauold/#purpose","text":"Tableau is one of the most commonly used tools across organisations, to extract business insights and gauge intended traction. The visualizations it offers make it one of the most popular tools as well. With this in mind, the following text has been designed to make integrating with Tableau simple and straightforward.","title":"Purpose"},{"location":"integrations/tableauold/#connector-driver-required","text":"Simba Presto 1.1.pkg or the latest one according to your system configurations. If driver is not downloaded or installed in your computer, download and install it from the following link: https://www.tableau.com/support/drivers?__full-version=20204.21.0114.0916#presto","title":"Connector driver required:"},{"location":"integrations/tableauold/#install-driver-and-setup-connection-for-the-tableau","text":"a) Double-click on downloaded Simba Presto ODBC Driver installer file and follow the instructions. b) Open Tableau desktop application, in the Connect section under the To a Server area, select More option c) Fill the dialogue box with the relevant information and click Sign In .","title":"Install driver and setup connection for the Tableau"},{"location":"integrations/tableauold/#information-required","text":"Server : Name of the server that hosts the database you want to connect to e.g annually-harmless-lark.dataos.io Port : e.g 7432 Catalog : e.g hive Schema : an optional field Authentication : Select authentication method as Username and enter the same in the next field.","title":"Information required:"},{"location":"integrations/tableauold/#-how-to-generate-usernamepassword-key-","text":"Coming soon.......... d) On successful connection, Tableau desktop will open with the relevant connection details showing in the left panel. If you encounter any error in setting up the connection, please check your credentials and try again or contact your administrator. e) In the left panel, search and select the relevant schema. f) Now search and select the relevant schema table. g) Now click Upload now to load data to preview. h) The data is now available for visualization.","title":"--- How to generate Username/Password key ---"},{"location":"minerva/index1/","text":"DataOS \u00ae Minerva \u00b6 Modern SQL query engine Minerva /m\u026a\u02c8n\u025c\u02d0rv\u0259/ (Latin: [m\u026a\u02c8n\u025brwa]; Etruscan: Menrva) is the Roman goddess of wisdom and strategic warfare, justice, law, victory, and the sponsor of arts, trade, and strategy. Architecture \u00b6 Component Layout \u00b6 Coming soon... Scaling Primitives \u00b6 Coming soon... Supported Depot Types \u00b6 Coming soon... Support Clients \u00b6 Coming soon...","title":"DataOS<sup>\u00ae</sup> Minerva"},{"location":"minerva/index1/#dataos-minerva","text":"Modern SQL query engine Minerva /m\u026a\u02c8n\u025c\u02d0rv\u0259/ (Latin: [m\u026a\u02c8n\u025brwa]; Etruscan: Menrva) is the Roman goddess of wisdom and strategic warfare, justice, law, victory, and the sponsor of arts, trade, and strategy.","title":"DataOS\u00ae Minerva"},{"location":"minerva/index1/#architecture","text":"","title":"Architecture"},{"location":"minerva/index1/#component-layout","text":"Coming soon...","title":"Component Layout"},{"location":"minerva/index1/#scaling-primitives","text":"Coming soon...","title":"Scaling Primitives"},{"location":"minerva/index1/#supported-depot-types","text":"Coming soon...","title":"Supported Depot Types"},{"location":"minerva/index1/#support-clients","text":"Coming soon...","title":"Support Clients"},{"location":"minerva/introduction/","text":"Introduction \u00b6 Minerva is an interactive query engine that makes it easy to analyze big data using standard SQL. It enables high-performance SQL access to a large variety of data sources, including traditional relational databases Oracle, PostgreSQL, MySQL, and Redshift and other data sources such as Kafka, Cassandra. You can query and explore data from these data sources without bringing it to DataOS. You can set up Minerva clusters that can process data from many different data sources even within a single query. This capability reduces the complexity of integrating multiple systems, which means that you can query different databases with different schemas in the same SQL statement simultaneously. A Minerva cluster consists of a single coordinator node and one or more worker nodes, and Minerva uses its own coordinator within the cluster to schedule queries among its workers. The coordinator is responsible for submitting, parsing, planning, and optimizing queries and query orchestration, and worker nodes are responsible for query processing. Minerva enables you to concurrently run hundreds of memory, I/O, and CPU-intensive queries. For such query load, a single Minerva cluster is not sufficient and you need to create multiple clusters. It can scale to hundreds of worker nodes while efficiently utilizing cluster resources. Minerva uses Gateway, a query router that sits in front of single or multiple Minerva clusters and becomes the interface for all the queries executed across the clusters. These queries are submitted from the Workbench. Gateway also ensures high availability in case of downtime and balances the load across the clusters.","title":"Introduction"},{"location":"minerva/introduction/#introduction","text":"Minerva is an interactive query engine that makes it easy to analyze big data using standard SQL. It enables high-performance SQL access to a large variety of data sources, including traditional relational databases Oracle, PostgreSQL, MySQL, and Redshift and other data sources such as Kafka, Cassandra. You can query and explore data from these data sources without bringing it to DataOS. You can set up Minerva clusters that can process data from many different data sources even within a single query. This capability reduces the complexity of integrating multiple systems, which means that you can query different databases with different schemas in the same SQL statement simultaneously. A Minerva cluster consists of a single coordinator node and one or more worker nodes, and Minerva uses its own coordinator within the cluster to schedule queries among its workers. The coordinator is responsible for submitting, parsing, planning, and optimizing queries and query orchestration, and worker nodes are responsible for query processing. Minerva enables you to concurrently run hundreds of memory, I/O, and CPU-intensive queries. For such query load, a single Minerva cluster is not sufficient and you need to create multiple clusters. It can scale to hundreds of worker nodes while efficiently utilizing cluster resources. Minerva uses Gateway, a query router that sits in front of single or multiple Minerva clusters and becomes the interface for all the queries executed across the clusters. These queries are submitted from the Workbench. Gateway also ensures high availability in case of downtime and balances the load across the clusters.","title":"Introduction"},{"location":"minerva/ondemandcompute/","text":"On Demand Computing [In Progress] \u00b6 Introduction \u00b6 In an enterprise system, demand for computing resources varies from time to time. In such a scenario, resources need to be provisioned to handle the increase/decrease in demand. On-demand computing is a delivery model in which computing resources such as computing power, storage, and memory are made available to the user as needed. The resources are provided on an as-needed and when-needed basis for specific data processing/exploratory analytical workloads. The on-demand compute model overcomes the common challenge of efficiently meeting fluctuating computational requirements. Now you don't have to over-provision resources upfront to handle peak levels of business activity in the future. Instead, you provision the number of resources that you actually need. You can scale these resources up or down as your business needs change. Tightly coupled storage and compute \u00b6 The traditional data warehouses always come with a compute on top of the data; the nodes deployed in the cluster are used for storing data and computation. However, the requirement of Storage and Compute is not always linear, and it varies for different workloads. Some workloads are compute-intensive; others may require a large data storage volume. With the nodes doing both, the enterprise must scale both simultaneously. This tight coupling leads to underutilized or scarce resources either on Storage or Compute, not giving the optimal use of provisioned capacity. There are situations when the enterprise needs to grow compute capabilities while keeping storage relatively constant, such as during high-traffic periods like popular annual shopping events or at the end of the quarter consolidation. Scaling these functions independently frees the enterprise to pay only for what it uses, as it uses it. To work with the data stored in the warehouse, you have to go via the data warehouse's compute tier that creates performance issues while simultaneously trying to load and query data. Traditional cluster configuration does not have a shared data cluster. Therefore, if multiple clusters are to be provisioned for different types of workloads e.g., ELT/ELT, BI & Reporting on same data- the only option is to copy or move the required data to their respective workload clusters leading to data duplication. Power of separation between compute and storage \u00b6 With decoupling the storage and compute, each of these can be scaled independent to each other. You can bring larger or smaller compute based on your needs. So, for example, imagine a reporting query that you have to run, which is going to last for six hours because it's going to scan six months' worth of data. You can bring a larger cluster for six hours to run that query. For complex queries you have to wait until data is completely loaded into the warehouse. Imagine that your team is trying to do ad-hoc analysis to understand what data they have, and typically, they're running queries on a smaller set of data. You need a smaller cluster, which is long-running. So, you have the ability to design your compute, which will work on the data to suffice the need that you're trying to address. You can manage two kinds of workloads \u2013 Batch and Interactive. Even in the case of Batch, there are ETL, dashboards queries, periodic reports, etc. In the case of ad hoc queries, the requirements might be specific to the user and use cases. For a data scientist, the data set and cluster capabilities requirements might be completely different from an analyst use case. The decoupling of storage and compute also works with individual departments as well. Many teams have their own budgeting constraints and work on different data sets. They like to have their own analytics setup and manage it according to their application lifecycle. This brings in huge cost savings and operational efficiencies as the resources are stood up only when they are required, only for the amount of resources that are required and only for the time they are required, after which they are terminated. On-Demand Compute in DataOS \u00b6 DataOS enables you to define and maintain a pool of resources that contains networks, servers, storage, applications, and services. This pool can serve the varying demand of resources and computing for various workload requirements. When you work with data in the DataOS storage built on top of Iceberg table format, you can bring whatever compute you want. Data storage in DataOS, essentially a lake house pattern, gives you an open data format. DataOS maintains data as single source of truth, the Business and IT teams executing various workloads can now have access to the required data (as per the governance policies) without having to move it to their local cluster. This creates a much streamlined, effective and optimized approach to provisioning of data across the organization. You can create a Minerva query engine cluster, which allows you to query the data through depots that you have in the system. You can attach and create your own cluster within the DataOS instance. The billing for that cluster can be attributed to you for your use case, and you can keep it private to yourself. So, you can be more cost-efficient. So, when you deploy your jobs for bringing data, like your Flare data processing jobs or querying your data on Workbench, you can ask for required compute. To create an on-demand compute type, you need to know which cluster needs to run on a specific kind of machine. You need to understand what kind of compute works best on various types of queries. Then you will be able to decide do you want to run it faster or cheaper? And based on that, you can adjust the compute. Compute structures in DataOS \u00b6 Create compute clusters on-demand for varied data processing/querying needs. This section describes the steps to define and create various compute for the different workload requirements. To define compute structures in DataOS, follow the three steps process given below: Provisioning VMs \u200b Define compute resource in DataOS\u200b Refer compute resource in Flare job or Minerva cluster\u200b Provisioning VMs \u00b6 DataOS uses Kubernetes for cluster and container management. It supports creating/defining groups of VMs (node pools) that have a certain profile- a specific CPU, memory, and disk capacity. You need to provision a group of VMs, register those with Kubernetes, and then with the DataOS as a compute resource. You can also create a group of VMs that use GPUs or a group of VMs with very small CPUs. Then once you have these groups created, you can register them with the DataOS. So now they're known what computes you have. Note : Please contact your administrator for creating node pool. Define compute resources in DataOS \u00b6 Compute in the DataOS context is just another primitive. You can define and name these compute resourcesin a YAML file. You can run applications in the DataOS by allocating memory through the resource manager. By Default, DataOS has two separate computes: 1) runnable for running resources which are essentially our workflows and services 2) queries which are Minerva clusters When DataOS is deployed, these two computes are registered by default with the DataOS. So they are part of the default install. DataOS allows you to add new groups of VMs and make them known to the DataOS and addressable via a name. - name : configure-dataos-resources-system values : install_and_upgrade : - name : \"runnable-default\" version : v1beta1 type : compute layer : system description : \"default runnable compute\" compute : type : K8SNodePoolLocal defaultFor : runnable nodePool : nodeSelector : \"dataos.io/purpose\" : \"runnable\" tolerations : - key : \"dedicated\" operator : \"Equal\" value : \"runnable\" effect : \"NoSchedule\" - name : \"query-default\" version : v1beta1 type : compute layer : system description : \"default query compute\" compute : type : K8SNodePoolLocal defaultFor : query nodePool : nodeSelector : \"dataos.io/purpose\" : \"query\" tolerations : - key : \"dedicated\" operator : \"Equal\" value : \"query\" effect : \"NoSchedule\" Create Minerva clusters \u00b6 When you define a cluster, you specify the node type. The node type determines each node's CPU, RAM, storage capacity, and storage drive type. For example, 16 cores | 128 GB RAM | 2 TB of local storage The above specifications are just a minimum recommendation. You should consider increasing these specifications based on your workloads and the amount of data you are processing. Various data processing requirements force data engineering teams to create multiple clusters. Define Cluster groups \u00b6 To learn more refer- Minerva Cluster Tuning Self-service access to on-demand compute on Workbench \u00b6 Selecting clusters for your query \u00b6 Default clusters You can choose to connect to Minerva clusters from Workbench as per the compute requirement. Running queries status \u00b6 Minerva usage statistics dashboard Query status, statistics List of Clusters \u00b6 List of Minerva Clusters added to the gateway. Managing Clusters \u00b6 You can add or delete clusters as per the query workload. How to do that, Workbench?? The resources may be maintained within the DataOS environment and made available by a cloud service provider. The complexity of managing cluster performance to meet business requirements is complex. You want best-in-class performance to meet the short and long-running, interactive workloads while reducing and controlling costs. Cluster usage status \u00b6 RoutingPolicy \u00b6 RoutingPolicy determines the criteria for a cluster to be qualified for a query. The default RoutingPolicy is RANDOM, which means any cluster can be chosen randomly to execute the query. - if it maintains a queue - As per query, load The routing of requests can be controlled by implementing RoutingRule. Adding your own rules \u00b6 By default, it has location-based, random, and static routing rules. Policies around cluster access \u00b6 As per the access policy, the gateway will forward the query to the assigned clusters and continue the entire query process. DataGateway is a service that sits between clients and Minerva clusters. It is essentially an intelligent HTTP proxy server that is an abstraction layer on top of the Minerva clusters that handles the following actions: Parse incoming SQL statements to get requested datasets Manage data access policy to limit users' data access by checking against the SQL results. Manage users' cluster access. Redirect users' queries to the authorized clusters. Inform users about the errors whenever the query is rejected, or exceptions from clusters are encountered.","title":"On Demand Computing"},{"location":"minerva/ondemandcompute/#on-demand-computing-in-progress","text":"","title":"On Demand Computing [In Progress]"},{"location":"minerva/ondemandcompute/#introduction","text":"In an enterprise system, demand for computing resources varies from time to time. In such a scenario, resources need to be provisioned to handle the increase/decrease in demand. On-demand computing is a delivery model in which computing resources such as computing power, storage, and memory are made available to the user as needed. The resources are provided on an as-needed and when-needed basis for specific data processing/exploratory analytical workloads. The on-demand compute model overcomes the common challenge of efficiently meeting fluctuating computational requirements. Now you don't have to over-provision resources upfront to handle peak levels of business activity in the future. Instead, you provision the number of resources that you actually need. You can scale these resources up or down as your business needs change.","title":"Introduction"},{"location":"minerva/ondemandcompute/#tightly-coupled-storage-and-compute","text":"The traditional data warehouses always come with a compute on top of the data; the nodes deployed in the cluster are used for storing data and computation. However, the requirement of Storage and Compute is not always linear, and it varies for different workloads. Some workloads are compute-intensive; others may require a large data storage volume. With the nodes doing both, the enterprise must scale both simultaneously. This tight coupling leads to underutilized or scarce resources either on Storage or Compute, not giving the optimal use of provisioned capacity. There are situations when the enterprise needs to grow compute capabilities while keeping storage relatively constant, such as during high-traffic periods like popular annual shopping events or at the end of the quarter consolidation. Scaling these functions independently frees the enterprise to pay only for what it uses, as it uses it. To work with the data stored in the warehouse, you have to go via the data warehouse's compute tier that creates performance issues while simultaneously trying to load and query data. Traditional cluster configuration does not have a shared data cluster. Therefore, if multiple clusters are to be provisioned for different types of workloads e.g., ELT/ELT, BI & Reporting on same data- the only option is to copy or move the required data to their respective workload clusters leading to data duplication.","title":"Tightly coupled storage and compute"},{"location":"minerva/ondemandcompute/#power-of-separation-between-compute-and-storage","text":"With decoupling the storage and compute, each of these can be scaled independent to each other. You can bring larger or smaller compute based on your needs. So, for example, imagine a reporting query that you have to run, which is going to last for six hours because it's going to scan six months' worth of data. You can bring a larger cluster for six hours to run that query. For complex queries you have to wait until data is completely loaded into the warehouse. Imagine that your team is trying to do ad-hoc analysis to understand what data they have, and typically, they're running queries on a smaller set of data. You need a smaller cluster, which is long-running. So, you have the ability to design your compute, which will work on the data to suffice the need that you're trying to address. You can manage two kinds of workloads \u2013 Batch and Interactive. Even in the case of Batch, there are ETL, dashboards queries, periodic reports, etc. In the case of ad hoc queries, the requirements might be specific to the user and use cases. For a data scientist, the data set and cluster capabilities requirements might be completely different from an analyst use case. The decoupling of storage and compute also works with individual departments as well. Many teams have their own budgeting constraints and work on different data sets. They like to have their own analytics setup and manage it according to their application lifecycle. This brings in huge cost savings and operational efficiencies as the resources are stood up only when they are required, only for the amount of resources that are required and only for the time they are required, after which they are terminated.","title":"Power of separation between compute and storage"},{"location":"minerva/ondemandcompute/#on-demand-compute-in-dataos","text":"DataOS enables you to define and maintain a pool of resources that contains networks, servers, storage, applications, and services. This pool can serve the varying demand of resources and computing for various workload requirements. When you work with data in the DataOS storage built on top of Iceberg table format, you can bring whatever compute you want. Data storage in DataOS, essentially a lake house pattern, gives you an open data format. DataOS maintains data as single source of truth, the Business and IT teams executing various workloads can now have access to the required data (as per the governance policies) without having to move it to their local cluster. This creates a much streamlined, effective and optimized approach to provisioning of data across the organization. You can create a Minerva query engine cluster, which allows you to query the data through depots that you have in the system. You can attach and create your own cluster within the DataOS instance. The billing for that cluster can be attributed to you for your use case, and you can keep it private to yourself. So, you can be more cost-efficient. So, when you deploy your jobs for bringing data, like your Flare data processing jobs or querying your data on Workbench, you can ask for required compute. To create an on-demand compute type, you need to know which cluster needs to run on a specific kind of machine. You need to understand what kind of compute works best on various types of queries. Then you will be able to decide do you want to run it faster or cheaper? And based on that, you can adjust the compute.","title":"On-Demand Compute in DataOS"},{"location":"minerva/ondemandcompute/#compute-structures-in-dataos","text":"Create compute clusters on-demand for varied data processing/querying needs. This section describes the steps to define and create various compute for the different workload requirements. To define compute structures in DataOS, follow the three steps process given below: Provisioning VMs \u200b Define compute resource in DataOS\u200b Refer compute resource in Flare job or Minerva cluster\u200b","title":"Compute structures in DataOS"},{"location":"minerva/ondemandcompute/#provisioning-vms","text":"DataOS uses Kubernetes for cluster and container management. It supports creating/defining groups of VMs (node pools) that have a certain profile- a specific CPU, memory, and disk capacity. You need to provision a group of VMs, register those with Kubernetes, and then with the DataOS as a compute resource. You can also create a group of VMs that use GPUs or a group of VMs with very small CPUs. Then once you have these groups created, you can register them with the DataOS. So now they're known what computes you have. Note : Please contact your administrator for creating node pool.","title":"Provisioning VMs"},{"location":"minerva/ondemandcompute/#define-compute-resources-in-dataos","text":"Compute in the DataOS context is just another primitive. You can define and name these compute resourcesin a YAML file. You can run applications in the DataOS by allocating memory through the resource manager. By Default, DataOS has two separate computes: 1) runnable for running resources which are essentially our workflows and services 2) queries which are Minerva clusters When DataOS is deployed, these two computes are registered by default with the DataOS. So they are part of the default install. DataOS allows you to add new groups of VMs and make them known to the DataOS and addressable via a name. - name : configure-dataos-resources-system values : install_and_upgrade : - name : \"runnable-default\" version : v1beta1 type : compute layer : system description : \"default runnable compute\" compute : type : K8SNodePoolLocal defaultFor : runnable nodePool : nodeSelector : \"dataos.io/purpose\" : \"runnable\" tolerations : - key : \"dedicated\" operator : \"Equal\" value : \"runnable\" effect : \"NoSchedule\" - name : \"query-default\" version : v1beta1 type : compute layer : system description : \"default query compute\" compute : type : K8SNodePoolLocal defaultFor : query nodePool : nodeSelector : \"dataos.io/purpose\" : \"query\" tolerations : - key : \"dedicated\" operator : \"Equal\" value : \"query\" effect : \"NoSchedule\"","title":"Define compute resources in DataOS"},{"location":"minerva/ondemandcompute/#create-minerva-clusters","text":"When you define a cluster, you specify the node type. The node type determines each node's CPU, RAM, storage capacity, and storage drive type. For example, 16 cores | 128 GB RAM | 2 TB of local storage The above specifications are just a minimum recommendation. You should consider increasing these specifications based on your workloads and the amount of data you are processing. Various data processing requirements force data engineering teams to create multiple clusters.","title":"Create Minerva clusters"},{"location":"minerva/ondemandcompute/#define-cluster-groups","text":"To learn more refer- Minerva Cluster Tuning","title":"Define Cluster groups"},{"location":"minerva/ondemandcompute/#self-service-access-to-on-demand-compute-on-workbench","text":"","title":"Self-service access to on-demand compute on Workbench"},{"location":"minerva/ondemandcompute/#selecting-clusters-for-your-query","text":"Default clusters You can choose to connect to Minerva clusters from Workbench as per the compute requirement.","title":"Selecting clusters for your query"},{"location":"minerva/ondemandcompute/#running-queries-status","text":"Minerva usage statistics dashboard Query status, statistics","title":"Running queries status"},{"location":"minerva/ondemandcompute/#list-of-clusters","text":"List of Minerva Clusters added to the gateway.","title":"List of Clusters"},{"location":"minerva/ondemandcompute/#managing-clusters","text":"You can add or delete clusters as per the query workload. How to do that, Workbench?? The resources may be maintained within the DataOS environment and made available by a cloud service provider. The complexity of managing cluster performance to meet business requirements is complex. You want best-in-class performance to meet the short and long-running, interactive workloads while reducing and controlling costs.","title":"Managing Clusters"},{"location":"minerva/ondemandcompute/#cluster-usage-status","text":"","title":"Cluster usage status"},{"location":"minerva/ondemandcompute/#routingpolicy","text":"RoutingPolicy determines the criteria for a cluster to be qualified for a query. The default RoutingPolicy is RANDOM, which means any cluster can be chosen randomly to execute the query. - if it maintains a queue - As per query, load The routing of requests can be controlled by implementing RoutingRule.","title":"RoutingPolicy"},{"location":"minerva/ondemandcompute/#adding-your-own-rules","text":"By default, it has location-based, random, and static routing rules.","title":"Adding your own rules"},{"location":"minerva/ondemandcompute/#policies-around-cluster-access","text":"As per the access policy, the gateway will forward the query to the assigned clusters and continue the entire query process. DataGateway is a service that sits between clients and Minerva clusters. It is essentially an intelligent HTTP proxy server that is an abstraction layer on top of the Minerva clusters that handles the following actions: Parse incoming SQL statements to get requested datasets Manage data access policy to limit users' data access by checking against the SQL results. Manage users' cluster access. Redirect users' queries to the authorized clusters. Inform users about the errors whenever the query is rejected, or exceptions from clusters are encountered.","title":"Policies around cluster access"},{"location":"minerva/tuneminerva/","text":"Minerva Cluster Tuning \u00b6 This article describes how you can create the Minerva clusters with optimized resources and configure it to connect to relational databases and various other data sources. Tune Minerva cluster \u00b6 To create and tune Minerva cluster, provide the configuration properties for the Minerva cluster and connectors. These properties are saved in a DataOS setup YAML file stored in the DataOS configuration directory. Cluster properties \u00b6 The default Minerva settings should work well for most workloads. You may adjust the following properties such as replicas, resources to ensure optimal performance: - version : v1beta1 name : minervab # Name of the Minerva cluster type : cluster description : the default minerva cluster b tags : - cluster - minerva cluster : # Minerva cluster properties nodeSelector : \"dataos.io/purpose\" : \"query\" toleration : query runAsApiKey : api-key minerva : # Tune these values for optimal performance replicas : 2 resources : limits : cpu : 2000m memory : 4Gi requests : cpu : 2000m memory : 4Gi debug : logLevel : INFO trinoLogLevel : ERROR Configure connectors \u00b6 Minerva query engine offers a large variety of connectors, for example MySQL, PostgreSQL, Oracle, Redshift. These properties in DataOS setup file will mount the connector as the Catalog . This catalog contains schemas and references your data source via a connector. For Minerva, you can define connectors for data sources in the following ways: Depot: If you have already defined Depots to access data sources then you can include the address of these Depots in the DataOS setup YAML file to confifure connectors. You can also include additional properties to optimize the Minerva performance. Catalog: Here you can provide the name and properties such as connection url, user name and password to access the data source. The following is an example of connector configuration using Depot definition and catalog properties. All these definitions will be converted to catalogs which you can access on DataOS Workbench. Note : Replace the connection properties as appropriate for your setup. cluster : nodeSelector : \"dataos.io/purpose\" : \"query\" toleration : query runAsApiKey : api-key minerva : replicas : 2 resources : limits : cpu : 2000m memory : 4Gi requests : cpu : 2000m memory : 4Gi debug : logLevel : INFO trinoLogLevel : ERROR depots : # Pre-defined Depots and their properties - address : dataos://icebase:default properties : iceberg.file-format : PARQUET iceberg.compression-codec : GZIP hive.config.resources : \"/usr/trino/etc/catalog/core-site.xml\" hive.parquet.use-column-names : \"true\" - address : dataos://filebase:default properties : hive.config.resources : \"/usr/trino/etc/catalog/core-site.xml\" hive.parquet.use-column-names : \"true\" catalogs : # Data source connectors and their properties - name : redshift type : redshift properties : connection-url : \"jdbc:redshift://URL:PORT/DB\" connection-user : \"USERNAME\" connection-password : \"PASSWORD\" - name : oracle type : oracle properties : connection-url : \"jdbc:oracle:thin:@URL:PORT/DB\" connection-user : \"USERNAME\" connection-password : \"PASSWORD\" - name : cache type : memory properties : memory.max-data-per-node : \"128MB\" - name : wrangler type : wrangler Connector properties \u00b6 You can find the configuration properties for a connector from the following table. Connector Name Property Description Kafka MySql Oracle Redshift Multi cluster setup \u00b6 You can add more clusters when a single cluster can not not handle the query load or If your cluster is facing a specific performance problem. The following YAML file is an example of a typical Minerva multi cluster configuration. Replicate the complete code in the DataOS installation setup file(dataos.install.core.kernel.values.yaml) as shown. Give unique name to each cluster. - version : v1beta1 # cluster- 1 name : minervab type : cluster description : the default minerva cluster b tags : - cluster - minerva cluster : nodeSelector : \"dataos.io/purpose\" : \"query\" toleration : query runAsApiKey : api-key minerva : replicas : 2 resources : limits : cpu : 2000m memory : 4Gi requests : cpu : 2000m memory : 4Gi debug : logLevel : INFO trinoLogLevel : ERROR depots : - address : dataos://icebase:default properties : iceberg.file-format : PARQUET iceberg.compression-codec : GZIP hive.config.resources : \"/usr/trino/etc/catalog/core-site.xml\" hive.parquet.use-column-names : \"true\" - address : dataos://filebase:default properties : hive.config.resources : \"/usr/trino/etc/catalog/core-site.xml\" hive.parquet.use-column-names : \"true\" - address : dataos://kafka:default properties : kafka.empty-field-strategy : \"ADD_DUMMY\" kafka.table-description-supplier : \"confluent\" kafka.default-schema : \"default\" kafka.confluent-subjects-cache-refresh-interval : \"5s\" kafka.confluent-schema-registry-url : \"http://schema-registry.caretaker.svc.cluster.local:8081\" catalogs : - name : redshift type : redshift properties : connection-url : \"jdbc:redshift://URL:PORT/DB\" connection-user : \"USERNAME\" connection-password : \"PASSWORD\" - name : oracle type : oracle properties : connection-url : \"jdbc:oracle:thin:@URL:PORT/DB\" connection-user : \"USERNAME\" connection-password : \"PASSWORD\" - name : cache type : memory properties : memory.max-data-per-node : \"128MB\" - version : v1beta1 name : minervabc # cluster- 2 type : cluster description : the default minerva cluster c tags : - cluster - minerva cluster : nodeSelector : \"dataos.io/purpose\" : \"query\" toleration : query runAsApiKey : api-key minerva : replicas : 2 resources : limits : cpu : 2000m memory : 4Gi requests : cpu : 2000m memory : 4Gi debug : logLevel : INFO trinoLogLevel : ERROR depots : - address : dataos://icebase:default properties : iceberg.file-format : PARQUET iceberg.compression-codec : GZIP hive.config.resources : \"/usr/trino/etc/catalog/core-site.xml\" hive.parquet.use-column-names : \"true\" - address : dataos://filebase:default properties : hive.config.resources : \"/usr/trino/etc/catalog/core-site.xml\" hive.parquet.use-column-names : \"true\" catalogs : - name : redshift type : redshift properties : connection-url : \"jdbc:redshift://URL:PORT/DB\" connection-user : \"USERNAME\" connection-password : \"PASSWORD\" - name : oracle type : oracle properties : connection-url : \"jdbc:oracle:thin:@URL:PORT/DB\" connection-user : \"USERNAME\" connection-password : \"PASSWORD\" - name : cache type : memory properties : memory.max-data-per-node : \"128MB\" Access catalogs from Workbench \u00b6 Once the connector configuration setup is completed, you can access your data assets for a specific data source from DataOS Workbench. Workbench uses Minerva with full SQL support and provides you an interface, allowing you to access your data sources as catalogs and discover schemas. Sign in to your DataOS instance with your username and password. Go to DataOS Wokbench. Select catalog from the drop-down. Specify the schemas, or tables that you want to access. Write SQL queries.","title":"Minerva Cluster Tuning"},{"location":"minerva/tuneminerva/#minerva-cluster-tuning","text":"This article describes how you can create the Minerva clusters with optimized resources and configure it to connect to relational databases and various other data sources.","title":"Minerva Cluster Tuning"},{"location":"minerva/tuneminerva/#tune-minerva-cluster","text":"To create and tune Minerva cluster, provide the configuration properties for the Minerva cluster and connectors. These properties are saved in a DataOS setup YAML file stored in the DataOS configuration directory.","title":"Tune Minerva cluster"},{"location":"minerva/tuneminerva/#cluster-properties","text":"The default Minerva settings should work well for most workloads. You may adjust the following properties such as replicas, resources to ensure optimal performance: - version : v1beta1 name : minervab # Name of the Minerva cluster type : cluster description : the default minerva cluster b tags : - cluster - minerva cluster : # Minerva cluster properties nodeSelector : \"dataos.io/purpose\" : \"query\" toleration : query runAsApiKey : api-key minerva : # Tune these values for optimal performance replicas : 2 resources : limits : cpu : 2000m memory : 4Gi requests : cpu : 2000m memory : 4Gi debug : logLevel : INFO trinoLogLevel : ERROR","title":"Cluster properties"},{"location":"minerva/tuneminerva/#configure-connectors","text":"Minerva query engine offers a large variety of connectors, for example MySQL, PostgreSQL, Oracle, Redshift. These properties in DataOS setup file will mount the connector as the Catalog . This catalog contains schemas and references your data source via a connector. For Minerva, you can define connectors for data sources in the following ways: Depot: If you have already defined Depots to access data sources then you can include the address of these Depots in the DataOS setup YAML file to confifure connectors. You can also include additional properties to optimize the Minerva performance. Catalog: Here you can provide the name and properties such as connection url, user name and password to access the data source. The following is an example of connector configuration using Depot definition and catalog properties. All these definitions will be converted to catalogs which you can access on DataOS Workbench. Note : Replace the connection properties as appropriate for your setup. cluster : nodeSelector : \"dataos.io/purpose\" : \"query\" toleration : query runAsApiKey : api-key minerva : replicas : 2 resources : limits : cpu : 2000m memory : 4Gi requests : cpu : 2000m memory : 4Gi debug : logLevel : INFO trinoLogLevel : ERROR depots : # Pre-defined Depots and their properties - address : dataos://icebase:default properties : iceberg.file-format : PARQUET iceberg.compression-codec : GZIP hive.config.resources : \"/usr/trino/etc/catalog/core-site.xml\" hive.parquet.use-column-names : \"true\" - address : dataos://filebase:default properties : hive.config.resources : \"/usr/trino/etc/catalog/core-site.xml\" hive.parquet.use-column-names : \"true\" catalogs : # Data source connectors and their properties - name : redshift type : redshift properties : connection-url : \"jdbc:redshift://URL:PORT/DB\" connection-user : \"USERNAME\" connection-password : \"PASSWORD\" - name : oracle type : oracle properties : connection-url : \"jdbc:oracle:thin:@URL:PORT/DB\" connection-user : \"USERNAME\" connection-password : \"PASSWORD\" - name : cache type : memory properties : memory.max-data-per-node : \"128MB\" - name : wrangler type : wrangler","title":"Configure connectors"},{"location":"minerva/tuneminerva/#connector-properties","text":"You can find the configuration properties for a connector from the following table. Connector Name Property Description Kafka MySql Oracle Redshift","title":"Connector properties"},{"location":"minerva/tuneminerva/#multi-cluster-setup","text":"You can add more clusters when a single cluster can not not handle the query load or If your cluster is facing a specific performance problem. The following YAML file is an example of a typical Minerva multi cluster configuration. Replicate the complete code in the DataOS installation setup file(dataos.install.core.kernel.values.yaml) as shown. Give unique name to each cluster. - version : v1beta1 # cluster- 1 name : minervab type : cluster description : the default minerva cluster b tags : - cluster - minerva cluster : nodeSelector : \"dataos.io/purpose\" : \"query\" toleration : query runAsApiKey : api-key minerva : replicas : 2 resources : limits : cpu : 2000m memory : 4Gi requests : cpu : 2000m memory : 4Gi debug : logLevel : INFO trinoLogLevel : ERROR depots : - address : dataos://icebase:default properties : iceberg.file-format : PARQUET iceberg.compression-codec : GZIP hive.config.resources : \"/usr/trino/etc/catalog/core-site.xml\" hive.parquet.use-column-names : \"true\" - address : dataos://filebase:default properties : hive.config.resources : \"/usr/trino/etc/catalog/core-site.xml\" hive.parquet.use-column-names : \"true\" - address : dataos://kafka:default properties : kafka.empty-field-strategy : \"ADD_DUMMY\" kafka.table-description-supplier : \"confluent\" kafka.default-schema : \"default\" kafka.confluent-subjects-cache-refresh-interval : \"5s\" kafka.confluent-schema-registry-url : \"http://schema-registry.caretaker.svc.cluster.local:8081\" catalogs : - name : redshift type : redshift properties : connection-url : \"jdbc:redshift://URL:PORT/DB\" connection-user : \"USERNAME\" connection-password : \"PASSWORD\" - name : oracle type : oracle properties : connection-url : \"jdbc:oracle:thin:@URL:PORT/DB\" connection-user : \"USERNAME\" connection-password : \"PASSWORD\" - name : cache type : memory properties : memory.max-data-per-node : \"128MB\" - version : v1beta1 name : minervabc # cluster- 2 type : cluster description : the default minerva cluster c tags : - cluster - minerva cluster : nodeSelector : \"dataos.io/purpose\" : \"query\" toleration : query runAsApiKey : api-key minerva : replicas : 2 resources : limits : cpu : 2000m memory : 4Gi requests : cpu : 2000m memory : 4Gi debug : logLevel : INFO trinoLogLevel : ERROR depots : - address : dataos://icebase:default properties : iceberg.file-format : PARQUET iceberg.compression-codec : GZIP hive.config.resources : \"/usr/trino/etc/catalog/core-site.xml\" hive.parquet.use-column-names : \"true\" - address : dataos://filebase:default properties : hive.config.resources : \"/usr/trino/etc/catalog/core-site.xml\" hive.parquet.use-column-names : \"true\" catalogs : - name : redshift type : redshift properties : connection-url : \"jdbc:redshift://URL:PORT/DB\" connection-user : \"USERNAME\" connection-password : \"PASSWORD\" - name : oracle type : oracle properties : connection-url : \"jdbc:oracle:thin:@URL:PORT/DB\" connection-user : \"USERNAME\" connection-password : \"PASSWORD\" - name : cache type : memory properties : memory.max-data-per-node : \"128MB\"","title":"Multi cluster setup"},{"location":"minerva/tuneminerva/#access-catalogs-from-workbench","text":"Once the connector configuration setup is completed, you can access your data assets for a specific data source from DataOS Workbench. Workbench uses Minerva with full SQL support and provides you an interface, allowing you to access your data sources as catalogs and discover schemas. Sign in to your DataOS instance with your username and password. Go to DataOS Wokbench. Select catalog from the drop-down. Specify the schemas, or tables that you want to access. Write SQL queries.","title":"Access catalogs from Workbench"},{"location":"surge/","text":"DataOS \u00ae Surge \u00b6 Complex Event Processing Link First \u00b6 Seconds \u00b6 Tab 1 Markdown content . Multiple paragraphs. Tab 2 More Markdown content . list item a list item b Tab A Different tab set. Tab B More content.","title":"DataOS<sup>\u00ae</sup> Surge"},{"location":"surge/#dataos-surge","text":"Complex Event Processing Link","title":"DataOS\u00ae Surge"},{"location":"surge/#first","text":"","title":"First"},{"location":"surge/#seconds","text":"Tab 1 Markdown content . Multiple paragraphs. Tab 2 More Markdown content . list item a list item b Tab A Different tab set. Tab B More content.","title":"Seconds"},{"location":"tutorials/cheatsheets/","text":"Markdown cheat-sheet \u00b6 The Markdown Guide Mastering Markdown (3 minute read) Smart symbols \u00b6 (tm) (c) (r) c/o +/- --> <-- <--> =/= 1/4 1st 2nd \u2122 \u00a9 \u00ae \u2105 \u00b1 \u2192 \u2190 \u2194 \u2260 \u00bc 1 st 2 nd Inline highlights \u00b6 Here is some code: `#!py3 import pymdownx; pymdownx.__version__`. The mock shebang will be treated like text here: ` #!js var test = 0; `. Here is some code: import pymdownx ; pymdownx . __version__ . The mock shebang will be treated like text here: #!js var test = 0; . Details \u00b6 Details must contain a blank line before they start. Use ??? to start a details block or ???+ if you want to start a details block whose default state is 'open'. Follow the start of the block with an optional class or classes (separated with spaces) and the summary contained in quotes. Content is placed below the header and must be indented. ???+ note \"Open styled details\" ??? danger \"Nested details!\" And more content again. Open styled details Nested details! And more content again. Tabs \u00b6 Example Tab \u00b6 === \"Tab 1\" Markdown **content**. Multiple paragraphs. === \"Tab 2\" More Markdown **content**. - list item a - list item b Tab 1 Markdown content . Multiple paragraphs. Tab 2 More Markdown content . list item a list item b Example Tab Breaks \u00b6 === \"Tab 1\" Markdown **content**. Multiple paragraphs. === \"Tab 2\" More Markdown **content**. - list item a - list item b ===! \"Tab A\" Different tab set. === \"Tab B\" ``` More content. ``` Tab 1 Markdown content . Multiple paragraphs. Tab 2 More Markdown content . list item a list item b Tab A Different tab set. Tab B More content. Tasklist \u00b6 Simply start each list item with a square bracket pair containing either a space (an unchecked item) or a x (a checked item). Dummy hierarchical task list - [X] item 1 * [X] item A * [ ] item B more text + [x] item a + [ ] item b + [x] item c * [X] item C - [ ] item 2 - [ ] item 3 Dummy hierarchical task list item 1 item A item B more text item a item b item c item C item 2 item 3 Delete \u00b6 To wrap content in a delete tag, simply surround the text with double ~. ~~Delete me~~ Delete me Subscript \u00b6 To denote a subscript, you can surround the desired content in single ~. CH~3~CH~2~OH text~a\\ subscript~ CH 3 CH 2 OH text a subscript Headers \u00b6 # H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 Alternatively, for H1 and H2, an underline-ish style: Alt-H1 ====== Alt-H2 ------ Emphasis \u00b6 Emphasis, aka italics, with *asterisks* or _underscores_. Strong emphasis, aka bold, with **asterisks** or __underscores__. Combined emphasis with **asterisks and _underscores_**. Strikethrough uses two tildes. ~~Scratch this.~~ Emphasis, aka italics, with asterisks or underscores . Strong emphasis, aka bold, with asterisks or underscores . Combined emphasis with asterisks and underscores . Strikethrough uses two tildes. Scratch this. Lists \u00b6 1. First ordered list item 2. Another item * Unordered sub-list. 1. Actual numbers don't matter, just that it's a number 1. Ordered sub-list 4. And another item. Some text that should be aligned with the above item. * Unordered list can use asterisks - Or minuses + Or pluses First ordered list item Another item Unordered sub-list. Actual numbers don't matter, just that it's a number Ordered sub-list And another item. Some text that should be aligned with the above item. Unordered list can use asterisks Or minuses Or pluses Links \u00b6 There are two ways to create links. [I'm an inline-style link](https://www.google.com) [I'm a reference-style link][Arbitrary case-insensitive reference text] [You can use numbers for reference-style link definitions][1] Or leave it empty and use the [link text itself][] Some text to show that the reference links can follow later. [arbitrary case-insensitive reference text]: https://www.mozilla.org [1]: http://slashdot.org [link text itself]: http://www.reddit.com I'm an inline-style link I'm a reference-style link You can use numbers for reference-style link definitions Or leave it empty and use the link text itself Some text to show that the reference links can follow later. Images \u00b6 Here's our logo (hover to see the title text): Inline-style: ![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 1\") Reference-style: ![alt text][logo] [logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 2\" Here's our logo (hover to see the title text): Inline-style: Reference-style: Code and Syntax Highlighting \u00b6 Code blocks are part of the Markdown spec, but syntax highlighting isn't. However, many renderers -- like Github's and Markdown Here -- support syntax highlighting. Markdown Here supports highlighting for dozens of languages (and not-really-languages, like diffs and HTTP headers); to see the complete list, and how to write the language names, see the highlight.js demo page . Inline `code` has `back-ticks around` it. Inline code has back-ticks around it. Blocks of code are either fenced by lines with three back-ticks ``` , or are indented with four spaces. I recommend only using the fenced code blocks -- they're easier and only they support syntax highlighting. ```javascript var s = \"JavaScript syntax highlighting\"; alert(s); ``` ```python s = \"Python syntax highlighting\" print s ``` ``` No language indicated, so no syntax highlighting. But let's throw in a <b>tag</b>. ``` Line highlights ```css hl_lines=\"12 2 5\" html { scroll-padding-top: 80px; } body { font-size: 0.85rem; } .md-ellipsis{ position: relative; padding-right: 20px; } .md-header-nav__topic pre { padding: 0; margin: 0; white-space: unset; position: absolute; right: 7px; top: -10px; color: white; } ``` var s = \"JavaScript syntax highlighting\" ; alert ( s ); s = \"Python syntax highlighting\" print s No language indicated, so no syntax highlighting in Markdown Here. But let's throw in a <b>tag</b>. html { scroll-padding-top : 80 px ; } body { font-size : 0.85 rem ; } . md-ellipsis { position : relative ; padding-right : 20 px ; } . md-header-nav__topic pre { padding : 0 ; margin : 0 ; white-space : unset ; position : absolute ; right : 7 px ; top : -10 px ; color : white ; } Tables \u00b6 Tables aren't part of the core Markdown spec, but they are part of GFM and Markdown Here supports them. They are an easy way of adding tables to your email -- a task that would otherwise require copy-pasting from another application. Colons can be used to align columns. | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown | Less | Pretty --- | --- | --- *Still* | `renders` | **nicely** 1 | 2 | 3 Colons can be used to align columns. Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown Less Pretty Still renders nicely 1 2 3 Blockquotes \u00b6 > Blockquotes are very handy in email to emulate reply text. > This line is part of the same quote. Quote break. > This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can *put* **Markdown** into a blockquote. Blockquotes are very handy in email to emulate reply text. This line is part of the same quote. Quote break. This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote. Inline HTML \u00b6 You can also use raw HTML in your Markdown, and it'll mostly work pretty well. <dl> <dt>Definition list</dt> <dd>Is something people use sometimes.</dd> <dt>Markdown in HTML</dt> <dd>Does *not* work **very** well. Use HTML <em>tags</em>.</dd> </dl> Definition list Is something people use sometimes. Markdown in HTML Does *not* work **very** well. Use HTML tags . Horizontal Rule \u00b6 Three or more... --- Hyphens *** Asterisks ___ Underscores Three or more... Hyphens Asterisks Underscores Line Breaks \u00b6 My basic recommendation for learning how line breaks work is to experiment and discover -- hit <Enter> once (i.e., insert one newline), then hit it twice (i.e., insert two newlines), see what happens. You'll soon learn to get what you want. \"Markdown Toggle\" is your friend. Here are some things to try out: Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a *separate paragraph*. This line is also a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the *same paragraph*. Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a separate paragraph . This line is also begins a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the same paragraph . (Technical note: Markdown Here uses GFM line breaks, so there's no need to use MD's two-space line breaks.) Youtube videos \u00b6 They can't be added directly but you can add an image with a link to the video like this: <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=YOUTUBE_VIDEO_ID_HERE \" target=\"_blank\"><img src=\"http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg\" alt=\"IMAGE ALT TEXT HERE\" width=\"240\" height=\"180\" border=\"10\" /></a> Or, in pure Markdown, but losing the image sizing and border: [![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](http://www.youtube.com/watch?v=YOUTUBE_VIDEO_ID_HERE)","title":"Markdown cheat-sheet"},{"location":"tutorials/cheatsheets/#markdown-cheat-sheet","text":"The Markdown Guide Mastering Markdown (3 minute read)","title":"Markdown cheat-sheet"},{"location":"tutorials/cheatsheets/#smart-symbols","text":"(tm) (c) (r) c/o +/- --> <-- <--> =/= 1/4 1st 2nd \u2122 \u00a9 \u00ae \u2105 \u00b1 \u2192 \u2190 \u2194 \u2260 \u00bc 1 st 2 nd","title":"Smart symbols"},{"location":"tutorials/cheatsheets/#inline-highlights","text":"Here is some code: `#!py3 import pymdownx; pymdownx.__version__`. The mock shebang will be treated like text here: ` #!js var test = 0; `. Here is some code: import pymdownx ; pymdownx . __version__ . The mock shebang will be treated like text here: #!js var test = 0; .","title":"Inline highlights"},{"location":"tutorials/cheatsheets/#details","text":"Details must contain a blank line before they start. Use ??? to start a details block or ???+ if you want to start a details block whose default state is 'open'. Follow the start of the block with an optional class or classes (separated with spaces) and the summary contained in quotes. Content is placed below the header and must be indented. ???+ note \"Open styled details\" ??? danger \"Nested details!\" And more content again. Open styled details Nested details! And more content again.","title":"Details"},{"location":"tutorials/cheatsheets/#tabs","text":"","title":"Tabs"},{"location":"tutorials/cheatsheets/#example-tab","text":"=== \"Tab 1\" Markdown **content**. Multiple paragraphs. === \"Tab 2\" More Markdown **content**. - list item a - list item b Tab 1 Markdown content . Multiple paragraphs. Tab 2 More Markdown content . list item a list item b","title":"Example Tab"},{"location":"tutorials/cheatsheets/#example-tab-breaks","text":"=== \"Tab 1\" Markdown **content**. Multiple paragraphs. === \"Tab 2\" More Markdown **content**. - list item a - list item b ===! \"Tab A\" Different tab set. === \"Tab B\" ``` More content. ``` Tab 1 Markdown content . Multiple paragraphs. Tab 2 More Markdown content . list item a list item b Tab A Different tab set. Tab B More content.","title":"Example Tab Breaks"},{"location":"tutorials/cheatsheets/#tasklist","text":"Simply start each list item with a square bracket pair containing either a space (an unchecked item) or a x (a checked item). Dummy hierarchical task list - [X] item 1 * [X] item A * [ ] item B more text + [x] item a + [ ] item b + [x] item c * [X] item C - [ ] item 2 - [ ] item 3 Dummy hierarchical task list item 1 item A item B more text item a item b item c item C item 2 item 3","title":"Tasklist"},{"location":"tutorials/cheatsheets/#delete","text":"To wrap content in a delete tag, simply surround the text with double ~. ~~Delete me~~ Delete me","title":"Delete"},{"location":"tutorials/cheatsheets/#subscript","text":"To denote a subscript, you can surround the desired content in single ~. CH~3~CH~2~OH text~a\\ subscript~ CH 3 CH 2 OH text a subscript","title":"Subscript"},{"location":"tutorials/cheatsheets/#headers","text":"# H1 ## H2 ### H3 #### H4 ##### H5 ###### H6 Alternatively, for H1 and H2, an underline-ish style: Alt-H1 ====== Alt-H2 ------","title":"Headers"},{"location":"tutorials/cheatsheets/#emphasis","text":"Emphasis, aka italics, with *asterisks* or _underscores_. Strong emphasis, aka bold, with **asterisks** or __underscores__. Combined emphasis with **asterisks and _underscores_**. Strikethrough uses two tildes. ~~Scratch this.~~ Emphasis, aka italics, with asterisks or underscores . Strong emphasis, aka bold, with asterisks or underscores . Combined emphasis with asterisks and underscores . Strikethrough uses two tildes. Scratch this.","title":"Emphasis"},{"location":"tutorials/cheatsheets/#lists","text":"1. First ordered list item 2. Another item * Unordered sub-list. 1. Actual numbers don't matter, just that it's a number 1. Ordered sub-list 4. And another item. Some text that should be aligned with the above item. * Unordered list can use asterisks - Or minuses + Or pluses First ordered list item Another item Unordered sub-list. Actual numbers don't matter, just that it's a number Ordered sub-list And another item. Some text that should be aligned with the above item. Unordered list can use asterisks Or minuses Or pluses","title":"Lists"},{"location":"tutorials/cheatsheets/#links","text":"There are two ways to create links. [I'm an inline-style link](https://www.google.com) [I'm a reference-style link][Arbitrary case-insensitive reference text] [You can use numbers for reference-style link definitions][1] Or leave it empty and use the [link text itself][] Some text to show that the reference links can follow later. [arbitrary case-insensitive reference text]: https://www.mozilla.org [1]: http://slashdot.org [link text itself]: http://www.reddit.com I'm an inline-style link I'm a reference-style link You can use numbers for reference-style link definitions Or leave it empty and use the link text itself Some text to show that the reference links can follow later.","title":"Links"},{"location":"tutorials/cheatsheets/#images","text":"Here's our logo (hover to see the title text): Inline-style: ![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 1\") Reference-style: ![alt text][logo] [logo]: https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \"Logo Title Text 2\" Here's our logo (hover to see the title text): Inline-style: Reference-style:","title":"Images"},{"location":"tutorials/cheatsheets/#code-and-syntax-highlighting","text":"Code blocks are part of the Markdown spec, but syntax highlighting isn't. However, many renderers -- like Github's and Markdown Here -- support syntax highlighting. Markdown Here supports highlighting for dozens of languages (and not-really-languages, like diffs and HTTP headers); to see the complete list, and how to write the language names, see the highlight.js demo page . Inline `code` has `back-ticks around` it. Inline code has back-ticks around it. Blocks of code are either fenced by lines with three back-ticks ``` , or are indented with four spaces. I recommend only using the fenced code blocks -- they're easier and only they support syntax highlighting. ```javascript var s = \"JavaScript syntax highlighting\"; alert(s); ``` ```python s = \"Python syntax highlighting\" print s ``` ``` No language indicated, so no syntax highlighting. But let's throw in a <b>tag</b>. ``` Line highlights ```css hl_lines=\"12 2 5\" html { scroll-padding-top: 80px; } body { font-size: 0.85rem; } .md-ellipsis{ position: relative; padding-right: 20px; } .md-header-nav__topic pre { padding: 0; margin: 0; white-space: unset; position: absolute; right: 7px; top: -10px; color: white; } ``` var s = \"JavaScript syntax highlighting\" ; alert ( s ); s = \"Python syntax highlighting\" print s No language indicated, so no syntax highlighting in Markdown Here. But let's throw in a <b>tag</b>. html { scroll-padding-top : 80 px ; } body { font-size : 0.85 rem ; } . md-ellipsis { position : relative ; padding-right : 20 px ; } . md-header-nav__topic pre { padding : 0 ; margin : 0 ; white-space : unset ; position : absolute ; right : 7 px ; top : -10 px ; color : white ; }","title":"Code and Syntax Highlighting"},{"location":"tutorials/cheatsheets/#tables","text":"Tables aren't part of the core Markdown spec, but they are part of GFM and Markdown Here supports them. They are an easy way of adding tables to your email -- a task that would otherwise require copy-pasting from another application. Colons can be used to align columns. | Tables | Are | Cool | | ------------- |:-------------:| -----:| | col 3 is | right-aligned | $1600 | | col 2 is | centered | $12 | | zebra stripes | are neat | $1 | The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown | Less | Pretty --- | --- | --- *Still* | `renders` | **nicely** 1 | 2 | 3 Colons can be used to align columns. Tables Are Cool col 3 is right-aligned $1600 col 2 is centered $12 zebra stripes are neat $1 The outer pipes (|) are optional, and you don't need to make the raw Markdown line up prettily. You can also use inline Markdown. Markdown Less Pretty Still renders nicely 1 2 3","title":"Tables"},{"location":"tutorials/cheatsheets/#blockquotes","text":"> Blockquotes are very handy in email to emulate reply text. > This line is part of the same quote. Quote break. > This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can *put* **Markdown** into a blockquote. Blockquotes are very handy in email to emulate reply text. This line is part of the same quote. Quote break. This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can put Markdown into a blockquote.","title":"Blockquotes"},{"location":"tutorials/cheatsheets/#inline-html","text":"You can also use raw HTML in your Markdown, and it'll mostly work pretty well. <dl> <dt>Definition list</dt> <dd>Is something people use sometimes.</dd> <dt>Markdown in HTML</dt> <dd>Does *not* work **very** well. Use HTML <em>tags</em>.</dd> </dl> Definition list Is something people use sometimes. Markdown in HTML Does *not* work **very** well. Use HTML tags .","title":"Inline HTML"},{"location":"tutorials/cheatsheets/#horizontal-rule","text":"Three or more... --- Hyphens *** Asterisks ___ Underscores Three or more... Hyphens Asterisks Underscores","title":"Horizontal Rule"},{"location":"tutorials/cheatsheets/#line-breaks","text":"My basic recommendation for learning how line breaks work is to experiment and discover -- hit <Enter> once (i.e., insert one newline), then hit it twice (i.e., insert two newlines), see what happens. You'll soon learn to get what you want. \"Markdown Toggle\" is your friend. Here are some things to try out: Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a *separate paragraph*. This line is also a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the *same paragraph*. Here's a line for us to start with. This line is separated from the one above by two newlines, so it will be a separate paragraph . This line is also begins a separate paragraph, but... This line is only separated by a single newline, so it's a separate line in the same paragraph . (Technical note: Markdown Here uses GFM line breaks, so there's no need to use MD's two-space line breaks.)","title":"Line Breaks"},{"location":"tutorials/cheatsheets/#youtube-videos","text":"They can't be added directly but you can add an image with a link to the video like this: <a href=\"http://www.youtube.com/watch?feature=player_embedded&v=YOUTUBE_VIDEO_ID_HERE \" target=\"_blank\"><img src=\"http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg\" alt=\"IMAGE ALT TEXT HERE\" width=\"240\" height=\"180\" border=\"10\" /></a> Or, in pure Markdown, but losing the image sizing and border: [![IMAGE ALT TEXT HERE](http://img.youtube.com/vi/YOUTUBE_VIDEO_ID_HERE/0.jpg)](http://www.youtube.com/watch?v=YOUTUBE_VIDEO_ID_HERE)","title":"Youtube videos"},{"location":"tutorials/dataosconcepts/","text":"Core Concepts of DataOS [Overview] \u00b6 What is DataOS? \u00b6 DataOS Capabilities \u00b6 One Platform for all your data activities Data Management The DataOS seamlessly connects to all your source and sink systems and transform and curate data with our model-based low-code declarative artifacts. ACID-compliant data lake enables teams to store structured, unstructured and streaming data. Out-of-the-box data quality and profiling functionality lets you get quality and trusted data without additional processing. Leverage our observability principles to create robust alerts across the data lifecycle. Data Exploration - DataOS\u2019s Datanet creates a connective tissue between all your data systems continuously adding usage-based intelligence to your data. Easily Discover, search and find trusted and context-aware data. Understand the journey, relationships and impact of your datasets and make decisions with confidence. DataOS also has powerful workbench that lets you query datasets with simple SQL queries. Data Activation Activate your data and share it across your enterprise. DataOS lets you create customized reports on any datasets in minutes, share and collaborate on them securely. Build trigger alerts and notifications to 10+ destinations including ServiceNow, Jira, Pager Duty, Slack, MS teams etc. Consume your shared data from SaaS systems like salesforce, zendesk, google ads etc Data Products Build and launch new data products faster. DataOS handles all the infrastructure needed - Run,host,store,process,sync,serve to build your data application so teams can focus on innvoation. Governance & Security DataOS\u2019s Tag-based governance enables flexible and granular policy creation that gives teams an automated and scalable way to ensure governance and compliance for data within your ecosystem. Attribute-based access control future-proofs your org and lets it adapts to changing and new compliance regulations. Easy-to-create access and Data policies let you control, track and audit access to data. Row-level filtering and column masking give you granular control of what data authenticated users can access. Advanced primitives like data masking, data abstraction and differential privacy ensure your teams are working with trusted data at all times. Data Sharing Share data easily across your business ecosystem without copying or moving data. DataOS\u2019s best in class governance enables you to control access in a secured and auditable environment. Share data and collaborate with customers, internal and external partners to unlock newer and richer insights. Data Modeling DataOS\u2019s foundational MML(Map-Model) architecture brings a declarative model-first approach to pipeline creation. It abstracts away the complexity of pipeline building. MML makes it easy to build and manage data pipelines helping data engineering teams to streamline and simplify their ETL processes. High-level conceptual working \u00b6 The general activities involved with big data processing with DataOS: 1. Ingesting data into the system 2. Persisting the data in storage 3. Computing and analyzing data 4. Visualizing the results 5. Sharing and collaboration DataOS components [Overview] \u00b6 Depots \u00b6 Depot is a resource type and core primitive within the dataOS ecosystem. Depot provides a reference to the Data source/sink and abstracts the details of their configurations and storage formats. It helps you to easily understand the data source/sink and connect with it. You can use these Depots for accessing, processing and exploring data within DataOS. Depot contains the data location information along with any credentials and secrets that are required to access data from an external source. Once the Depot is created, you can use it in your Flare jobs, services and in other tools within the DataOS system. Datanet \u00b6 DataOS\u2019s Datanet creates a connective tissue between all your data systems continuously adding usage-based intelligence to your data. You can get a holistic view of all the data and understand the journey, relationships and impact of your datasets. You can easily search and find trusted and context-aware data. Datanet application consists of majorly five components - Search Fabric Policies Depots Tag Manager Search this component helps us to search across different entities like- dataset, job, workflow, service, function, query, dashboard etc. This search page is built using Elasticsearch. Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It gives us the ability to filter out the result as well facets based on the search criteria. The user also has the option to navigate the respective detail page. Fabric Fabric component provides the capability to search among different nodes like- job, dataset, depot, workflow, service, function, query and dashboard. It is a graphical representation of a search page. It also helps us to know the relationship among different nodes. We can also get primary information related to the node. Fabric component provides the capability to search among different nodes like- job, dataset, depot, workflow, service, function, query and dashboard. It is a graphical representation of a search page. It also helps us to know the relationship among different nodes. We can also get primary information related to the node. G6 graph library is used to build this component. Policies This component contains two things Access Policy Access policy helps to identify the list of access level policies that are available in the system. We can get the info about which policy tags have what level of access with respect to the subject and predicates. Mask Policies These policies are global policies available in the system which contains all the column level policy. This will helps to get the info about applied tags, priority and masking strategy. Depots This component shows the list of depots available in the system. This shows high level info about a depot like- name, type, description, catalog etc. Tag Manager This component displays list of tags available in the system. We have the functionality to create a new tag and also we can update an existing tag also in the system. Catalog \u00b6 Dataset \u00b6 Schema/tables \u00b6 Dictionary \u00b6 Linage \u00b6 You need to know the origin of data you are handling or interpreting. Impact \u00b6 Engineering teams in yoiur organization depend on this information to analyse the impact of change on all downstream processes and products and choose the appropriate strategy for implementing the change. Measures \u00b6 Fingerprinting \u00b6 Metrics and Checks Flare Minerva Workbench Atlas Basic Concepts and Terminology [Overview] AI Machine Learning Fingerprinting Data Profiling Cluster Computing Stream Data vs Batch Data Data Pipelines Data Ingestion/Exploration ELT vs ETL Data Lake vs Data Warehouse Data Sources A dataset in DataOS is a structured collection of data that you import or connect to. These datasets can be created while ingesting stream or batch data into DataOS. You can also create new datasets by joining existing datasets. You use these datasets while crteating reports and dashboards or doing descriptive analysis.","title":"Core Concepts of DataOS [Overview]"},{"location":"tutorials/dataosconcepts/#core-concepts-of-dataos-overview","text":"","title":"Core Concepts of DataOS [Overview]"},{"location":"tutorials/dataosconcepts/#what-is-dataos","text":"","title":"What is DataOS?"},{"location":"tutorials/dataosconcepts/#dataos-capabilities","text":"One Platform for all your data activities Data Management The DataOS seamlessly connects to all your source and sink systems and transform and curate data with our model-based low-code declarative artifacts. ACID-compliant data lake enables teams to store structured, unstructured and streaming data. Out-of-the-box data quality and profiling functionality lets you get quality and trusted data without additional processing. Leverage our observability principles to create robust alerts across the data lifecycle. Data Exploration - DataOS\u2019s Datanet creates a connective tissue between all your data systems continuously adding usage-based intelligence to your data. Easily Discover, search and find trusted and context-aware data. Understand the journey, relationships and impact of your datasets and make decisions with confidence. DataOS also has powerful workbench that lets you query datasets with simple SQL queries. Data Activation Activate your data and share it across your enterprise. DataOS lets you create customized reports on any datasets in minutes, share and collaborate on them securely. Build trigger alerts and notifications to 10+ destinations including ServiceNow, Jira, Pager Duty, Slack, MS teams etc. Consume your shared data from SaaS systems like salesforce, zendesk, google ads etc Data Products Build and launch new data products faster. DataOS handles all the infrastructure needed - Run,host,store,process,sync,serve to build your data application so teams can focus on innvoation. Governance & Security DataOS\u2019s Tag-based governance enables flexible and granular policy creation that gives teams an automated and scalable way to ensure governance and compliance for data within your ecosystem. Attribute-based access control future-proofs your org and lets it adapts to changing and new compliance regulations. Easy-to-create access and Data policies let you control, track and audit access to data. Row-level filtering and column masking give you granular control of what data authenticated users can access. Advanced primitives like data masking, data abstraction and differential privacy ensure your teams are working with trusted data at all times. Data Sharing Share data easily across your business ecosystem without copying or moving data. DataOS\u2019s best in class governance enables you to control access in a secured and auditable environment. Share data and collaborate with customers, internal and external partners to unlock newer and richer insights. Data Modeling DataOS\u2019s foundational MML(Map-Model) architecture brings a declarative model-first approach to pipeline creation. It abstracts away the complexity of pipeline building. MML makes it easy to build and manage data pipelines helping data engineering teams to streamline and simplify their ETL processes.","title":"DataOS Capabilities"},{"location":"tutorials/dataosconcepts/#high-level-conceptual-working","text":"The general activities involved with big data processing with DataOS: 1. Ingesting data into the system 2. Persisting the data in storage 3. Computing and analyzing data 4. Visualizing the results 5. Sharing and collaboration","title":"High-level conceptual working"},{"location":"tutorials/dataosconcepts/#dataos-components-overview","text":"","title":"DataOS components [Overview]"},{"location":"tutorials/dataosconcepts/#depots","text":"Depot is a resource type and core primitive within the dataOS ecosystem. Depot provides a reference to the Data source/sink and abstracts the details of their configurations and storage formats. It helps you to easily understand the data source/sink and connect with it. You can use these Depots for accessing, processing and exploring data within DataOS. Depot contains the data location information along with any credentials and secrets that are required to access data from an external source. Once the Depot is created, you can use it in your Flare jobs, services and in other tools within the DataOS system.","title":"Depots"},{"location":"tutorials/dataosconcepts/#datanet","text":"DataOS\u2019s Datanet creates a connective tissue between all your data systems continuously adding usage-based intelligence to your data. You can get a holistic view of all the data and understand the journey, relationships and impact of your datasets. You can easily search and find trusted and context-aware data. Datanet application consists of majorly five components - Search Fabric Policies Depots Tag Manager Search this component helps us to search across different entities like- dataset, job, workflow, service, function, query, dashboard etc. This search page is built using Elasticsearch. Elasticsearch is a search engine based on the Lucene library. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. It gives us the ability to filter out the result as well facets based on the search criteria. The user also has the option to navigate the respective detail page. Fabric Fabric component provides the capability to search among different nodes like- job, dataset, depot, workflow, service, function, query and dashboard. It is a graphical representation of a search page. It also helps us to know the relationship among different nodes. We can also get primary information related to the node. Fabric component provides the capability to search among different nodes like- job, dataset, depot, workflow, service, function, query and dashboard. It is a graphical representation of a search page. It also helps us to know the relationship among different nodes. We can also get primary information related to the node. G6 graph library is used to build this component. Policies This component contains two things Access Policy Access policy helps to identify the list of access level policies that are available in the system. We can get the info about which policy tags have what level of access with respect to the subject and predicates. Mask Policies These policies are global policies available in the system which contains all the column level policy. This will helps to get the info about applied tags, priority and masking strategy. Depots This component shows the list of depots available in the system. This shows high level info about a depot like- name, type, description, catalog etc. Tag Manager This component displays list of tags available in the system. We have the functionality to create a new tag and also we can update an existing tag also in the system.","title":"Datanet"},{"location":"tutorials/dataosconcepts/#catalog","text":"","title":"Catalog"},{"location":"tutorials/dataosconcepts/#dataset","text":"","title":"Dataset"},{"location":"tutorials/dataosconcepts/#schematables","text":"","title":"Schema/tables"},{"location":"tutorials/dataosconcepts/#dictionary","text":"","title":"Dictionary"},{"location":"tutorials/dataosconcepts/#linage","text":"You need to know the origin of data you are handling or interpreting.","title":"Linage"},{"location":"tutorials/dataosconcepts/#impact","text":"Engineering teams in yoiur organization depend on this information to analyse the impact of change on all downstream processes and products and choose the appropriate strategy for implementing the change.","title":"Impact"},{"location":"tutorials/dataosconcepts/#measures","text":"","title":"Measures"},{"location":"tutorials/dataosconcepts/#fingerprinting","text":"Metrics and Checks Flare Minerva Workbench Atlas Basic Concepts and Terminology [Overview] AI Machine Learning Fingerprinting Data Profiling Cluster Computing Stream Data vs Batch Data Data Pipelines Data Ingestion/Exploration ELT vs ETL Data Lake vs Data Warehouse Data Sources A dataset in DataOS is a structured collection of data that you import or connect to. These datasets can be created while ingesting stream or batch data into DataOS. You can also create new datasets by joining existing datasets. You use these datasets while crteating reports and dashboards or doing descriptive analysis.","title":"Fingerprinting"},{"location":"tutorials/depot/","text":"DataOS \u00ae DEPOT \u00b6 This document talks about DataOS\u00ae Depots and how to create them. What is a Depot? \u00b6 Depot is a resource which helps you to easily understand the data source/sink within DataOS\u00ae. The concept of a depot is to provide the information of data source/sink in an abstract manner. Table of contents \u00b6 Types of depot How to create a depot Config Detail for each depot type. Types of depot \u00b6 FILE PULSAR GCS BIGQUERY ABFSS KAFKA S3 MYSQL POSTGRESQL JDBC ELASTICSEARCH PRESTO REDIS How to create a depot \u00b6 Create a yaml with below content. version: v1beta1 name: \"raw01\" #(REQUIRED) type: depot #(REQUIRED) tags: #(OPTIONAL) - google-cloud owner: bob_tmdc_io #(OPTIONAL) description: \"Default Raw Zone Depot\" #(OPTIONAL) depot: #(REQUIRED) corresponding section for the type spec: #(REQUIRED) depot connection specification bucket: <bucket-name>, relativePath: \"raw\" connectionSecret: #(OPTIONAL) secrets to access the depot - acl: r #(OPTIONAL) secret that gives reader permissions type: key-value-properties - acl: rw #(OPTIONAL) secret that gives read & write permissions type: key-value-properties external: true hiveSync: false type: <depot-type> #(REQUIRED) depot type Run dataos-ctl apply -f <filepath> -n <workspace> Config Detail for each depot type. \u00b6 FILE \u00b6 version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: path: \"tmp/dataos\" external: true hiveSync: false type: FILE PULSAR \u00b6 version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: adminUrl: <admin-url> serviceUrl: <service-url> external: true hiveSync: false type: PULSAR BIGQUERY \u00b6 version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: project: \"aerobik-dataos\" dataset : \"dataset01\" params: key: \"value\" connectionSecret: - acl: rw values: bucket: <bucket-name> json_keyfile: <json-file-path> - acl: r values: json_keyfile: <json-file-path> external: true hiveSync: false type: BIGQUERY GCS \u00b6 version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: bucket: <bucket-name> relativePath: \"raw\" connectionSecret: - acl: rw values: email: <service-accounnt-email> gcskey_json: <json-file-path> projectid: <project-id> - acl: r values: email: <service-accounnt-email> gcskey_json: <json-file-path> projectid: <project-id> external: true hiveSync: false type: GCS ABFSS \u00b6 version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: account: \"aerobik-dataos\" container: \"container01\" connectionSecret: - acl: rw values: azurestorageaccountname: <azure-storage-account-name> azurestorageaccountkey: <azure-storage-account-key> - acl: r values: azurestorageaccountname: <azure-storage-account-name> azurestorageaccountkey: <azure-storage-account-key> external: true hiveSync: false type: ABFSS KAFKA \u00b6 version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: brokers: [\"localhost:9092\", \"localhost:9093\"] external: true hiveSync: false type: KAFKA S3 \u00b6 version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: bucket: <bucket-name> relativePath: \"raw\" connectionSecret: - acl: rw values: accesskeyid: <access-key-id> secretkey: <secret-key> - acl: r values: accesskeyid: <access-key-id> secretkey: <secret-key> external: true hiveSync: false type: S3 MYSQL \u00b6 version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: host: \"localhost\", port: \"3306\", database: \"mysql\" connectionSecret: - acl: rw values: username: <username> password: <password> - acl: r values: username: <username> password: <password> external: true hiveSync: false type: MYSQL POSTGRESQL \u00b6 version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: host: \"localhost\" port: \"3306\" database: \"mysql\" connectionSecret: - acl: rw values: username: <username> password: <password> - acl: r values: username: <username> password: <password> external: true hiveSync: false type: POSTGRESQL JDBC \u00b6 version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: subprotocol: \"mysql\" host: \"localhost\" port: \"3306\" database: \"mysql\" connectionSecret: - acl: rw values: username: <username> password: <password> - acl: r values: username: <username> password: <password> external: true hiveSync: false type: JDBC ELASTICSEARCH \u00b6 version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: nodes: [\"localhost:9092\", \"localhost:9093\"] connectionSecret: - acl: rw values: username: <username> password: <password> - acl: r values: username: <username> password: <password> external: true hiveSync: false type: ELASTICSEARCH PRESTO \u00b6 version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: host: \"localhost\" port: \"5432\" catalog: \"postgres\" connectionSecret: - acl: rw values: username: <username> password: <password> - acl: r values: username: <username> password: <password> external: true hiveSync: false type: PRESTO REDIS \u00b6 version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: host: \"localhost\" port: 5432 db: 10 table: \"user\" connectionSecret: - acl: rw values: password: <password> - acl: r values: password: <password> external: true hiveSync: false type: REDIS","title":"DataOS<sup>\u00ae</sup> DEPOT"},{"location":"tutorials/depot/#dataos-depot","text":"This document talks about DataOS\u00ae Depots and how to create them.","title":"DataOS\u00ae DEPOT"},{"location":"tutorials/depot/#what-is-a-depot","text":"Depot is a resource which helps you to easily understand the data source/sink within DataOS\u00ae. The concept of a depot is to provide the information of data source/sink in an abstract manner.","title":"What is a Depot?"},{"location":"tutorials/depot/#table-of-contents","text":"Types of depot How to create a depot Config Detail for each depot type.","title":"Table of contents"},{"location":"tutorials/depot/#types-of-depot","text":"FILE PULSAR GCS BIGQUERY ABFSS KAFKA S3 MYSQL POSTGRESQL JDBC ELASTICSEARCH PRESTO REDIS","title":"Types of depot"},{"location":"tutorials/depot/#how-to-create-a-depot","text":"Create a yaml with below content. version: v1beta1 name: \"raw01\" #(REQUIRED) type: depot #(REQUIRED) tags: #(OPTIONAL) - google-cloud owner: bob_tmdc_io #(OPTIONAL) description: \"Default Raw Zone Depot\" #(OPTIONAL) depot: #(REQUIRED) corresponding section for the type spec: #(REQUIRED) depot connection specification bucket: <bucket-name>, relativePath: \"raw\" connectionSecret: #(OPTIONAL) secrets to access the depot - acl: r #(OPTIONAL) secret that gives reader permissions type: key-value-properties - acl: rw #(OPTIONAL) secret that gives read & write permissions type: key-value-properties external: true hiveSync: false type: <depot-type> #(REQUIRED) depot type Run dataos-ctl apply -f <filepath> -n <workspace>","title":"How to create a depot"},{"location":"tutorials/depot/#config-detail-for-each-depot-type","text":"","title":"Config Detail for each depot type."},{"location":"tutorials/depot/#file","text":"version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: path: \"tmp/dataos\" external: true hiveSync: false type: FILE","title":"FILE"},{"location":"tutorials/depot/#pulsar","text":"version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: adminUrl: <admin-url> serviceUrl: <service-url> external: true hiveSync: false type: PULSAR","title":"PULSAR"},{"location":"tutorials/depot/#bigquery","text":"version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: project: \"aerobik-dataos\" dataset : \"dataset01\" params: key: \"value\" connectionSecret: - acl: rw values: bucket: <bucket-name> json_keyfile: <json-file-path> - acl: r values: json_keyfile: <json-file-path> external: true hiveSync: false type: BIGQUERY","title":"BIGQUERY"},{"location":"tutorials/depot/#gcs","text":"version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: bucket: <bucket-name> relativePath: \"raw\" connectionSecret: - acl: rw values: email: <service-accounnt-email> gcskey_json: <json-file-path> projectid: <project-id> - acl: r values: email: <service-accounnt-email> gcskey_json: <json-file-path> projectid: <project-id> external: true hiveSync: false type: GCS","title":"GCS"},{"location":"tutorials/depot/#abfss","text":"version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: account: \"aerobik-dataos\" container: \"container01\" connectionSecret: - acl: rw values: azurestorageaccountname: <azure-storage-account-name> azurestorageaccountkey: <azure-storage-account-key> - acl: r values: azurestorageaccountname: <azure-storage-account-name> azurestorageaccountkey: <azure-storage-account-key> external: true hiveSync: false type: ABFSS","title":"ABFSS"},{"location":"tutorials/depot/#kafka","text":"version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: brokers: [\"localhost:9092\", \"localhost:9093\"] external: true hiveSync: false type: KAFKA","title":"KAFKA"},{"location":"tutorials/depot/#s3","text":"version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: bucket: <bucket-name> relativePath: \"raw\" connectionSecret: - acl: rw values: accesskeyid: <access-key-id> secretkey: <secret-key> - acl: r values: accesskeyid: <access-key-id> secretkey: <secret-key> external: true hiveSync: false type: S3","title":"S3"},{"location":"tutorials/depot/#mysql","text":"version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: host: \"localhost\", port: \"3306\", database: \"mysql\" connectionSecret: - acl: rw values: username: <username> password: <password> - acl: r values: username: <username> password: <password> external: true hiveSync: false type: MYSQL","title":"MYSQL"},{"location":"tutorials/depot/#postgresql","text":"version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: host: \"localhost\" port: \"3306\" database: \"mysql\" connectionSecret: - acl: rw values: username: <username> password: <password> - acl: r values: username: <username> password: <password> external: true hiveSync: false type: POSTGRESQL","title":"POSTGRESQL"},{"location":"tutorials/depot/#jdbc","text":"version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: subprotocol: \"mysql\" host: \"localhost\" port: \"3306\" database: \"mysql\" connectionSecret: - acl: rw values: username: <username> password: <password> - acl: r values: username: <username> password: <password> external: true hiveSync: false type: JDBC","title":"JDBC"},{"location":"tutorials/depot/#elasticsearch","text":"version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: nodes: [\"localhost:9092\", \"localhost:9093\"] connectionSecret: - acl: rw values: username: <username> password: <password> - acl: r values: username: <username> password: <password> external: true hiveSync: false type: ELASTICSEARCH","title":"ELASTICSEARCH"},{"location":"tutorials/depot/#presto","text":"version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: host: \"localhost\" port: \"5432\" catalog: \"postgres\" connectionSecret: - acl: rw values: username: <username> password: <password> - acl: r values: username: <username> password: <password> external: true hiveSync: false type: PRESTO","title":"PRESTO"},{"location":"tutorials/depot/#redis","text":"version: v1beta1 name: \"raw01\" type: depot tags: - raw01 owner: bob_tmdc_io description: \"Default Raw Zone Depot\" depot: spec: host: \"localhost\" port: 5432 db: 10 table: \"user\" connectionSecret: - acl: rw values: password: <password> - acl: r values: password: <password> external: true hiveSync: false type: REDIS","title":"REDIS"},{"location":"tutorials/mermaid/","text":"Mermaid Diagrams \u00b6 Mermaid Home Mermaid Live Editor Class Diagram \u00b6 classDiagram Animal < |-- Duck Animal < |-- Fish Animal < |-- Zebra Animal : +int age Animal : +String gender Animal: +isMammal() Animal: +mate() class Duck{ +String beakColor +swim() +quack() } class Fish{ -int sizeInFeet -canEat() } class Zebra{ +bool is_wild +run() } Sequence Diagram \u00b6 sequenceDiagram Alice->>+John: Hello John, how are you? Alice->>+John: John, can you hear me? John-->>-Alice: Hi Alice, I can hear you! John-->>-Alice: I feel great! State Diagram \u00b6 stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*] Gantt \u00b6 gantt title A Gantt Diagram dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d Pie Chart \u00b6 pie title Pets adopted by volunteers \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15 ER Diagram \u00b6 erDiagram CUSTOMER }|..|{ DELIVERY-ADDRESS : has CUSTOMER ||--o{ ORDER : places CUSTOMER ||--o{ INVOICE : \"liable for\" DELIVERY-ADDRESS ||--o{ ORDER : receives INVOICE ||--|{ ORDER : covers ORDER ||--|{ ORDER-ITEM : includes PRODUCT-CATEGORY ||--|{ PRODUCT : contains PRODUCT ||--o{ ORDER-ITEM : \"ordered in\"","title":"Mermaid Diagrams"},{"location":"tutorials/mermaid/#mermaid-diagrams","text":"Mermaid Home Mermaid Live Editor","title":"Mermaid Diagrams"},{"location":"tutorials/mermaid/#class-diagram","text":"classDiagram Animal < |-- Duck Animal < |-- Fish Animal < |-- Zebra Animal : +int age Animal : +String gender Animal: +isMammal() Animal: +mate() class Duck{ +String beakColor +swim() +quack() } class Fish{ -int sizeInFeet -canEat() } class Zebra{ +bool is_wild +run() }","title":"Class Diagram"},{"location":"tutorials/mermaid/#sequence-diagram","text":"sequenceDiagram Alice->>+John: Hello John, how are you? Alice->>+John: John, can you hear me? John-->>-Alice: Hi Alice, I can hear you! John-->>-Alice: I feel great!","title":"Sequence Diagram"},{"location":"tutorials/mermaid/#state-diagram","text":"stateDiagram-v2 [*] --> Still Still --> [*] Still --> Moving Moving --> Still Moving --> Crash Crash --> [*]","title":"State Diagram"},{"location":"tutorials/mermaid/#gantt","text":"gantt title A Gantt Diagram dateFormat YYYY-MM-DD section Section A task :a1, 2014-01-01, 30d Another task :after a1 , 20d section Another Task in sec :2014-01-12 , 12d another task : 24d","title":"Gantt"},{"location":"tutorials/mermaid/#pie-chart","text":"pie title Pets adopted by volunteers \"Dogs\" : 386 \"Cats\" : 85 \"Rats\" : 15","title":"Pie Chart"},{"location":"tutorials/mermaid/#er-diagram","text":"erDiagram CUSTOMER }|..|{ DELIVERY-ADDRESS : has CUSTOMER ||--o{ ORDER : places CUSTOMER ||--o{ INVOICE : \"liable for\" DELIVERY-ADDRESS ||--o{ ORDER : receives INVOICE ||--|{ ORDER : covers ORDER ||--|{ ORDER-ITEM : includes PRODUCT-CATEGORY ||--|{ PRODUCT : contains PRODUCT ||--o{ ORDER-ITEM : \"ordered in\"","title":"ER Diagram"},{"location":"tutorials/nestedfieldsinspark3/","text":"Nested Fields: Spark3 Way \u00b6 New ways of dealing with nested fields in spark 3.1.1 release. The spark ecosystem has been evolving rapidly since spark3 has released. The community has been continuously solving major problems being faced by the developers, handling nested fields in spark is never easy. For those who are still wondering what are nested fields I am talking about, the below snippet showcase an example json with nested attributes. In above json snippet metadata contains two nested fields introducing themselves. In traditional spark way, in order to change one of these fields we will have to create another struct with same name and with all the fields like this- deviceDf.withColumn(\"metadata\", struct( col(\"metadata.iAmNested\"), lit(\"It's not easy to change this!\") // will change iAmAlsoNested )) Think about a metadata field with 50 nested fields! Not at all easy to manipulate the metadata field after that, right? Here is the modern way of doing this in spark :) Spark 3.1.1 has introduced withField api to deal with such a scenario, see the new flavour - deviceDf.withColumn(\"metadata\", 'metadata. withField (\"iAmAlsoNested\", lit(\" Yay! We did that! \")) ) In essence you can traverse any nested field now and update it like this - deviceDf.withColumn(\"metadata\", col(\"metadata\").withField(\" x.y.z.deeplyNested \", lit(\" Can deal with deeply nested fields too! \")) )) Not only updating a nested field, you can also drop one or more nested fields like this - deviceDf.withColumn(\"metadata\", col(\"metadata\"). dropFields (\" x.y.z.deeplyNestedOne \") ) In the end this is life saviour :)","title":"Handle nested fields in Spark3"},{"location":"tutorials/nestedfieldsinspark3/#nested-fields-spark3-way","text":"New ways of dealing with nested fields in spark 3.1.1 release. The spark ecosystem has been evolving rapidly since spark3 has released. The community has been continuously solving major problems being faced by the developers, handling nested fields in spark is never easy. For those who are still wondering what are nested fields I am talking about, the below snippet showcase an example json with nested attributes. In above json snippet metadata contains two nested fields introducing themselves. In traditional spark way, in order to change one of these fields we will have to create another struct with same name and with all the fields like this- deviceDf.withColumn(\"metadata\", struct( col(\"metadata.iAmNested\"), lit(\"It's not easy to change this!\") // will change iAmAlsoNested )) Think about a metadata field with 50 nested fields! Not at all easy to manipulate the metadata field after that, right? Here is the modern way of doing this in spark :) Spark 3.1.1 has introduced withField api to deal with such a scenario, see the new flavour - deviceDf.withColumn(\"metadata\", 'metadata. withField (\"iAmAlsoNested\", lit(\" Yay! We did that! \")) ) In essence you can traverse any nested field now and update it like this - deviceDf.withColumn(\"metadata\", col(\"metadata\").withField(\" x.y.z.deeplyNested \", lit(\" Can deal with deeply nested fields too! \")) )) Not only updating a nested field, you can also drop one or more nested fields like this - deviceDf.withColumn(\"metadata\", col(\"metadata\"). dropFields (\" x.y.z.deeplyNestedOne \") ) In the end this is life saviour :)","title":"Nested Fields: Spark3\u00a0Way"},{"location":"tutorials/rfmanalysis/","text":"RFM: Recency, Frequency, Monetary \u00b6 RFM is a method where you identify customers based on the recency of their last purchase, the total number of purchases they have made (frequency) and the amount they have spent (monetary). The marketers can target specific customer groups with communications that are much pertinent to their specific shared behaviour Recency: The time elapsed since a customer\u2019s last activity or transaction with the brand, forms the recency factor of RFM. An activity, though a transaction for most cases, can be a site/ store visit, adding products to cart etc. The guiding principle here is that the more recently a customer has interacted or transacted with a brand, the more likely that customer will be responsive to communications from the brand. Frequency: The number of times a customer transacted or interacted with the brand during a time window. The guiding principle here is that more frequent the activities by a customer the more engaged anf loyal they are; and vice versa. Monetary: Also referred to as \u201cmonetary value,\u201d this factor reflects how much a customer has spent with the brand during a particular period of time. Big spenders should usually be treated differently than customers who spend little. Combining this with frequency, an estimate on average spending can be made Calculating Recency, Frequency and Monetary \u00b6 Consider the following data exploration query: select /* main query*/ trans_frequency , count ( accountname ) as customer_count , round ( avg ( ltv )) as avg_ltv , round ( avg ( avg_rev )) as avg_rev from ( select /* sub-query */ accountname , count ( distinct transaction_id ) as trans_frequency , round ( sum ( product_revenue )) as ltv , round ( avg ( transaction_revenue )) as avg_rev from raw01 . orders_05 group by 1 order by 2 desc ) group by 1 order by 1 The query has a nested sub-query inside, which creates an output table having the transaction frequency (trans_frequency), lifetime value (ltv) and average revenue per transaction (avg_rev) grouped by the name (accountname). Sample output is as follows: accountname trans_frequency ltv avg_rev Acc1 488 667890 1984 Acc2 389 547900 2051 Acc3 356 172072 662 The main query here tries to understand the distribution of customers in frequency bands. trans_frequency customer_count avg_ltv avg_rev 205 1 185767 1746 208 1 181988 1369 203 1 178912 992 The following query delves into calculating the actual frequency (interactions) and on the basis of that, the recency. select days_since_last_purchase , count ( accountname ) as customer_count from ( select accountname , count ( distinct date ) as frequency , round ( sum ( product_revenue )) as ltv , date_diff ( 'day' , max ( date ( date_parse ( date , '%Y%m%d' ))), current_date ) as days_since_last_purchase from raw01 . orders_05 group by 1 order by 2 desc ) group by 1 order by 1 The sub-query lists 'distinct' date as frequency, because every row (interaction) has a unique timestamp and hence its count is the effective frequency we need for our analysis. The date_diff here tries to mark the recency in days, of a customer's interactions; by calculating the day difference between interaction date and today. Sample output is as follows: accountname frequency ltv days_since_last_purchase AccA 33 81897 75 AccB 33 133918 63 AccC 32 118922 66 The main query tries to group the customers into recency bands. The sample output is as follows: days_since_last_purchase customer_count 63 2951 64 3188 65 3372 For calculating the monetary value, we work on customer's ltv. Bucketizing it and dividing the customers in groups based on the buckets. select ( case when ltv <= 250 then '$250' when ltv > 250 AND ltv <= 550 then '$250-$550' when ltv > 550 and ltv <= 1200 THEn '$550-$1200' WHEN ltv > 1200 AND ltv <= 3000 then '$1200 - $3000' when ltv > 3000 then '>$3000' end ) as ltv_bucket , count ( accountname ) as customer_count from ( select account_id , accountname , count ( distinct date ) as frequency , round ( sum ( product_revenue )) as ltv , date_diff ( 'day' , max ( date ( date_parse ( date , '%Y%m%d' ))), current_date ) as days_since_last_purchase from raw01 . orders_05 group by 1 , 2 ) group by 1 The sample output is as follows: ltv_bucket customer_count $250 7207 $250 - $550 5443 $550 - $1200 5999 $1200 - $3000 6768 >$3000 These queries try to get the recency, frequency and monetary value separately and accordingly group the customers. However, unless customers have a score attached to their RFM values, the exercise is not fruitful. Keeping that in mind, the following query tries to attach scores to each customer based on their RFM values. create view raw01 . orders_rfm_analysis as ( select account_id , accountname , frequency , recency , ltv , case when frequency = 1 then 1 when frequency = 2 then 2 when frequency = 3 then 3 when frequency = 4 then 4 when frequency > 4 then 5 end as f_score , case when recency = 1 then 5 when recency = 2 then 4 when recency > 2 and recency <= 7 then 3 when recency > 7 and recency <= 14 then 2 when recency > 14 then 1 end as r_score , case when ltv <= 250 then 1 when ltv > 250 AND ltv <= 550 then 2 when ltv > 550 and ltv <= 1200 THEn 3 WHEN ltv > 1200 AND ltv <= 3000 then 4 when ltv > 3000 then 5 end as m_score FROM ( select account_id , accountname , count ( distinct date ) as frequency , round ( sum ( product_revenue )) as ltv , date_diff ( 'day' , max ( date ( date_parse ( date , '%Y%m%d' ))), current_date ) as recency from raw01 . orders_05 group by 1 , 2 )) ``` The actual RFM values are mapped to scores . The scores are marked from 1 to 5 , 5 being the most preferred value and 1 being the least . This bucketization brings all customers to a level playing field , to further group them and in effect , identify the priority ones . A sample of rows in the view is as follows : account_id | accountname | frequency | recency | ltv | f_score | r_score | m_score --- | --- | --- | --- | --- | --- | --- | --- 8509724 | AcctA | 14 | 3 | 22509 | 5 | 3 | 5 4609619 | AcctB | 3 | 6 | 25972 | 3 | 3 | 5 1002949 | AcctC | 2 | 68 | 1641 | 2 | 1 | 4 1602819 | AcctX | 3 | 81 | 1004 | 3 | 1 | 3 Customers with different combination of scores would mean different 'segments' . These segments are the actual deliverable of the exercise , pointing to the current state of the customer . The following query tries to define some of the segments based on the scores identified above : ``` sql create view raw01 . segments as select account_id , accountname , frequency as overall_frequency , recency as overall_recency , ltv as overall_ltv , f_score as overall_f_score , r_score as overall_r_score , m_score as overall_m_score , case when ( r_score between 4 and 5 ) and ( f_score between 4 and 5 ) and ( m_score between 4 and 5 ) then 'Y' else 'N' end as overall_champions , case when ( r_score between 2 and 5 ) and ( f_score between 3 and 5 ) and ( m_score between 3 and 5 ) then 'Y' else 'N' end as overall_loyal_customers , case when ( r_score between 3 and 5 ) and ( f_score between 1 and 3 ) and ( m_score between 1 and 3 ) then 'Y' else 'N' end as overall_potential_loyalist , case when ( r_score between 4 and 5 ) and ( f_score between 0 and 1 ) and ( m_score between 0 and 1 ) then 'Y' else 'N' end as overall_recent_customers , case when ( r_score between 3 and 4 ) and ( f_score between 0 and 1 ) and ( m_score between 0 and 1 ) then 'Y' else 'N' end as overall_promising , case when ( r_score between 2 and 3 ) and ( f_score between 2 and 3 ) and ( m_score between 2 and 3 ) then 'Y' else 'N' end as overall_customer_needs_attention , case when ( r_score between 2 and 3 ) and ( f_score between 0 and 2 ) and ( m_score between 0 and 2 ) then 'Y' else 'N' end as overall_about_to_sleep , case when ( r_score between 0 and 2 ) and ( f_score between 2 and 5 ) and ( m_score between 2 and 5 ) then 'Y' else 'N' end as overall_at_risk , case when ( r_score between 0 and 1 ) and ( f_score between 4 and 5 ) and ( m_score between 4 and 5 ) then 'Y' else 'N' end as overall_cant_lose_them , case when ( r_score between 1 and 2 ) and ( f_score between 1 and 2 ) and ( m_score between 1 and 2 ) then 'Y' else 'N' end as overall_hibernating , case when ( r_score between 0 and 2 ) and ( f_score between 0 and 2 ) and ( m_score between 0 and 2 ) then 'Y' else 'N' end as overall_lost from raw01 . orders_rfm_analysis Champions: Top tier (4 to 5) recency, frequency and monetary values. These are the priority target customers, deserving most attention and care. Loyal Customers: A recency score ranging from 2 to 5, frequency and monetary scores ranging from 3 to 5 point to a group of customers who are frequent and have a decent average transaction value but may not be as recent. Potential Loyalists: A recency score ranging from 3 to 5, frequency and monetary scores ranging from 1 to 3 point to a group of customers who are less frequent and have low average transaction value but are recent, and with appropriate targetting, can upscale to Loyal customers. Recent Customers: A recency score ranging from 4 to 5, frequency and monetary scores ranging from 0 to 1 point to a group of customers who have recently shown interest in the product. This may be the right time to target such customers. Promising Customers: A recency score ranging from 3 to 4, frequency and monetary scores ranging from 0 to 1 point to a group of customers who have fairly recently shown interest in the product. This may be the right time to target such customers. Customers needing attention: A recency, frequency and monetary scores ranging from 2 to 3 point to a group of customers who need immediate attention to upscale to Loyal or champion customers. Customers about to sleep: A recency score ranging from 2 to 3, frequency and monetary scores ranging from 0 to 2 point to a group of customers who close to being dormant/ go into hibernation. Customers at risk: A recency score ranging from 0 to 2, frequency and monetary scores ranging from 2 to 5 point to a group of customers who might be lost and need attention. Customers that shouldn't be lost ('Can't lose them'): A recency score ranging from 0 to 2, frequency and monetary scores ranging from 2 to 5 point to a group of customers who have a potential to be champions but have not shown any interest recently. Hibernating customers: A recency, frequency and monetary scores ranging from 1 to 2 point to a group of customers who are dormant or hibernating. Any efforts to target these customers can be deprioritized. Lost customers: A recency, frequency and monetary scores ranging from 0 to 2 point to a group of customers who are lost and all further efforts to target these customers should be stopped.","title":"RFM Analysis"},{"location":"tutorials/rfmanalysis/#rfm-recency-frequency-monetary","text":"RFM is a method where you identify customers based on the recency of their last purchase, the total number of purchases they have made (frequency) and the amount they have spent (monetary). The marketers can target specific customer groups with communications that are much pertinent to their specific shared behaviour Recency: The time elapsed since a customer\u2019s last activity or transaction with the brand, forms the recency factor of RFM. An activity, though a transaction for most cases, can be a site/ store visit, adding products to cart etc. The guiding principle here is that the more recently a customer has interacted or transacted with a brand, the more likely that customer will be responsive to communications from the brand. Frequency: The number of times a customer transacted or interacted with the brand during a time window. The guiding principle here is that more frequent the activities by a customer the more engaged anf loyal they are; and vice versa. Monetary: Also referred to as \u201cmonetary value,\u201d this factor reflects how much a customer has spent with the brand during a particular period of time. Big spenders should usually be treated differently than customers who spend little. Combining this with frequency, an estimate on average spending can be made","title":"RFM: Recency, Frequency, Monetary"},{"location":"tutorials/rfmanalysis/#calculating-recency-frequency-and-monetary","text":"Consider the following data exploration query: select /* main query*/ trans_frequency , count ( accountname ) as customer_count , round ( avg ( ltv )) as avg_ltv , round ( avg ( avg_rev )) as avg_rev from ( select /* sub-query */ accountname , count ( distinct transaction_id ) as trans_frequency , round ( sum ( product_revenue )) as ltv , round ( avg ( transaction_revenue )) as avg_rev from raw01 . orders_05 group by 1 order by 2 desc ) group by 1 order by 1 The query has a nested sub-query inside, which creates an output table having the transaction frequency (trans_frequency), lifetime value (ltv) and average revenue per transaction (avg_rev) grouped by the name (accountname). Sample output is as follows: accountname trans_frequency ltv avg_rev Acc1 488 667890 1984 Acc2 389 547900 2051 Acc3 356 172072 662 The main query here tries to understand the distribution of customers in frequency bands. trans_frequency customer_count avg_ltv avg_rev 205 1 185767 1746 208 1 181988 1369 203 1 178912 992 The following query delves into calculating the actual frequency (interactions) and on the basis of that, the recency. select days_since_last_purchase , count ( accountname ) as customer_count from ( select accountname , count ( distinct date ) as frequency , round ( sum ( product_revenue )) as ltv , date_diff ( 'day' , max ( date ( date_parse ( date , '%Y%m%d' ))), current_date ) as days_since_last_purchase from raw01 . orders_05 group by 1 order by 2 desc ) group by 1 order by 1 The sub-query lists 'distinct' date as frequency, because every row (interaction) has a unique timestamp and hence its count is the effective frequency we need for our analysis. The date_diff here tries to mark the recency in days, of a customer's interactions; by calculating the day difference between interaction date and today. Sample output is as follows: accountname frequency ltv days_since_last_purchase AccA 33 81897 75 AccB 33 133918 63 AccC 32 118922 66 The main query tries to group the customers into recency bands. The sample output is as follows: days_since_last_purchase customer_count 63 2951 64 3188 65 3372 For calculating the monetary value, we work on customer's ltv. Bucketizing it and dividing the customers in groups based on the buckets. select ( case when ltv <= 250 then '$250' when ltv > 250 AND ltv <= 550 then '$250-$550' when ltv > 550 and ltv <= 1200 THEn '$550-$1200' WHEN ltv > 1200 AND ltv <= 3000 then '$1200 - $3000' when ltv > 3000 then '>$3000' end ) as ltv_bucket , count ( accountname ) as customer_count from ( select account_id , accountname , count ( distinct date ) as frequency , round ( sum ( product_revenue )) as ltv , date_diff ( 'day' , max ( date ( date_parse ( date , '%Y%m%d' ))), current_date ) as days_since_last_purchase from raw01 . orders_05 group by 1 , 2 ) group by 1 The sample output is as follows: ltv_bucket customer_count $250 7207 $250 - $550 5443 $550 - $1200 5999 $1200 - $3000 6768 >$3000 These queries try to get the recency, frequency and monetary value separately and accordingly group the customers. However, unless customers have a score attached to their RFM values, the exercise is not fruitful. Keeping that in mind, the following query tries to attach scores to each customer based on their RFM values. create view raw01 . orders_rfm_analysis as ( select account_id , accountname , frequency , recency , ltv , case when frequency = 1 then 1 when frequency = 2 then 2 when frequency = 3 then 3 when frequency = 4 then 4 when frequency > 4 then 5 end as f_score , case when recency = 1 then 5 when recency = 2 then 4 when recency > 2 and recency <= 7 then 3 when recency > 7 and recency <= 14 then 2 when recency > 14 then 1 end as r_score , case when ltv <= 250 then 1 when ltv > 250 AND ltv <= 550 then 2 when ltv > 550 and ltv <= 1200 THEn 3 WHEN ltv > 1200 AND ltv <= 3000 then 4 when ltv > 3000 then 5 end as m_score FROM ( select account_id , accountname , count ( distinct date ) as frequency , round ( sum ( product_revenue )) as ltv , date_diff ( 'day' , max ( date ( date_parse ( date , '%Y%m%d' ))), current_date ) as recency from raw01 . orders_05 group by 1 , 2 )) ``` The actual RFM values are mapped to scores . The scores are marked from 1 to 5 , 5 being the most preferred value and 1 being the least . This bucketization brings all customers to a level playing field , to further group them and in effect , identify the priority ones . A sample of rows in the view is as follows : account_id | accountname | frequency | recency | ltv | f_score | r_score | m_score --- | --- | --- | --- | --- | --- | --- | --- 8509724 | AcctA | 14 | 3 | 22509 | 5 | 3 | 5 4609619 | AcctB | 3 | 6 | 25972 | 3 | 3 | 5 1002949 | AcctC | 2 | 68 | 1641 | 2 | 1 | 4 1602819 | AcctX | 3 | 81 | 1004 | 3 | 1 | 3 Customers with different combination of scores would mean different 'segments' . These segments are the actual deliverable of the exercise , pointing to the current state of the customer . The following query tries to define some of the segments based on the scores identified above : ``` sql create view raw01 . segments as select account_id , accountname , frequency as overall_frequency , recency as overall_recency , ltv as overall_ltv , f_score as overall_f_score , r_score as overall_r_score , m_score as overall_m_score , case when ( r_score between 4 and 5 ) and ( f_score between 4 and 5 ) and ( m_score between 4 and 5 ) then 'Y' else 'N' end as overall_champions , case when ( r_score between 2 and 5 ) and ( f_score between 3 and 5 ) and ( m_score between 3 and 5 ) then 'Y' else 'N' end as overall_loyal_customers , case when ( r_score between 3 and 5 ) and ( f_score between 1 and 3 ) and ( m_score between 1 and 3 ) then 'Y' else 'N' end as overall_potential_loyalist , case when ( r_score between 4 and 5 ) and ( f_score between 0 and 1 ) and ( m_score between 0 and 1 ) then 'Y' else 'N' end as overall_recent_customers , case when ( r_score between 3 and 4 ) and ( f_score between 0 and 1 ) and ( m_score between 0 and 1 ) then 'Y' else 'N' end as overall_promising , case when ( r_score between 2 and 3 ) and ( f_score between 2 and 3 ) and ( m_score between 2 and 3 ) then 'Y' else 'N' end as overall_customer_needs_attention , case when ( r_score between 2 and 3 ) and ( f_score between 0 and 2 ) and ( m_score between 0 and 2 ) then 'Y' else 'N' end as overall_about_to_sleep , case when ( r_score between 0 and 2 ) and ( f_score between 2 and 5 ) and ( m_score between 2 and 5 ) then 'Y' else 'N' end as overall_at_risk , case when ( r_score between 0 and 1 ) and ( f_score between 4 and 5 ) and ( m_score between 4 and 5 ) then 'Y' else 'N' end as overall_cant_lose_them , case when ( r_score between 1 and 2 ) and ( f_score between 1 and 2 ) and ( m_score between 1 and 2 ) then 'Y' else 'N' end as overall_hibernating , case when ( r_score between 0 and 2 ) and ( f_score between 0 and 2 ) and ( m_score between 0 and 2 ) then 'Y' else 'N' end as overall_lost from raw01 . orders_rfm_analysis Champions: Top tier (4 to 5) recency, frequency and monetary values. These are the priority target customers, deserving most attention and care. Loyal Customers: A recency score ranging from 2 to 5, frequency and monetary scores ranging from 3 to 5 point to a group of customers who are frequent and have a decent average transaction value but may not be as recent. Potential Loyalists: A recency score ranging from 3 to 5, frequency and monetary scores ranging from 1 to 3 point to a group of customers who are less frequent and have low average transaction value but are recent, and with appropriate targetting, can upscale to Loyal customers. Recent Customers: A recency score ranging from 4 to 5, frequency and monetary scores ranging from 0 to 1 point to a group of customers who have recently shown interest in the product. This may be the right time to target such customers. Promising Customers: A recency score ranging from 3 to 4, frequency and monetary scores ranging from 0 to 1 point to a group of customers who have fairly recently shown interest in the product. This may be the right time to target such customers. Customers needing attention: A recency, frequency and monetary scores ranging from 2 to 3 point to a group of customers who need immediate attention to upscale to Loyal or champion customers. Customers about to sleep: A recency score ranging from 2 to 3, frequency and monetary scores ranging from 0 to 2 point to a group of customers who close to being dormant/ go into hibernation. Customers at risk: A recency score ranging from 0 to 2, frequency and monetary scores ranging from 2 to 5 point to a group of customers who might be lost and need attention. Customers that shouldn't be lost ('Can't lose them'): A recency score ranging from 0 to 2, frequency and monetary scores ranging from 2 to 5 point to a group of customers who have a potential to be champions but have not shown any interest recently. Hibernating customers: A recency, frequency and monetary scores ranging from 1 to 2 point to a group of customers who are dormant or hibernating. Any efforts to target these customers can be deprioritized. Lost customers: A recency, frequency and monetary scores ranging from 0 to 2 point to a group of customers who are lost and all further efforts to target these customers should be stopped.","title":"Calculating Recency, Frequency and Monetary"},{"location":"tutorials/skewandkurtosis/","text":"Skewness \u00b6 Skewness is the degree of distortion or asymmetry from normal distribution or a symmetrical bell curve. A symmetrical distribution has a skewness of 0. There are two types of skew: Positive: When the right side tail is longer and thicker than the left, the skewness is said to be positive. The mean is highest followed by median and then mode. Negative: When the left side tail is longer and thicker than the right, the skewness is said to be negative. The mode is highest followed by median and then mean. Pic credits: codeburst.io Measure and Interpretation \u00b6 Skewness value between -0.5 and 0.5 indicates that the data is approximately symmetrical. Skewness value between -1 and -0.5 (negatively skewed) or between 0.5 and 1 (positively skewed) indicates that the data is moderately skewed . Skewness value less than -1 (negatively skewed) or greater than 1 (positively skewed) indicates that the data is highly skewed . Example: Lets say a shop has products costing from $100 to $1000, averaging about $500. If majority of the products cost less than $500, there is positive skewness and if the majority cost more than $500 there is negative skewness. Kurtosis \u00b6 It is the measure of outliers or extremes present in the data distribution. High kurtosis means that the data is outlier heavy or the tails are heavy, in which case the data needs to be thoroughly investigated. On the other hand, low kurtosis would mean lack of outliers/ light tails, which also mandates an investigation. Excess Kurtosis \u00b6 Excess kurtosis is a metric to compare kurtosis of a normal distribution to that of the relevant data. Excess Kurtosis = Kurtosis - 3 Types of Kurtosis \u00b6 Mesokurtic (Kurtosis \u2248 3): This distribution has kurtosis statistic similar or close to that of the normal distribution. This includes the outliers conforming to the normal distribution as well. Leptokurtic (Kurtosis > 3): This Distribution is longer, tails are fatter, the bulk of data appears in a 'lean' vertical range. Peak is higher and sharper than Mesokurtic, which means that data are heavy-tailed or profusion of outliers. Pic credits: towardsdatascience.com Platykurtic: (Kurtosis < 3): Distribution is shorter, tails are thinner than the normal distribution. The peak is lower and broader than Mesokurtic, which means that data is light-tailed and has a lack of outliers.","title":"Skew and Kurtosis"},{"location":"tutorials/skewandkurtosis/#skewness","text":"Skewness is the degree of distortion or asymmetry from normal distribution or a symmetrical bell curve. A symmetrical distribution has a skewness of 0. There are two types of skew: Positive: When the right side tail is longer and thicker than the left, the skewness is said to be positive. The mean is highest followed by median and then mode. Negative: When the left side tail is longer and thicker than the right, the skewness is said to be negative. The mode is highest followed by median and then mean. Pic credits: codeburst.io","title":"Skewness"},{"location":"tutorials/skewandkurtosis/#measure-and-interpretation","text":"Skewness value between -0.5 and 0.5 indicates that the data is approximately symmetrical. Skewness value between -1 and -0.5 (negatively skewed) or between 0.5 and 1 (positively skewed) indicates that the data is moderately skewed . Skewness value less than -1 (negatively skewed) or greater than 1 (positively skewed) indicates that the data is highly skewed . Example: Lets say a shop has products costing from $100 to $1000, averaging about $500. If majority of the products cost less than $500, there is positive skewness and if the majority cost more than $500 there is negative skewness.","title":"Measure and Interpretation"},{"location":"tutorials/skewandkurtosis/#kurtosis","text":"It is the measure of outliers or extremes present in the data distribution. High kurtosis means that the data is outlier heavy or the tails are heavy, in which case the data needs to be thoroughly investigated. On the other hand, low kurtosis would mean lack of outliers/ light tails, which also mandates an investigation.","title":"Kurtosis"},{"location":"tutorials/skewandkurtosis/#excess-kurtosis","text":"Excess kurtosis is a metric to compare kurtosis of a normal distribution to that of the relevant data. Excess Kurtosis = Kurtosis - 3","title":"Excess Kurtosis"},{"location":"tutorials/skewandkurtosis/#types-of-kurtosis","text":"Mesokurtic (Kurtosis \u2248 3): This distribution has kurtosis statistic similar or close to that of the normal distribution. This includes the outliers conforming to the normal distribution as well. Leptokurtic (Kurtosis > 3): This Distribution is longer, tails are fatter, the bulk of data appears in a 'lean' vertical range. Peak is higher and sharper than Mesokurtic, which means that data are heavy-tailed or profusion of outliers. Pic credits: towardsdatascience.com Platykurtic: (Kurtosis < 3): Distribution is shorter, tails are thinner than the normal distribution. The peak is lower and broader than Mesokurtic, which means that data is light-tailed and has a lack of outliers.","title":"Types of Kurtosis"},{"location":"tutorials/writing/","text":"Adding content \u00b6 First, Learn Markdown! Then, get yourself come toolings. If you are using Visual Studio Code - which we recommend - these 2 plugins would sure help: Markdown Preview Enhanced Mermaid Markdown Syntax Highlighting Learning materials \u00b6 Markdown \u00b6 The Markdown Guide Mastering Markdown (3 minute read) MkDocs \u00b6 Project documentation with Markdown A Material Design theme for MkDocs A Mermaid graphs plugin for mkdocs Diagrams \u00b6 Mermaid Writing documentation \u00b6 Say, you want to add a new section on DataOS \u00ae Alerting capabilities. 1. Create a new directory alerts under /docs 2. Add a new file index.md under /docs/alerts/ 3. Write your content in this file to your heart's content 4. Update nav section in mkdocs.yaml file to add this file to the navigation tabs nav: - Alerts: alerts/index.md 5. Push your changes to the repo git add . git commit -m 'your comments' git push origin master","title":"Adding content"},{"location":"tutorials/writing/#adding-content","text":"First, Learn Markdown! Then, get yourself come toolings. If you are using Visual Studio Code - which we recommend - these 2 plugins would sure help: Markdown Preview Enhanced Mermaid Markdown Syntax Highlighting","title":"Adding content"},{"location":"tutorials/writing/#learning-materials","text":"","title":"Learning materials"},{"location":"tutorials/writing/#markdown","text":"The Markdown Guide Mastering Markdown (3 minute read)","title":"Markdown"},{"location":"tutorials/writing/#mkdocs","text":"Project documentation with Markdown A Material Design theme for MkDocs A Mermaid graphs plugin for mkdocs","title":"MkDocs"},{"location":"tutorials/writing/#diagrams","text":"Mermaid","title":"Diagrams"},{"location":"tutorials/writing/#writing-documentation","text":"Say, you want to add a new section on DataOS \u00ae Alerting capabilities. 1. Create a new directory alerts under /docs 2. Add a new file index.md under /docs/alerts/ 3. Write your content in this file to your heart's content 4. Update nav section in mkdocs.yaml file to add this file to the navigation tabs nav: - Alerts: alerts/index.md 5. Push your changes to the repo git add . git commit -m 'your comments' git push origin master","title":"Writing documentation"},{"location":"workbench/","text":"DataOS \u00ae Workbench \u00b6 Introduction \u00b6 Purpose \u00b6 Workbench is a gateway for the user to explore data stored in any format within DataOS\u00ae. Users can write, run, and visualize SQL query results. Goin a step further they can save and share those queries with other users within an organization. Users can also publish queries to DataOS\u00ae Dashboard app in order to create business reports. Workbench has all the information from the Profiling and Catalog sections, removing the need to run ad-hoc analysis to understand data quality or working out the business context. Presto Query engine \u00b6 Presto is a distributed SQL query engine designed to query large data sets distributed over one or more heterogeneous data sources. Workbench uses Presto as the primary query engine. Presto is a tool designed to efficiently query vast amounts of data using distributed queries. Presto was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Presto is not limited to accessing HDFS. Presto can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Presto was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP). For more details, visit presto documentation Getting Started \u00b6 Launching Workbench \u00b6 Navigating from the apps screen, the workbench launcher can be found in the business section. Please refer to the highlight in the following screenshot Choosing a connection \u00b6 To begin with, the user needs to choose a schema from a dropdown selection of available schemas. Consider this as options to choose between querying datasets vis-a-vis datastreams. DataOS\u00ae Heap: Caching layer, stores rolling data of past two weeks(configurable). Enables App ecosystem DataOS\u00ae Table: Persistence layer, stores all the historical data. Can be accessed by standard query engine. Schemas/ Datalake zones: The left margin shows list of all Schemas/ zones. The lake can be divided into multiple zones, for instance one such division is described below: Raw - Immutable storage area Transient - Data stored temporarily for processing Enriched - Enhanced/Processed data storage Common - Curated data storage area Features \u00b6 Create a query \u00b6 Once the connection and schema have been chosen (Refer \u2018Choosing a connection\u2019 from the table of contents), one can starting writing the query. To begin with, a simple query can help you understand better: SELECT * FROM raw01.city_02 LIMIT 10 Where raw01 and city_02 are the schema and table names respectively. While the query is being formulated, the auto-complete functionality suggests the probable options (raw01, city_02 etc) and their types (raw01 schema, city_02 table etc) Refer the following screenshots for more clarity: Formatting shortcut \u00b6 Quick format shortcut saves the effort needed to organize a long complicated query and makes it easier to troubleshoot through the logic. A query written in any form, can be formatted to a more readable structure Visualize results \u00b6 User has the option to generate visualizations on the query results in a chart format. He can choose between line chart, bar chart, stacked bar or a scatter plot to better analyze the patterns. Following are the forms of chart formats supported as of now: Line chart Bar chart Horizontal Vertical Scatterplot Stacked bar chart Horizontal Vertical As an example, let us take the following query: SELECT department_name as department, category_name as category, sum (order_amount) as sales FROM set01.orders_enriched_view WHERE order_status = 'purchase' GROUP BY 1,2; This query, which fetches the department-wise retail sales data, can be better visualized using a scatterplot. Here, y-axis shows the sales revenue and x-axis shows the department types and category of each sale is represented by the colour of each circle e.g Red - Mens Shorts etc. Export Results \u00b6 The results of a query can be exported into a .csv, .xlsx or a .json file. The results can also be viewed in a tabular format as depicted in the screenshot below. The link can copied and shared for reference. Download option still enables the user to download the file in the same formats. This also helps in keeping an audit trail of who (user who opened the link and then downloaded, has to be logged into DataOS\u00ae at the time) downloaded the file, which is not possible when the file is downloaded first and then shared with anyone. Query results can also be exported to dashboard to plot a chart, after providing the query name, description and tags. Once in the Dashboard app, relevant visualizations and edits can be made to achieve the desired cahrts. Saved Benches \u00b6 Queries can be saved after assigning a relevant name for future reference. User can make the bench Public or Private as he deems fit. Private benches are not visible to any other user, Public benches are visible to and editable by anyone. User has the option to add tags to each of the saved benches. And it is strongly encouraged, as it helps a lot, functionally, to understand the context of the queries and to optimize searching entities. Tags have been provisioned extensively across DataOS\u00ae. Tags will also help in audits to understand the purpose behind an activity. History \u00b6 User will be able to view queries run by other users and himself, using the history tab on top. This feature helps in the following ways: Educating users: Users can refer to previously run queries to learn and reuse, part or whole query to achieve the desired results. This saves a lot of their time, otherwise spent in studying schema and tables and validating query logic and syntax. Review and validation: Users can go to history to validate queries run by others to further optimize and focus on a pertinent subset of data from the results. Re-Fetching old data: Users can go in and run their own queries to refetch previously saved data. Studio \u00b6 This feature can be accessed via the toggle switch as shown below: Studio offers two ways for the user to generate a query. Aggregate Raw Records Aggregate option serves the user with column based 'Measures', which are different statistical functions for each column. 'Group By' allows the user to summarize the datasets based on column(s) and perform aggregation on top of it. Time refers to the columns which timestamps, which can be used. Date Range and Granularity are further filters for the timestamp. '+Rule' offers use of relational operators based on column's datatype. '+Group' adds a new group of rules Finally, 'Generate SQL' converts the studio field values into an editable SQL query on the editor pane on the right side. Raw Records option, offers a different approach to the user. Instead of measures, user can find a simple list of columns (with their datatypes) and create rules and groups as per need. A 'Measure' can be added via the '+Measure' button, wherein user can insert his query to generate the custom measure he is looking for. Prepend toggle lets the user keep his latest generated query on top. Recipes/ Examples \u00b6 1: Converting JSON array of object into rows using \u2018UNNEST\u2019 \u00b6 Problem statement: Convert a JSON array of objects to an explorable row format. Key: - \u2018Events\u2019 : Table - \u2018Properties\u2019: Column holding the JSON string, one of the attributes of which is an array of objects. { \"event\" : \"click_event\" , \"properties\" : { \"section\" : \"top_nav\" , \"items\" : [{ \"item_id\" : 12345 , \"position\" : 1 }, { \"item_id\" : 67890 , \"position\" : 2 }, { \"item_id\" : 54321 , \"position\" : 3 }] } } This data needs to be queried into the following resultant form: event section item_id position click_event top_nav 12345 1 click_event top_nav 67890 2 click_event top_nav 54321 3 For this we will use the UNNEST function, which is used to expand an ARRAY or MAP into a relation. Step 1: Converting the array of objects to an array of map: SELECT json_extract_scalar ( properties , '$.event' ) AS event , json_extract_scalar ( properties , '$.section' ) AS section , CAST ( json_extract ( properties , '$.items' ) AS ARRAY < MAP < VARCHAR , VARCHAR >> ) AS items FROM events Here: json_extract_scalar : Parses the json string a returns a varchar value. json_extract : Parses the json string a returns a json value. event section items click_event top_nav [{item_id=12345, position=1}, {item_id= 567890, position=2}, {tem_id=54321, position=3}] Step 2: Breaking array into rows SELECT event , section , item FROM ( SELECT json_extract_scalar ( properties , '$.event' ) AS event , json_extract_scalar ( properties , '$.section' ) AS section , CAST ( json_extract ( properties , '$.items' ) AS ARRAY < MAP < VARCHAR , VARCHAR >> ) AS items FROM events ) CROSS JOIN UNNEST ( items ) AS items ( item ) The query above achieves breaking \u2018Items\u2019 array into separate \u2018item\u2019 rows. The result is as following: event section items click_event top_nav {item_id=12345, position=1} click_event top_nav {item_id= 567890, position=2} click_event top_nav {tem_id=54321, position=3} Step 3: Break the values of map into separate columns SELECT event , section , item [ 'item_id' ] AS item_id , item [ 'position' ] AS position FROM ( SELECT json_extract_scalar ( properties , '$.event' ) AS event , json_extract_scalar ( properties , '$.section' ) AS section , CAST ( json_extract ( properties , '$.items' ) AS ARRAY < MAP < varchar , varchar >> ) AS items FROM events ) CROSS JOIN UNNEST ( items ) AS items ( item ) event section item_id position click_event top_nav 12345 1 click_event top_nav 67890 2 click_event top_nav 54321 3 Source: https://medium.com/@hafizbadrie/prestodb-convert-json-array-of-objects-into-rows-d9c916724dfc 2: SQL queries for Funnel Analysis \u00b6 Funnels, as the name suggests, represent the flow of users performing multiple succedent events. One such funnel could be calculating the conversion rate of users, by analyzing how many users start and progress to which step while using a product. This is how one can localise the user retention issues, and work on fixing the problem area. Engagement Flow: User Sign up \u2192 Product integration \u2192 Schedule report (First use of the product) Step 1: User sign up Table: Bots (Users are termed bots here) To get the number of sign ups in the month of May: SELECT count ( * ) FROM bots WHERE created_at BETWEEN \u2018 2017 - 05 - 01 \u2019 AND \u2018 2017 - 05 - 31 \u2019 Count: 745 Step 2: Product Integration Table: Integrations (Each activity around a product integration is recorded here) To get the number of integrations in the month of May: SELECT count ( * ) FROM bots WHERE ( SELECT count ( * ) FROM integrations WHERE integrations . bot_id = bots . id ) > 0 AND created_at BETWEEN \u2018 2017 - 05 - 01 \u2019 AND \u2018 2017 - 05 - 31 \u2019 Count: 104 Step 3: Report Scheduling (First Use) Table: scheduled_reports (records all activity pertaining to report scheduling) To get the number of integrations in the month of May: SELECT count ( * ) FROM bots WHERE ( SELECT count ( * ) FROM integrations WHERE integrations . bot_id = bots . id ) > 0 AND ( SELECT count ( * ) FROM scheduled_reports WHERE scheduled_reports . bot_id = bots . id ) > 0 AND created_at BETWEEN \u2018 2017 - 05 - 01 \u2019 AND \u2018 2017 - 05 - 31 \u2019 Count:61","title":"DataOS<sup>\u00ae</sup> Workbench"},{"location":"workbench/#dataos-workbench","text":"","title":"DataOS\u00ae Workbench"},{"location":"workbench/#introduction","text":"","title":"Introduction"},{"location":"workbench/#purpose","text":"Workbench is a gateway for the user to explore data stored in any format within DataOS\u00ae. Users can write, run, and visualize SQL query results. Goin a step further they can save and share those queries with other users within an organization. Users can also publish queries to DataOS\u00ae Dashboard app in order to create business reports. Workbench has all the information from the Profiling and Catalog sections, removing the need to run ad-hoc analysis to understand data quality or working out the business context.","title":"Purpose"},{"location":"workbench/#presto-query-engine","text":"Presto is a distributed SQL query engine designed to query large data sets distributed over one or more heterogeneous data sources. Workbench uses Presto as the primary query engine. Presto is a tool designed to efficiently query vast amounts of data using distributed queries. Presto was designed as an alternative to tools that query HDFS using pipelines of MapReduce jobs, such as Hive or Pig, but Presto is not limited to accessing HDFS. Presto can be and has been extended to operate over different kinds of data sources, including traditional relational databases and other data sources such as Cassandra. Presto was designed to handle data warehousing and analytics: data analysis, aggregating large amounts of data and producing reports. These workloads are often classified as Online Analytical Processing (OLAP). For more details, visit presto documentation","title":"Presto Query engine"},{"location":"workbench/#getting-started","text":"","title":"Getting Started"},{"location":"workbench/#launching-workbench","text":"Navigating from the apps screen, the workbench launcher can be found in the business section. Please refer to the highlight in the following screenshot","title":"Launching Workbench"},{"location":"workbench/#choosing-a-connection","text":"To begin with, the user needs to choose a schema from a dropdown selection of available schemas. Consider this as options to choose between querying datasets vis-a-vis datastreams. DataOS\u00ae Heap: Caching layer, stores rolling data of past two weeks(configurable). Enables App ecosystem DataOS\u00ae Table: Persistence layer, stores all the historical data. Can be accessed by standard query engine. Schemas/ Datalake zones: The left margin shows list of all Schemas/ zones. The lake can be divided into multiple zones, for instance one such division is described below: Raw - Immutable storage area Transient - Data stored temporarily for processing Enriched - Enhanced/Processed data storage Common - Curated data storage area","title":"Choosing a connection"},{"location":"workbench/#features","text":"","title":"Features"},{"location":"workbench/#create-a-query","text":"Once the connection and schema have been chosen (Refer \u2018Choosing a connection\u2019 from the table of contents), one can starting writing the query. To begin with, a simple query can help you understand better: SELECT * FROM raw01.city_02 LIMIT 10 Where raw01 and city_02 are the schema and table names respectively. While the query is being formulated, the auto-complete functionality suggests the probable options (raw01, city_02 etc) and their types (raw01 schema, city_02 table etc) Refer the following screenshots for more clarity:","title":"Create a query"},{"location":"workbench/#formatting-shortcut","text":"Quick format shortcut saves the effort needed to organize a long complicated query and makes it easier to troubleshoot through the logic. A query written in any form, can be formatted to a more readable structure","title":"Formatting shortcut"},{"location":"workbench/#visualize-results","text":"User has the option to generate visualizations on the query results in a chart format. He can choose between line chart, bar chart, stacked bar or a scatter plot to better analyze the patterns. Following are the forms of chart formats supported as of now: Line chart Bar chart Horizontal Vertical Scatterplot Stacked bar chart Horizontal Vertical As an example, let us take the following query: SELECT department_name as department, category_name as category, sum (order_amount) as sales FROM set01.orders_enriched_view WHERE order_status = 'purchase' GROUP BY 1,2; This query, which fetches the department-wise retail sales data, can be better visualized using a scatterplot. Here, y-axis shows the sales revenue and x-axis shows the department types and category of each sale is represented by the colour of each circle e.g Red - Mens Shorts etc.","title":"Visualize results"},{"location":"workbench/#export-results","text":"The results of a query can be exported into a .csv, .xlsx or a .json file. The results can also be viewed in a tabular format as depicted in the screenshot below. The link can copied and shared for reference. Download option still enables the user to download the file in the same formats. This also helps in keeping an audit trail of who (user who opened the link and then downloaded, has to be logged into DataOS\u00ae at the time) downloaded the file, which is not possible when the file is downloaded first and then shared with anyone. Query results can also be exported to dashboard to plot a chart, after providing the query name, description and tags. Once in the Dashboard app, relevant visualizations and edits can be made to achieve the desired cahrts.","title":"Export Results"},{"location":"workbench/#saved-benches","text":"Queries can be saved after assigning a relevant name for future reference. User can make the bench Public or Private as he deems fit. Private benches are not visible to any other user, Public benches are visible to and editable by anyone. User has the option to add tags to each of the saved benches. And it is strongly encouraged, as it helps a lot, functionally, to understand the context of the queries and to optimize searching entities. Tags have been provisioned extensively across DataOS\u00ae. Tags will also help in audits to understand the purpose behind an activity.","title":"Saved Benches"},{"location":"workbench/#history","text":"User will be able to view queries run by other users and himself, using the history tab on top. This feature helps in the following ways: Educating users: Users can refer to previously run queries to learn and reuse, part or whole query to achieve the desired results. This saves a lot of their time, otherwise spent in studying schema and tables and validating query logic and syntax. Review and validation: Users can go to history to validate queries run by others to further optimize and focus on a pertinent subset of data from the results. Re-Fetching old data: Users can go in and run their own queries to refetch previously saved data.","title":"History"},{"location":"workbench/#studio","text":"This feature can be accessed via the toggle switch as shown below: Studio offers two ways for the user to generate a query. Aggregate Raw Records Aggregate option serves the user with column based 'Measures', which are different statistical functions for each column. 'Group By' allows the user to summarize the datasets based on column(s) and perform aggregation on top of it. Time refers to the columns which timestamps, which can be used. Date Range and Granularity are further filters for the timestamp. '+Rule' offers use of relational operators based on column's datatype. '+Group' adds a new group of rules Finally, 'Generate SQL' converts the studio field values into an editable SQL query on the editor pane on the right side. Raw Records option, offers a different approach to the user. Instead of measures, user can find a simple list of columns (with their datatypes) and create rules and groups as per need. A 'Measure' can be added via the '+Measure' button, wherein user can insert his query to generate the custom measure he is looking for. Prepend toggle lets the user keep his latest generated query on top.","title":"Studio"},{"location":"workbench/#recipes-examples","text":"","title":"Recipes/ Examples"},{"location":"workbench/#1-converting-json-array-of-object-into-rows-using-unnest","text":"Problem statement: Convert a JSON array of objects to an explorable row format. Key: - \u2018Events\u2019 : Table - \u2018Properties\u2019: Column holding the JSON string, one of the attributes of which is an array of objects. { \"event\" : \"click_event\" , \"properties\" : { \"section\" : \"top_nav\" , \"items\" : [{ \"item_id\" : 12345 , \"position\" : 1 }, { \"item_id\" : 67890 , \"position\" : 2 }, { \"item_id\" : 54321 , \"position\" : 3 }] } } This data needs to be queried into the following resultant form: event section item_id position click_event top_nav 12345 1 click_event top_nav 67890 2 click_event top_nav 54321 3 For this we will use the UNNEST function, which is used to expand an ARRAY or MAP into a relation. Step 1: Converting the array of objects to an array of map: SELECT json_extract_scalar ( properties , '$.event' ) AS event , json_extract_scalar ( properties , '$.section' ) AS section , CAST ( json_extract ( properties , '$.items' ) AS ARRAY < MAP < VARCHAR , VARCHAR >> ) AS items FROM events Here: json_extract_scalar : Parses the json string a returns a varchar value. json_extract : Parses the json string a returns a json value. event section items click_event top_nav [{item_id=12345, position=1}, {item_id= 567890, position=2}, {tem_id=54321, position=3}] Step 2: Breaking array into rows SELECT event , section , item FROM ( SELECT json_extract_scalar ( properties , '$.event' ) AS event , json_extract_scalar ( properties , '$.section' ) AS section , CAST ( json_extract ( properties , '$.items' ) AS ARRAY < MAP < VARCHAR , VARCHAR >> ) AS items FROM events ) CROSS JOIN UNNEST ( items ) AS items ( item ) The query above achieves breaking \u2018Items\u2019 array into separate \u2018item\u2019 rows. The result is as following: event section items click_event top_nav {item_id=12345, position=1} click_event top_nav {item_id= 567890, position=2} click_event top_nav {tem_id=54321, position=3} Step 3: Break the values of map into separate columns SELECT event , section , item [ 'item_id' ] AS item_id , item [ 'position' ] AS position FROM ( SELECT json_extract_scalar ( properties , '$.event' ) AS event , json_extract_scalar ( properties , '$.section' ) AS section , CAST ( json_extract ( properties , '$.items' ) AS ARRAY < MAP < varchar , varchar >> ) AS items FROM events ) CROSS JOIN UNNEST ( items ) AS items ( item ) event section item_id position click_event top_nav 12345 1 click_event top_nav 67890 2 click_event top_nav 54321 3 Source: https://medium.com/@hafizbadrie/prestodb-convert-json-array-of-objects-into-rows-d9c916724dfc","title":"1: Converting JSON array of object into rows using \u2018UNNEST\u2019"},{"location":"workbench/#2-sql-queries-for-funnel-analysis","text":"Funnels, as the name suggests, represent the flow of users performing multiple succedent events. One such funnel could be calculating the conversion rate of users, by analyzing how many users start and progress to which step while using a product. This is how one can localise the user retention issues, and work on fixing the problem area. Engagement Flow: User Sign up \u2192 Product integration \u2192 Schedule report (First use of the product) Step 1: User sign up Table: Bots (Users are termed bots here) To get the number of sign ups in the month of May: SELECT count ( * ) FROM bots WHERE created_at BETWEEN \u2018 2017 - 05 - 01 \u2019 AND \u2018 2017 - 05 - 31 \u2019 Count: 745 Step 2: Product Integration Table: Integrations (Each activity around a product integration is recorded here) To get the number of integrations in the month of May: SELECT count ( * ) FROM bots WHERE ( SELECT count ( * ) FROM integrations WHERE integrations . bot_id = bots . id ) > 0 AND created_at BETWEEN \u2018 2017 - 05 - 01 \u2019 AND \u2018 2017 - 05 - 31 \u2019 Count: 104 Step 3: Report Scheduling (First Use) Table: scheduled_reports (records all activity pertaining to report scheduling) To get the number of integrations in the month of May: SELECT count ( * ) FROM bots WHERE ( SELECT count ( * ) FROM integrations WHERE integrations . bot_id = bots . id ) > 0 AND ( SELECT count ( * ) FROM scheduled_reports WHERE scheduled_reports . bot_id = bots . id ) > 0 AND created_at BETWEEN \u2018 2017 - 05 - 01 \u2019 AND \u2018 2017 - 05 - 31 \u2019 Count:61","title":"2: SQL queries for Funnel Analysis"}]}